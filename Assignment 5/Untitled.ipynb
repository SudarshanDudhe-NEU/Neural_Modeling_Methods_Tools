{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40039b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'siddhartha', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the']\n",
      "Vocabulary Size: 4028\n",
      "Example token indices: [3518, 2684, 1572, 1067, 2360, 3145, 3548, 1067, 1871, 1396, 3518, 3740, 2360, 138, 140, 1772, 3518, 3695, 3333, 111, 2228, 2416, 2470, 2360, 3518, 3967, 207, 2300, 724, 111, 3937, 92, 2300, 2885, 3891, 4015, 2138, 713, 1875, 1498, 1875, 236, 2404, 2895, 1875, 3685, 3518, 3503, 2360, 3518]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sudarshan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the NLTK tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to clean and preprocess the text\n",
    "def preprocess_text(file_path):\n",
    "    # Read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Path to your Gutenberg text file\n",
    "gutenberg_text_path = 'siddhartha.txt'\n",
    "tokens = preprocess_text(gutenberg_text_path)\n",
    "\n",
    "# Example: Print first 50 tokens\n",
    "print(tokens[:50])\n",
    "\n",
    "# Create a vocabulary and mapping from words to indices\n",
    "vocab = sorted(set(tokens))\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Convert tokens to integers\n",
    "token_indices = [word_to_idx[word] for word in tokens]\n",
    "\n",
    "# Print vocabulary size and token indices example\n",
    "print(f\"Vocabulary Size: {len(vocab)}\")\n",
    "print(f\"Example token indices: {token_indices[:50]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b919c3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized keyword arguments passed to Embedding: {'batch_input_shape': [64, None]}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Build the model\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_rnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Compile the model with a loss function and optimizer\u001b[39;00m\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mbuild_rnn_model\u001b[0;34m(vocab_size, embedding_dim, rnn_units, batch_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_rnn_model\u001b[39m(vocab_size, embedding_dim, rnn_units, batch_size):\n\u001b[1;32m     13\u001b[0m     model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m---> 14\u001b[0m         \u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_input_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     15\u001b[0m         SimpleRNN(rnn_units, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, stateful\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, recurrent_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglorot_uniform\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     16\u001b[0m         Dense(vocab_size)\n\u001b[1;32m     17\u001b[0m     ])\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:81\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero, lora_rank, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     72\u001b[0m     input_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     80\u001b[0m ):\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m=\u001b[39m output_dim\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:264\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_shape_arg \u001b[38;5;241m=\u001b[39m input_shape_arg\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized keyword arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast \u001b[38;5;241m=\u001b[39m autocast\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized keyword arguments passed to Embedding: {'batch_input_shape': [64, None]}"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 64\n",
    "rnn_units = 256\n",
    "batch_size = 64\n",
    "\n",
    "# Define the RNN model\n",
    "def build_rnn_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        SimpleRNN(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_rnn_model(vocab_size, embedding_dim, rnn_units, batch_size)\n",
    "\n",
    "# Compile the model with a loss function and optimizer\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae230637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (64, 100), Target shape: (64,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 02:45:56.339172: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Sequence length for input\n",
    "seq_length = 100\n",
    "\n",
    "# Create sequences of fixed length\n",
    "def create_sequences(token_indices, seq_length):\n",
    "    sequences = []\n",
    "    next_words = []\n",
    "    for i in range(0, len(token_indices) - seq_length, 1):\n",
    "        sequences.append(token_indices[i:i + seq_length])\n",
    "        next_words.append(token_indices[i + seq_length])\n",
    "    \n",
    "    return np.array(sequences), np.array(next_words)\n",
    "\n",
    "sequences, next_words = create_sequences(token_indices, seq_length)\n",
    "\n",
    "# Reshape and prepare for model\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences, next_words))\n",
    "dataset = dataset.shuffle(buffer_size=len(sequences)).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "# Example: Print shape of dataset\n",
    "for input_seq, target_seq in dataset.take(1):\n",
    "    print(f\"Input shape: {input_seq.shape}, Target shape: {target_seq.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f568b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training the model with BPTT\n",
    "# epochs = 10\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(dataset, epochs=epochs)\n",
    "\n",
    "# # Save the trained model\n",
    "# model.save('trained_rnn_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecfa4ee",
   "metadata": {},
   "source": [
    "Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7c85bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Path to your GloVe embeddings (ensure the file 'glove.6B.100d.txt' is inside this directory)\n",
    "glove_dir = \"data/glove.6B.100d.txt\"\n",
    "embedding_dim = 100  # GloVe embedding dimension\n",
    "\n",
    "# Load the GloVe embeddings into a dictionary\n",
    "def load_glove_embeddings(glove_dir):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_dir, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Load GloVe vectors\n",
    "embeddings_index = load_glove_embeddings(glove_dir)\n",
    "print(f\"Found {len(embeddings_index)} word vectors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8def3113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (4028, 100)\n"
     ]
    }
   ],
   "source": [
    "# Create an embedding matrix for our vocabulary\n",
    "def create_embedding_matrix(vocab, embeddings_index, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "    for word, i in word_to_idx.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words found in GloVe are added to the embedding matrix\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            # Words not found in GloVe are initialized randomly\n",
    "            embedding_matrix[i] = np.random.uniform(-0.1, 0.1, embedding_dim)\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(vocab, embeddings_index, embedding_dim)\n",
    "\n",
    "# Print shape of the embedding matrix\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eb397ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">402,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">91,392</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4028</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,035,196</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │       \u001b[38;5;34m402,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_2 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │        \u001b[38;5;34m91,392\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m4028\u001b[0m)             │     \u001b[38;5;34m1,035,196\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,529,388</span> (5.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,529,388\u001b[0m (5.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,529,388</span> (5.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,529,388\u001b[0m (5.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the RNN model with GloVe embeddings\n",
    "def build_rnn_model_with_glove(vocab_size, embedding_dim, rnn_units, embedding_matrix):\n",
    "    inputs = Input(shape=(None,), batch_size=batch_size)\n",
    "\n",
    "    # Embedding layer\n",
    "    embedding_layer = Embedding(vocab_size, embedding_dim,\n",
    "                                embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "                                trainable=False)(inputs)\n",
    "\n",
    "    # RNN layer\n",
    "    rnn_layer = SimpleRNN(rnn_units, stateful=True, recurrent_initializer='glorot_uniform')(embedding_layer)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(vocab_size)(rnn_layer)\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Build the model using GloVe embeddings\n",
    "model = build_rnn_model_with_glove(vocab_size, embedding_dim, rnn_units, embedding_matrix)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "047b115a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (64, 100), Target shape: (64,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 02:47:03.269838: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Create sequences of fixed length\n",
    "def create_sequences(token_indices, seq_length):\n",
    "    sequences = []\n",
    "    next_words = []\n",
    "    for i in range(0, len(token_indices) - seq_length):\n",
    "        sequences.append(token_indices[i:i + seq_length])\n",
    "        next_words.append(token_indices[i + seq_length])  # This is the target (next word)\n",
    "    \n",
    "    return np.array(sequences), np.array(next_words)\n",
    "\n",
    "sequences, next_words = create_sequences(token_indices, seq_length)\n",
    "\n",
    "# Reshape and prepare for model\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences, next_words))\n",
    "dataset = dataset.shuffle(buffer_size=len(sequences)).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "# Example: Print shape of dataset\n",
    "for input_seq, target_seq in dataset.take(1):\n",
    "    print(f\"Input shape: {input_seq.shape}, Target shape: {target_seq.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5e3af15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 50ms/step - loss: 6.4779\n",
      "Epoch 2/10\n",
      "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 51ms/step - loss: 5.2396\n",
      "Epoch 3/10\n",
      "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 51ms/step - loss: 4.6226\n",
      "Epoch 4/10\n",
      "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 51ms/step - loss: 4.0837\n",
      "Epoch 5/10\n",
      "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 53ms/step - loss: 3.5752\n",
      "Epoch 6/10\n",
      "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 59ms/step - loss: 3.1523\n",
      "Epoch 7/10\n",
      "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - loss: 2.8223\n",
      "Epoch 8/10\n",
      "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 61ms/step - loss: 2.4332\n",
      "Epoch 9/10\n",
      "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 60ms/step - loss: 2.1427\n",
      "Epoch 10/10\n",
      "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - loss: 1.9229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "epochs = 10\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(dataset, epochs=epochs)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('rnn_model_with_glove.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2eb7d25e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Generate text based on a question\u001b[39;00m\n\u001b[1;32m     34\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwho are the key characters in the story?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 35\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, start_string, num_generate)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_text\u001b[39m(model, start_string, num_generate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Convert start string to numbers (tokenization)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     input_eval \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mword_to_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstart_string\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m     input_eval \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(input_eval)[np\u001b[38;5;241m.\u001b[39mnewaxis, :]  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Generate text\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_text\u001b[39m(model, start_string, num_generate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Convert start string to numbers (tokenization)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     input_eval \u001b[38;5;241m=\u001b[39m [\u001b[43mword_to_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m start_string\u001b[38;5;241m.\u001b[39msplit()]\n\u001b[1;32m      6\u001b[0m     input_eval \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(input_eval)[np\u001b[38;5;241m.\u001b[39mnewaxis, :]  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Generate text\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'key'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_text(model, start_string, num_generate=100):\n",
    "    # Convert start string to numbers (tokenization)\n",
    "    input_eval = [word_to_idx[s] for s in start_string.split()]\n",
    "    input_eval = np.array(input_eval)[np.newaxis, :]  # Add batch dimension\n",
    "\n",
    "    # Generate text\n",
    "    text_generated = []\n",
    "\n",
    "    # Temperature parameter for randomness\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    predictions = model(input_eval)\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "    # Sample a word from the predictions\n",
    "    predicted_id = tf.random.categorical(predictions[-1, :], num_samples=1)[-1, 0].numpy()\n",
    "    text_generated.append(idx_to_word[predicted_id])\n",
    "\n",
    "    for _ in range(num_generate):\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)  # Reshape for the model\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # Sample a new word\n",
    "        predicted_id = tf.random.categorical(predictions[-1, :], num_samples=1)[-1, 0].numpy()\n",
    "        text_generated.append(idx_to_word[predicted_id])\n",
    "\n",
    "    return ' '.join(text_generated)\n",
    "\n",
    "# Generate text based on a question\n",
    "question = \"who are the key characters in the story?\"\n",
    "response = generate_text(model, question, num_generate=50)\n",
    "print(f\"Model Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "290688d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word-to-index and index-to-word mappings\n",
    "words = list(set(token_indices))  # Assuming token_indices is your list of words\n",
    "word_to_idx = {word: index for index, word in enumerate(words)}\n",
    "word_to_idx['<UNK>'] = len(word_to_idx)  # Add an unknown token\n",
    "idx_to_word = {index: word for word, index in word_to_idx.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76e4f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 02:56:11.443339: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: Can not squeeze dim[0], expected a dimension of 1, got 64\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Squeeze_device_/job:localhost/replica:0/task:0/device:CPU:0}} Can not squeeze dim[0], expected a dimension of 1, got 64 [Op:Squeeze] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Example of generating text\u001b[39;00m\n\u001b[1;32m     45\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho are the key characters in the story?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 46\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 25\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, start_string, num_generate)\u001b[0m\n\u001b[1;32m     22\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(input_eval)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Squeeze only if the output has size 1 in that dimension\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# This will work if you have a batch size of 1 or no batch dimension\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Sample a word from the predictions\u001b[39;00m\n\u001b[1;32m     28\u001b[0m predicted_id \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mcategorical(predictions[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:5983\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5982\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5983\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Squeeze_device_/job:localhost/replica:0/task:0/device:CPU:0}} Can not squeeze dim[0], expected a dimension of 1, got 64 [Op:Squeeze] name: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_text(model, start_string, num_generate=100):\n",
    "    # Convert start string to numbers (tokenization), handle unknown words\n",
    "    input_eval = []\n",
    "    for word in start_string.split():\n",
    "        input_eval.append(word_to_idx.get(word, word_to_idx['<UNK>']))  # Use <UNK> index for unknown words\n",
    "\n",
    "    input_eval = np.array(input_eval)[np.newaxis, :]  # Add batch dimension\n",
    "\n",
    "    # Generate text\n",
    "    text_generated = []\n",
    "\n",
    "    # Temperature parameter for randomness\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Do not reset states if the model is stateless\n",
    "    # model.reset_states()  # Only if using stateful LSTM\n",
    "\n",
    "    # Use model to predict\n",
    "    predictions = model(input_eval)\n",
    "\n",
    "    # Squeeze only if the output has size 1 in that dimension\n",
    "    predictions = tf.squeeze(predictions, axis=0)  # This will work if you have a batch size of 1 or no batch dimension\n",
    "\n",
    "    # Sample a word from the predictions\n",
    "    predicted_id = tf.random.categorical(predictions[-1, :], num_samples=1)[-1, 0].numpy()\n",
    "    text_generated.append(idx_to_word[predicted_id])\n",
    "\n",
    "    for _ in range(num_generate):\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)  # Reshape for the model\n",
    "        predictions = model(input_eval)\n",
    "\n",
    "        # We don't squeeze here since we want to keep the batch dimension\n",
    "        predictions = tf.squeeze(predictions, axis=0)\n",
    "\n",
    "        # Sample a new word\n",
    "        predicted_id = tf.random.categorical(predictions[-1, :], num_samples=1)[-1, 0].numpy()\n",
    "        text_generated.append(idx_to_word[predicted_id])\n",
    "\n",
    "    return ' '.join(text_generated)\n",
    "\n",
    "# Example of generating text\n",
    "question = \"Who are the key characters in the story?\"\n",
    "response = generate_text(model, question, num_generate=50)\n",
    "print(f\"Model Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab2887cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 22\n",
      "X shape: (20, 2)\n",
      "y shape: (20,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized keyword arguments passed to Embedding: {'input_length': 2}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(Dense(vocabulary_size, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m---> 63\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocabulary_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     65\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[0;32mIn[25], line 58\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(vocabulary_size, embedding_dim, rnn_units)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model\u001b[39m(vocabulary_size, embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, rnn_units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     57\u001b[0m     model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m---> 58\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocabulary_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     59\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(SimpleRNN(units\u001b[38;5;241m=\u001b[39mrnn_units))\n\u001b[1;32m     60\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(Dense(vocabulary_size, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:81\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero, lora_rank, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     72\u001b[0m     input_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     80\u001b[0m ):\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m=\u001b[39m output_dim\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:264\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_shape_arg \u001b[38;5;241m=\u001b[39m input_shape_arg\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized keyword arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast \u001b[38;5;241m=\u001b[39m autocast\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized keyword arguments passed to Embedding: {'input_length': 2}"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)  # Replace multiple spaces with a single space\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", text)  # Keep only certain characters\n",
    "    return text.lower()  # Convert to lowercase\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "From fairest creatures we desire increase. \n",
    "That thereby beauty’s rose might never die. \n",
    "But as the riper should by time decease.\n",
    "\"\"\"\n",
    "\n",
    "# Preprocess the text\n",
    "text_cleaned = preprocess(text)\n",
    "tokens = text_cleaned.split()  # Simple tokenization\n",
    "\n",
    "# Step 2: Create word-to-index and index-to-word mappings\n",
    "word_to_index = {word: i for i, word in enumerate(set(tokens))}\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "\n",
    "vocabulary_size = len(word_to_index)\n",
    "print(\"Vocabulary Size:\", vocabulary_size)\n",
    "\n",
    "# Step 3: Prepare Training Data with Trigrams\n",
    "def create_ngrams(tokens, n=3):\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def create_sequences(tokens, n=3):\n",
    "    sequences = create_ngrams(tokens, n)\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for seq in sequences:\n",
    "        X.append([word_to_index[word] for word in seq[:-1]])  # First two words as input\n",
    "        y.append(word_to_index[seq[-1]])  # Last word as output\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sequences(tokens, n=3)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# Convert y to categorical\n",
    "y = to_categorical(y, num_classes=vocabulary_size)\n",
    "\n",
    "# Step 4: Build the RNN Model\n",
    "def create_model(vocabulary_size, embedding_dim=50, rnn_units=100):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=X.shape[1]))\n",
    "    model.add(SimpleRNN(units=rnn_units))\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "model = create_model(vocabulary_size)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Step 5: Train the Model\n",
    "epochs = 100\n",
    "model.fit(X, y, batch_size=32, epochs=epochs)\n",
    "\n",
    "# Step 6: Generate Text\n",
    "def generate_text(model, seed_text, n_words, word_to_index, index_to_word):\n",
    "    for _ in range(n_words):\n",
    "        tokenized_seed = [word_to_index[word] for word in seed_text.split()]\n",
    "        tokenized_seed = pad_sequences([tokenized_seed], maxlen=X.shape[1], padding='pre')\n",
    "        predicted_probs = model.predict(tokenized_seed, verbose=0)[0]\n",
    "        \n",
    "        # Sample a word based on probabilities\n",
    "        sampled_word = np.random.choice(vocabulary_size, p=predicted_probs)\n",
    "        \n",
    "        # Add sampled word to the seed text\n",
    "        seed_text += ' ' + index_to_word[sampled_word]\n",
    "    \n",
    "    return seed_text\n",
    "\n",
    "# Generate text starting with a specific seed\n",
    "seed_text = 'from fairest'  # Change this to your desired start words\n",
    "generated_text = generate_text(model, seed_text, 10, word_to_index, index_to_word)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb954c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
