{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59dff3f8",
   "metadata": {},
   "source": [
    "Assignment - 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cfafe4",
   "metadata": {},
   "source": [
    "# Text Generation using RNNs\n",
    "\n",
    "In this notebook, we will explore how to build and train a Recurrent Neural Network (RNN) to generate text based on a corpus. We will use a trigram approach for input and output sequence generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ed050",
   "metadata": {},
   "source": [
    "#### Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab072c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba386f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/sudarshan/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK model data (you need to do this once)\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc816b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "\n",
    "In this section, we preprocess the text data by:\n",
    "- Removing unnecessary characters and multiple spaces.\n",
    "- Converting the text to lowercase for consistency.\n",
    "\n",
    "### Steps:\n",
    "1. Load the raw text data.\n",
    "2. Apply regex for cleaning.\n",
    "3. Tokenize the text into individual words.\n",
    "\n",
    "```python\n",
    "# Example Python code for preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a748789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_roman_numerals(text):\n",
    "    pattern = r\"\\b(?=[MDCLXVIΙ])M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})([IΙ]X|[IΙ]V|V?[IΙ]{0,3})\\b\\.?\"\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d26b31b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading txt file...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk import tokenize\n",
    "\n",
    "#alphabets= \"([A-Za-z])\"\n",
    "#prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "#suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "#starters = \"(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "#acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "#websites = \"[.](com|net|org|io|gov|edu|me)\"\n",
    "#digits = \"([0-9])\"\n",
    "\n",
    "# If you want to restrict the size of the voabulary\n",
    "# Right now, we set it in the next cell to be the entire vocabular: vocabulary_size = len(word_freq.items())\n",
    "#vocabulary_size = 3000\n",
    "\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "text = ''\n",
    "print( \"Reading txt file...\")\n",
    "with open(r'data/siddhartha.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text = text.replace(\",\\n\",\"\\n\")\n",
    "\n",
    "# too many commas if i do this\n",
    "#text = text.replace(\",\",\" ,\")\n",
    "#text = text.replace(\":\",\" ,\")\n",
    "#text = text.replace(\";\",\" ,\")\n",
    "\n",
    "#.. so i do this instead\n",
    "text = text.replace(\",\",\"\")\n",
    "text = text.replace(\":\",\"\")\n",
    "text = text.replace(\";\",\"\")\n",
    "\n",
    "# too many apostrophes in shakespeare\n",
    "text = text.replace(\"’\",\"\")\n",
    "\n",
    "text = text.replace(\"?\\n\",\".\\n\")\n",
    "text = text.replace(\"!\\n\",\".\\n\")\n",
    "text = text.replace(\"?\",\"\")\n",
    "text = text.replace(\"!\",\"\")\n",
    "#text = text.replace(\"\\n\",\" \")\n",
    "\n",
    "text = text.replace('I ', 'i ')\n",
    "text = clean_roman_numerals(text)\n",
    "#text = text.replace('&', '')\n",
    "\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "text = _RE_COMBINE_WHITESPACE.sub(\" \", text).strip()\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dcd8218",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()\n",
    "text = text.replace('i ', 'I ')\n",
    "\n",
    "leftovers = ['ii', 'iii', 'cxi', 'cx', 'cxx', 'xx', 'xxxvi', 'xxxvi', 'xxxv', 'xxxi', 'xxi', 'cvi ', 'ci ', 'xvi ', 'lxi ', \n",
    "             'lxv','lxvi', 'lxxi', 'lxxvi', 'lxxvi', 'lxxv', 'lxxxi', 'cxxxi', 'cxxxi', 'cxxx', 'cxli', 'cxlvi', 'cxvl', \n",
    "             'cli ', 'cl ', 'cxxxvi','cvi ', 'cv ', 'ci ', 'cx ', 'cxx', 'cxi', 'li ' , 'xxx', 'xxvi', 'xxv', 'cxv', 'xci', \n",
    "             'xli', 'lxvi', 'lxi ', ' c ', 'lxxxvi', 'lxxxvi', 'lxxxv', ' v ', 'vi ', ' l ', 'lvi ', 'lv ', 'xlv ', ' x ', \n",
    "             'xi ', 'xl ', 'ix ']\n",
    "for rn in leftovers:\n",
    "    text = text.replace(rn, '')\n",
    "\n",
    "text = text.replace('.  ', '. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17508643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he saw merchants trading princes hunting mourners wailing for their dead whores offering themselves physicians trying to help the sick priests determining the most suitable day for seeding lovers loving mothers nursing their children—and all of this was not worthy of one look from his eye it all lied it all stank it all stank of lies it all pretended to be meaningful and joyful and beautiful and it all was just concealed putrefaction.\n",
      "\n",
      "the world tasted bitter.\n",
      "\n",
      "life was torture.\n",
      "\n",
      "a goal stood before siddhartha a single goal to become empty empty of thirst empty of wishing empty of dreams empty of joy and sorrow.\n",
      "\n",
      "dead to himself not to be a self any more to find tranquility with an emptied heart to be open to miracles in unselfish thoughts that was his goal.\n",
      "\n",
      "once all of my self was overcome and had died once every desire and every urge was silent in the heart then the ultimate part of me had to awake the innermost of my being which is no longer my self the great secret.\n",
      "\n",
      "silently siddhartha exposed himself to burning rays of the sun directly above glowing with pain glowing with thirst and stood there until he neither felt any pain nor thirst any more.\n",
      "\n",
      "silently he stood there in the rainy season from his hair the water was dripping over freezing shoulders over freezing hips and legs and the penitent stood there until he could not feel the cold in his shoulders and legs any more until they were silent until they were quiet.\n",
      "\n",
      "silently he cowered in the thorny bushes blood dripped from the burning skin from festering wounds dripped pus and siddhartha stayed rigidly stayed motionless until no blood flowed any more until nothing stung any more until nothing burned any more.\n",
      "\n",
      "siddhartha sat upright and learned to breathe sparingly learned to get along with only few breathes learned to stop breathing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = tokenize.sent_tokenize(text)\n",
    "for i in range(100, 110):\n",
    "    print(sentences[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5cb83e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 40000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d6f07",
   "metadata": {},
   "source": [
    "### 2. Creating Word Mappings\n",
    "Here, we convert the cleaned text into numerical form by creating two dictionaries:\n",
    "\n",
    "word_to_index: Maps each word to a unique index.\n",
    "index_to_word: Reverse mapping to retrieve words from their corresponding indices.\n",
    "This allows us to prepare the data for model training.\n",
    "### Example code for word mappings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "44c2bd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 1476 sentences.\n",
      "Found 4089 unique words tokens.\n",
      "The least frequent word in our vocabulary is 'newsletter' and appeared 1 times.\n",
      "Using vocabulary size 4089.\n",
      "\n",
      "Example sentence: 'SENTENCE_START SENTENCE_START the project gutenberg ebook of siddhartha this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever SENTENCE_EN SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'SENTENCE_START', 'the', 'project', 'gutenberg', 'ebook', 'of', 'siddhartha', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'SENTENCE_EN', 'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "# Append SENTENCE_START and SENTENCE_END\n",
    "sentences = [\"%s %s %s\" % (sentence_start_token, x[:-1].replace(\"&\",\"\"), sentence_end_token) for x in sentences] \n",
    "print(  \"Parsed %d sentences.\" % (len(sentences)))\n",
    "\n",
    "# Tokenize the sentences into words, making sure to remove end-of-sentence period\n",
    "tokenized_sentences = [nltk.word_tokenize(sent.replace('.', '')) for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(  \"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "#for i, sent in enumerate(tokenized_sentences):\n",
    "#    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "vocabulary_size = len(word_freq.items())\n",
    "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "\n",
    "print(  \"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print(  \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7a418d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SENTENCE_START', 2952),\n",
       " ('the', 2239),\n",
       " ('SENTENCE_EN', 1476),\n",
       " ('SENTENCE_END', 1476),\n",
       " ('and', 1423),\n",
       " ('to', 1226),\n",
       " ('of', 1112),\n",
       " ('a', 967),\n",
       " ('he', 935),\n",
       " ('his', 708),\n",
       " ('in', 689),\n",
       " ('had', 527),\n",
       " ('was', 512),\n",
       " ('this', 491),\n",
       " ('it', 484),\n",
       " ('you', 462),\n",
       " ('him', 458),\n",
       " ('with', 411),\n",
       " ('I', 395),\n",
       " ('“', 383)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "94a82c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SENTENCE_START SENTENCE_START the project gutenberg ebook of siddhartha this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever SENTENCE_EN SENTENCE_END',\n",
       " 'SENTENCE_START SENTENCE_START you may copy it give it away or re-use it under the terms of the project gutenberg license included with this ebook or online at www.gutenberg.org SENTENCE_EN SENTENCE_END',\n",
       " 'SENTENCE_START SENTENCE_START if you are not located in the united states you will have to check the laws of the country where you are located before using this ebook SENTENCE_EN SENTENCE_END',\n",
       " 'SENTENCE_START SENTENCE_START title siddhartha author hermann hesse release date february 1 2001 [ebook #2500] most recently updated december 22 2021 language english credits michael pullen chandra yenco and isaac jones *** start of the project gutenberg ebook siddhartha *** siddhartha an indian tale by herman hesse * * * contents first part the son of the brahman with the samanas gotama awakening second part kamala with the childlike people sansara by the river the ferryman the son om govinda first part to romain rolland my dear friend the son of the brahman in the shade of the house in the sunshine of the riverbank near the boats in the shade of the sal-wood forest in the shade of the fig tree is where siddhartha grew up the handsome son of the brahman the young falcon together with his friend govinda son of a brahman SENTENCE_EN SENTENCE_END',\n",
       " 'SENTENCE_START SENTENCE_START the sun tanned his light shoulders by the banks of the river when bathing performing the sacred ablutions the sacred offerings SENTENCE_EN SENTENCE_END']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da152e7",
   "metadata": {},
   "source": [
    "### 3. Preparing Trigrams and Sequences\n",
    "We now prepare the input sequences (bigrams) and the target word (third word) using trigrams. The process involves:\n",
    "\n",
    "Creating sequences of n-grams (specifically trigrams).\n",
    "Mapping each word in the sequence to its index.\n",
    "### Example code for creating n-grams and sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "39ed19ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 ms, sys: 4.77 ms, total: 15.9 ms\n",
      "Wall time: 37.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 269),\n",
       " (('in', 'the'), 215),\n",
       " (('he', 'had'), 187),\n",
       " (('to', 'the'), 111),\n",
       " (('of', 'his'), 87),\n",
       " (('to', 'be'), 85),\n",
       " (('in', 'his'), 77),\n",
       " (('and', 'the'), 77),\n",
       " (('I', 'have'), 73),\n",
       " (('for', 'a'), 72)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "bigram_counts = Counter(ngrams(text.split(), 2))\n",
    "bigram_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b39910cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.4 ms, sys: 2.22 ms, total: 21.6 ms\n",
      "Wall time: 41.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 269),\n",
       " (('in', 'the'), 215),\n",
       " (('he', 'had'), 187),\n",
       " (('to', 'the'), 111),\n",
       " (('of', 'his'), 87),\n",
       " (('to', 'be'), 85),\n",
       " (('in', 'his'), 77),\n",
       " (('and', 'the'), 77),\n",
       " (('I', 'have'), 73),\n",
       " (('for', 'a'), 72)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import collections\n",
    "def ngrams(text, n=2):\n",
    "    return zip(*[text[i:] for i in range(n)])\n",
    "bigram_counts = collections.Counter(ngrams(text.split(), 2))\n",
    "bigram_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "48912b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the project gutenberg ebook of siddhartha this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever. you may copy it give it away or re-use it under the terms of the project gutenberg license included with this ebook or online at www.gutenberg.org. if you are not located in the united states you will have to check the laws of the country where you are located before using this ebook. title siddhartha author hermann hesse release date february 1 2001 [ebook #2500] most recently updated december 22 2021 language english credits michael pullen chandra yenco and isaac jones *** start of the project gutenberg ebook siddhartha *** siddhartha an indian tale by herman hesse * * * contents first part the son of the brahman with the samanas gotama awakening second part kamala with the childlike people sansara by the river the ferryman the son om govinda first part to romain rolland my dear friend the '"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2ccad6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.”', 159),\n",
       " ('but', 117),\n",
       " ('he', 104),\n",
       " ('and', 68),\n",
       " ('the', 63),\n",
       " ('I', 55),\n",
       " ('siddhartha', 45),\n",
       " ('it', 40),\n",
       " ('when', 31),\n",
       " ('for', 30)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_word_counts = Counter([ p.replace('. ', '') for p in re.findall('\\..[^\" \"]*', text)])\n",
    "first_word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "edd67c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = [[sentence_start_token] for sent,times in first_word_counts if sent != 'o.']\n",
    "#y_train = [sent for sent in first_word_counts if sent != 'o.']\n",
    "X_train = [[sentence_start_token]*c for sent,c in first_word_counts.items() if sent != 'o.']\n",
    "y_train = [[sent]*c for sent,c in first_word_counts.items() if sent != 'o.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "20f09c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [item for sublist in X_train for item in sublist]\n",
    "y_train = [item for sublist in y_train for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c3baf489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SENTENCE_START',\n",
       " 'SENTENCE_START',\n",
       " 'SENTENCE_START',\n",
       " 'SENTENCE_START',\n",
       " 'SENTENCE_START',\n",
       " 'SENTENCE_START',\n",
       " 'SENTENCE_START',\n",
       " 'SENTENCE_START',\n",
       " 'SENTENCE_START',\n",
       " 'SENTENCE_START']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c4f08497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '.gutenberg.org.', '.gutenberg.org.', '.gutenberg.org.', '.gutenberg.org.', 'title', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'joy', 'bliss', 'love', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'govinda', 'govinda', 'govinda', 'govinda', 'govinda', 'govinda', 'govinda', 'govinda', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'siddhartha', 'walking', 'dreams', 'so', 'so', 'so', 'so', 'so', 'so', 'surely', 'surely', '“your', 'marvellous', '.—but', '.—but', '.—but', 'his', 'his', 'his', 'his', 'his', 'his', 'thus', 'thus', 'thus', 'thus', 'thus', 'thus', 'thus', 'thus', 'thus', 'thus', 'often', 'often', 'often', 'often', 'often', 'often', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '.”', '“govinda”', 'while', 'while', 'while', 'while', 'while', 'after', 'after', 'after', 'after', 'after', 'after', 'after', 'after', 'after', 'once', 'once', 'once', 'once', 'once', 'behind', 'soon', '“o', '“o', 'arrow-fast', 'tomorrow', 'tomorrow', 'speak', 'quoth', 'quoth', 'quoth', 'quoth', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'my', 'my', 'my', 'my', 'may', 'may', 'may', 'silent', 'then', 'then', 'then', 'then', 'then', 'then', 'then', 'then', 'then', 'then', 'then', 'then', 'then', 'then', 'then', 'then', 'then', 'then', 'then', 'then', 'then', '“what', 'through', 'through', 'through', 'pale', 'pale', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', '“siddhartha”', '“siddhartha”', '“siddhartha”', '“you', '“you', '“you', '“you', '“you', '“you', '“you', '“you', '“you', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'go', 'as', 'as', 'as', 'as', 'as', 'as', '“I', '“I', '“I', '“I', '“I', '“I', '“I', '“I', '“I', '“I', '“I', '“I', '“I', '“I', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'feverish', 'life', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'dead', 'dead', 'dead', 'silently', 'silently', 'silently', 'silently', 'instructed', 'these', 'these', 'these', 'these', 'these', 'though', 'though', 'by', 'by', 'by', 'by', 'by', 'by', 'occasionally', '“how', '“how', '“how', '“how', 'youll', 'youll', 'youll', 'youll', 'quickly', 'quickly', 'one', 'one', 'one', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'its', 'we', 'we', 'we', 'we', 'we', 'we', 'hell', 'hell', 'oh', 'oh', 'im', 'im', 'im', 'im', 'im', 'im', 'im', 'im', 'im', 'im', 'perhaps', 'perhaps', 'perhaps', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'yes', 'yes', 'yes', 'yes', 'at', 'at', 'at', 'at', 'at', 'at', 'at', 'at', 'many', 'many', 'many', 'many', 'many', 'many', 'many', 'many', 'many', 'everywhere', '“oh', '“oh', '“today', '“today', 'verily', 'mock', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'truly', 'truly', 'truly', '“let', 'near', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'all', '“look', '“this', '“this', '“this', '“this', 'never', 'never', 'never', 'never', 'never', 'never', 'never', 'gotama', 'calmly', 'suffering', 'right', 'always', 'together', 'together', '“too', 'does', 'everything', 'everything', 'everything', 'whether', 'whether', 'please', 'please', 'now', 'now', 'now', 'now', 'now', 'now', 'now', 'now', 'now', 'now', 'now', 'now', 'now', 'now', 'now', 'now', 'now', 'youve', 'youve', 'youve', 'only', 'only', 'salvation', 'be', 'be', 'be', 'be', 'be', 'well', 'awakening', 'slowly', 'slowly', 'slowly', 'slowly', 'slowly', 'slowly', 'slowly', 'slowly', 'slower', '“oh”', 'neither', 'neither', 'beautiful', 'beautiful', 'beautiful', 'beautiful', 'beautiful', 'blue', '“when', '“when', 'because', 'because', 'because', 'whatever', 'still', 'deeply', 'deeply', 'deeply', 'deeply', 'deeply', 'deeply', 'deeply', 'deeply', 'nobody', 'out', 'second', 'differently', 'differently', 'short', 'light', 'again', 'both', 'why', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'sad', '.—when', '“yes”', 'much', 'much', 'much', 'much', '“surely.', 'commemorate', 'smiling', '“he', 'like', 'like', 'like', 'like', 'like', 'like', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'looking', 'looking', 'politely', 'politely', 'before', 'under', 'pursuing', 'among', 'among', '“werent', '“its', '“its', '“never', 'even', 'clothes', 'clothes', 'do', 'do', 'did', 'did', 'did', '“it', '“it', '“it', '“it', '“it', '“it', '“it', 'would', 'would', 'more', 'kamala', 'kamala', 'kamala', 'kamala', 'kamala', 'kamala', 'kamala', '“beautiful', 'breathing', '“very', '“yes', '“yes', '“youre', '“youre', '“theres', '“hurry', 'without', 'without', 'without', 'contently', 'contently', 'being', 'being', 'suddenly', '“simple', '“things', '“things', '“they', 'kamaswamI', 'kamaswamI', 'kamaswamI', 'kamaswamI', 'kamaswamI', 'kamaswamI', 'kamaswamI', '“youve', '“youve', '“youve', '“but', '“but', '“but', '“but', 'from', 'from', 'from', 'look', 'look', 'look', 'nothing', 'nothing', 'everyone', 'everyone', '“perhaps', 'might', 'wait', '“excellent”', '“and', '“and', '“many', 'twice', 'him', 'wonderful', 'here', 'here', 'nevertheless', 'nevertheless', 'nevertheless', 'nevertheless', 'nevertheless', 'nevertheless', 'besides', 'however', 'however', 'welcome', 'few', '“no”', 'others', 'most', 'most', 'most', 'most', 'thousands', '“again', 'her', 'her', 'youre', 'youre', 'isnt', 'isnt', 'years', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'just', 'just', 'just', 'property', 'ill', 'tiredness', 'tiredness', 'not', 'not', 'not', 'not', 'of', 'of', 'starting', 'worthless', 'alone', 'shivers', 'since', 'since', 'since', 'passionately', 'om', 'om', 'deep', 'quietly', 'quietly', 'quietly', '“however', 'therefore', 'therefore', 'therefore', 'therefore', 'therefore', 'therefore', 'badly', '“permit', '“now', '“now', 'where', 'where', 'where', 'remember', 'thinking', 'was', 'was', 'wondrous', 'wonderfully', 'wonderfully', 'ive', 'ive', 'let', 'let', 'let', 'wherever', 'good', 'had', 'had', 'had', 'too', 'into', 'cheerfully', 'tenderly', 'bright', '“would', '“once', 'wouldnt', '“im', '“im', '“im', 'behold', '“at', 'havent', 'gratefully', 'afterwards', 'until', 'vasudeva', 'vasudeva', 'vasudeva', 'vasudeva', 'vasudeva', 'vasudeva', 'vasudeva', 'vasudeva', 'vasudeva', 'vasudeva', 'vasudeva', 'listening', 'stay', 'see', 'see', 'see', 'see', 'lets', 'lets', 'incessantly', '“did', 'also', 'kindly', 'kindly', 'hurriedly', 'kamalas', '“hes', 'alas', 'alas', 'alas', '“shell', 'pain', 'theyve', 'early', '“no', 'rich', '“pardon', 'your', 'water', '“give', 'tell', 'tell', 'very', 'have', '“often', 'indeed', '“get', '“a', '“a', 'hes', 'hes', 'hes', '“why', '“why', 'either', 'clearly', 'instead', 'sadly', 'instantly', 'worthy', 'unchanged', 'throughout', 'softly', '“do', '“do', '“listen', '“listen', 'already', 'brightly', 'brightly', 'farewell', '“ive', '“youll', 'arent', 'searching', '“are', '.—and', 'sometimes', 'wisdom', 'knowledge', 'time', '.—these', 'or', '.)', '“bend', '“bend', '***', 'creating', '.s.', '.s.', '.s.', '.s.', '.s.', '.s.', 'special', 'project', 'project', 'project', 'redistribution', 'start', '.gutenberg.org/license.', 'general', 'general', '.a.', '.e.8.', '.e.8.', '.b.', 'below.', '.e', '1.', '1.', 'nearly', 'copyright', '1.e.', '.e.1.', '.e.1.', '1.e.2.', '.e.1', '.e.1', '.e.1', '.e.7', '.e.7', '.e.8', '.e.8', '.e.9.', '.e.9.', '.e.3.', 'additional', '1.e.4.', '1.e.5.', '1.e.6.', '.gutenberg.org)', 'any', '.e.7.', 'royalty', 'royalty', '•', '•', '.f.3', '.f.3', '.f.3', '1.e.9.', 'contact', '1.f.', '.f.1.', 'despite', '1.f.2.', '.f.3.', '1.f.3.', '1.f.4.', '1.f.5.', '1.f.6.', 'section', 'information', 'information', 'information', 'volunteers', 'contributions', 'email', '.gutenberg.org/contact', 'compliance', '.gutenberg.org/donate.', '.gutenberg.org/donate.', 'international', 'u.s.', 'donations', 'hart']\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e93ddbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1655, 1655)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8146156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def fisher_yates (arr1, arr2):\n",
    "     \n",
    "    # We will Start from the last element\n",
    "    # and swap one by one.\n",
    "    n = len(arr1)\n",
    "    if n != len(arr2):\n",
    "        return None\n",
    "    \n",
    "    for i in range(n - 1, 0, -1):\n",
    "\n",
    "        # Pick a random index from 0 to i\n",
    "        j = random.randint(0, i)\n",
    "        #print(i, j)\n",
    "\n",
    "        # Swap arr[i] with the element at random index\n",
    "        arr1[i], arr1[j] = arr1[j], arr1[i]\n",
    "        arr2[i], arr2[j] = arr2[j], arr2[i]\n",
    "        \n",
    "    return arr1, arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8f74cec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['b', 'c', 'a'], ['2', '3', '1'])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random as rd\n",
    "one = ['a', 'b', 'c']\n",
    "two = ['1', '2', '3']\n",
    "one, two = fisher_yates(one, two)\n",
    "one, two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f464fbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['c'], ['b'], ['a']], [['3'], ['2'], ['1']])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one = [['a'], ['b'], ['c']]\n",
    "two = [['1'], ['2'], ['3']]\n",
    "one, two = fisher_yates(one, two)\n",
    "one, two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a3f5ecf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1655, 1655)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = fisher_yates(X_train, y_train)\n",
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d8b4a492",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens = [[word_to_index[symbol]] for symbol,word in zip(X_train, y_train) if word in word_to_index]\n",
    "y_tokens = [[word_to_index[word]] for symbol,word in zip(X_train, y_train) if word in word_to_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "18b6cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_tokens\n",
    "y_train = y_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4e222685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1314, 1314)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6b9046b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0], [0], [0], [0], [0]], [[27], [18], [668], [27], [41]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:5], y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d272b623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram- 2 length: 24854\n",
      "ngram- 3 length: 37872\n",
      "ngram- 4 length: 41022\n",
      "ngram- 5 length: 41764\n",
      "ngram- 6 length: 41974\n",
      "ngram- 7 length: 42048\n",
      "ngram- 8 length: 42077\n",
      "ngram- 9 length: 42090\n",
      "ngram- 10 length: 42095\n",
      "ngram- 11 length: 42098\n",
      "ngram- 12 length: 42101\n",
      "ngram- 13 length: 42104\n",
      "ngram- 14 length: 42106\n",
      "ngram- 15 length: 42107\n",
      "ngram- 16 length: 42107\n",
      "ngram- 17 length: 42107\n",
      "ngram- 18 length: 42107\n",
      "ngram- 19 length: 42107\n",
      "ngram- 20 length: 42107\n"
     ]
    }
   ],
   "source": [
    "ngrams_up_to_20 = []\n",
    "for i in range(2, 21):\n",
    "    ngram_counts = Counter(ngrams(text.split(), i))\n",
    "    print('ngram-', i, 'length:', len(ngram_counts))\n",
    "    ngrams_up_to_20.append(ngram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "34724cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_periods(ngram):\n",
    "    for wrd in ngram[0]:\n",
    "        if '.' in wrd or \"’\" in wrd or \"‘\" in wrd:\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "def my_filter(ngrams):\n",
    "    return filter(remove_periods, ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "66cb0d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5130,\n",
       " [(('of', 'the'), 269),\n",
       "  (('in', 'the'), 215),\n",
       "  (('he', 'had'), 187),\n",
       "  (('to', 'the'), 111),\n",
       "  (('of', 'his'), 87),\n",
       "  (('to', 'be'), 85),\n",
       "  (('in', 'his'), 77),\n",
       "  (('and', 'the'), 77),\n",
       "  (('I', 'have'), 73),\n",
       "  (('for', 'a'), 72),\n",
       "  (('from', 'the'), 71),\n",
       "  (('with', 'the'), 70),\n",
       "  (('a', 'long'), 68),\n",
       "  (('he', 'was'), 68),\n",
       "  (('the', 'river'), 67),\n",
       "  (('had', 'been'), 62),\n",
       "  (('by', 'the'), 61),\n",
       "  (('it', 'is'), 61),\n",
       "  (('of', 'a'), 60),\n",
       "  (('to', 'him'), 60),\n",
       "  (('long', 'time'), 57),\n",
       "  (('project', 'gutenberg™'), 55),\n",
       "  (('with', 'a'), 53),\n",
       "  (('of', 'this'), 51),\n",
       "  (('when', 'he'), 50),\n",
       "  (('did', 'not'), 48),\n",
       "  (('at', 'the'), 48),\n",
       "  (('for', 'the'), 46),\n",
       "  (('the', 'world'), 46),\n",
       "  (('into', 'the'), 46),\n",
       "  (('that', 'he'), 46),\n",
       "  (('the', 'same'), 44),\n",
       "  (('had', 'to'), 42),\n",
       "  (('it', 'was'), 41),\n",
       "  (('able', 'to'), 41),\n",
       "  (('in', 'a'), 40),\n",
       "  (('as', 'a'), 39),\n",
       "  (('him', 'and'), 39),\n",
       "  (('on', 'the'), 38),\n",
       "  (('all', 'of'), 35),\n",
       "  (('his', 'heart'), 35),\n",
       "  (('full', 'of'), 35),\n",
       "  (('he', 'saw'), 34),\n",
       "  (('was', 'not'), 34),\n",
       "  (('the', 'project'), 33),\n",
       "  (('and', 'he'), 33),\n",
       "  (('like', 'a'), 33),\n",
       "  (('just', 'as'), 33),\n",
       "  (('out', 'of'), 32),\n",
       "  (('in', 'this'), 32),\n",
       "  (('want', 'to'), 31),\n",
       "  (('was', 'a'), 31),\n",
       "  (('was', 'the'), 31),\n",
       "  (('him', 'to'), 31),\n",
       "  (('the', 'forest'), 31),\n",
       "  (('there', 'was'), 30),\n",
       "  (('this', 'is'), 30),\n",
       "  (('the', 'exalted'), 30),\n",
       "  (('he', 'felt'), 30),\n",
       "  (('which', 'he'), 30),\n",
       "  (('do', 'not'), 29),\n",
       "  (('you', 'have'), 28),\n",
       "  (('with', 'his'), 27),\n",
       "  (('of', 'all'), 27),\n",
       "  (('as', 'if'), 27),\n",
       "  (('and', 'to'), 27),\n",
       "  (('any', 'more'), 27),\n",
       "  (('project', 'gutenberg'), 26),\n",
       "  (('you', 'are'), 26),\n",
       "  (('the', 'samanas'), 26),\n",
       "  (('the', 'teachings'), 26),\n",
       "  (('no', 'longer'), 26),\n",
       "  (('the', 'boy'), 26),\n",
       "  (('that', 'you'), 25),\n",
       "  (('on', 'his'), 25),\n",
       "  (('I', 'am'), 25),\n",
       "  (('through', 'the'), 24),\n",
       "  (('from', 'his'), 24),\n",
       "  (('to', 'his'), 24),\n",
       "  (('to', 'a'), 24),\n",
       "  (('like', 'this'), 24),\n",
       "  (('there', 'is'), 24),\n",
       "  (('I', 'had'), 24),\n",
       "  (('seemed', 'to'), 24),\n",
       "  (('his', 'eyes'), 23),\n",
       "  (('he', 'thought'), 23),\n",
       "  (('the', 'buddha'), 23),\n",
       "  (('had', 'not'), 23),\n",
       "  (('if', 'you'), 22),\n",
       "  (('my', 'dear'), 22),\n",
       "  (('when', 'the'), 22),\n",
       "  (('and', 'his'), 22),\n",
       "  (('who', 'had'), 22),\n",
       "  (('exalted', 'one'), 22),\n",
       "  (('looked', 'at'), 22),\n",
       "  (('his', 'father'), 21),\n",
       "  (('become', 'a'), 21),\n",
       "  (('one', 'of'), 21),\n",
       "  (('and', 'in'), 21),\n",
       "  (('this', 'was'), 21),\n",
       "  (('on', 'a'), 21),\n",
       "  (('to', 'do'), 21),\n",
       "  (('saw', 'the'), 21),\n",
       "  (('he', 'could'), 21),\n",
       "  (('is', 'a'), 21),\n",
       "  (('used', 'to'), 21),\n",
       "  (('siddhartha', 'had'), 20),\n",
       "  (('to', 'become'), 20),\n",
       "  (('wanted', 'to'), 20),\n",
       "  (('and', 'a'), 20),\n",
       "  (('not', 'the'), 20),\n",
       "  (('his', 'own'), 20),\n",
       "  (('a', 'samana'), 20),\n",
       "  (('to', 'me'), 20),\n",
       "  (('he', 'is'), 20),\n",
       "  (('have', 'been'), 20),\n",
       "  (('that', 'the'), 19),\n",
       "  (('was', 'no'), 19),\n",
       "  (('once', 'again'), 19),\n",
       "  (('and', 'had'), 19),\n",
       "  (('learned', 'to'), 19),\n",
       "  (('and', 'was'), 19),\n",
       "  (('what', 'is'), 19),\n",
       "  (('but', 'the'), 19),\n",
       "  (('terms', 'of'), 18),\n",
       "  (('his', 'son'), 18),\n",
       "  (('the', 'most'), 18),\n",
       "  (('is', 'the'), 18),\n",
       "  (('as', 'he'), 18),\n",
       "  (('of', 'him'), 18),\n",
       "  (('gutenberg™', 'electronic'), 18),\n",
       "  (('the', 'ferryman'), 17),\n",
       "  (('as', 'well'), 17),\n",
       "  (('but', 'he'), 17),\n",
       "  (('the', 'grove'), 17),\n",
       "  (('the', 'old'), 17),\n",
       "  (('my', 'friend'), 17),\n",
       "  (('the', 'first'), 17),\n",
       "  (('if', 'he'), 17),\n",
       "  (('into', 'a'), 17),\n",
       "  (('away', 'from'), 17),\n",
       "  (('which', 'had'), 17),\n",
       "  (('come', 'to'), 17),\n",
       "  (('he', 'did'), 17),\n",
       "  (('learned', 'from'), 17),\n",
       "  (('the', 'city'), 17),\n",
       "  (('order', 'to'), 17),\n",
       "  (('in', 'order'), 17),\n",
       "  (('have', 'to'), 16),\n",
       "  (('the', 'brahman'), 16),\n",
       "  (('all', 'the'), 16),\n",
       "  (('was', 'it'), 16),\n",
       "  (('which', 'was'), 16),\n",
       "  (('and', 'of'), 16),\n",
       "  (('his', 'life'), 16),\n",
       "  (('in', 'him'), 16),\n",
       "  (('part', 'of'), 16),\n",
       "  (('is', 'not'), 16),\n",
       "  (('and', 'yet'), 16),\n",
       "  (('a', 'smile'), 16),\n",
       "  (('I', 'was'), 16),\n",
       "  (('to', 'you'), 16),\n",
       "  (('now', 'he'), 16),\n",
       "  (('how', 'to'), 15),\n",
       "  (('to', 'learn'), 15),\n",
       "  (('came', 'to'), 15),\n",
       "  (('him', 'the'), 15),\n",
       "  (('about', 'the'), 15),\n",
       "  (('quoth', 'siddhartha'), 15),\n",
       "  (('that', 'it'), 15),\n",
       "  (('in', 'my'), 15),\n",
       "  (('be', 'a'), 15),\n",
       "  (('could', 'not'), 15),\n",
       "  (('do', 'you'), 15),\n",
       "  (('oh', 'govinda'), 15),\n",
       "  (('then', 'he'), 15),\n",
       "  (('reached', 'the'), 15),\n",
       "  (('so', 'many'), 15),\n",
       "  (('but', 'siddhartha'), 15),\n",
       "  (('he', 'has'), 15),\n",
       "  (('like', 'to'), 15),\n",
       "  (('the', 'hut'), 15),\n",
       "  (('the', 'river.'), 15),\n",
       "  (('the', 'united'), 14),\n",
       "  (('and', 'with'), 14),\n",
       "  (('his', 'friend'), 14),\n",
       "  (('into', 'his'), 14),\n",
       "  (('who', 'was'), 14),\n",
       "  (('up', 'to'), 14),\n",
       "  (('he', 'would'), 14),\n",
       "  (('not', 'a'), 14),\n",
       "  (('siddhartha', 'the'), 14),\n",
       "  (('him', 'as'), 14),\n",
       "  (('the', 'water'), 14),\n",
       "  (('one', 'and'), 14),\n",
       "  (('and', 'it'), 14),\n",
       "  (('where', 'the'), 14),\n",
       "  (('such', 'a'), 14),\n",
       "  (('river', 'and'), 14),\n",
       "  (('a', 'single'), 14),\n",
       "  (('that', 'I'), 14),\n",
       "  (('the', 'path'), 14),\n",
       "  (('that', 'this'), 14),\n",
       "  (('him', 'he'), 14),\n",
       "  (('the', 'other'), 14),\n",
       "  (('this', 'very'), 14),\n",
       "  (('time', 'he'), 14),\n",
       "  (('was', 'still'), 14),\n",
       "  (('had', 'learned'), 14),\n",
       "  (('electronic', 'works'), 14),\n",
       "  (('this', 'agreement'), 14),\n",
       "  (('the', 'terms'), 13),\n",
       "  (('the', 'young'), 13),\n",
       "  (('they', 'were'), 13),\n",
       "  (('to', 'live'), 13),\n",
       "  (('it', 'seemed'), 13),\n",
       "  (('he', 'looked'), 13),\n",
       "  (('to', 'him.'), 13),\n",
       "  (('a', 'new'), 13),\n",
       "  (('is', 'what'), 13),\n",
       "  (('and', 'that'), 13),\n",
       "  (('you', 'and'), 13),\n",
       "  (('said', 'to'), 13),\n",
       "  (('of', 'these'), 13),\n",
       "  (('venerable', 'one'), 13),\n",
       "  (('his', 'face'), 13),\n",
       "  (('but', 'I'), 13),\n",
       "  (('would', 'be'), 13),\n",
       "  (('listened', 'to'), 13),\n",
       "  (('be', 'able'), 13),\n",
       "  (('so', 'it'), 13),\n",
       "  (('which', 'I'), 13),\n",
       "  (('by', 'a'), 13),\n",
       "  (('at', 'him'), 13),\n",
       "  (('gutenberg', 'literary'), 13),\n",
       "  (('literary', 'archive'), 13),\n",
       "  (('with', 'this'), 12),\n",
       "  (('more', 'than'), 12),\n",
       "  (('would', 'not'), 12),\n",
       "  (('siddhartha', 'was'), 12),\n",
       "  (('the', 'night'), 12),\n",
       "  (('him.', 'he'), 12),\n",
       "  (('had', 'already'), 12),\n",
       "  (('they', 'had'), 12),\n",
       "  (('he', 'knew'), 12),\n",
       "  (('of', 'them'), 12),\n",
       "  (('to', 'tell'), 12),\n",
       "  (('looked', 'into'), 12),\n",
       "  (('siddhartha', 'and'), 12),\n",
       "  (('which', 'is'), 12),\n",
       "  (('and', 'siddhartha'), 12),\n",
       "  (('to', 'get'), 12),\n",
       "  (('have', 'learned'), 12),\n",
       "  (('on', 'this'), 12),\n",
       "  (('a', 'man'), 12),\n",
       "  (('it', 'and'), 12),\n",
       "  (('had', 'also'), 12),\n",
       "  (('you', 'would'), 12),\n",
       "  (('never', 'before'), 12),\n",
       "  (('him', 'who'), 12),\n",
       "  (('I', 'do'), 12),\n",
       "  (('to', 'have'), 12),\n",
       "  (('it', 'in'), 12),\n",
       "  (('can', 'be'), 12),\n",
       "  (('as', 'it'), 12),\n",
       "  (('thought', 'of'), 12),\n",
       "  (('the', 'boat'), 12),\n",
       "  (('united', 'states'), 11),\n",
       "  (('under', 'the'), 11),\n",
       "  (('you', 'will'), 11),\n",
       "  (('his', 'soul'), 11),\n",
       "  (('he', 'who'), 11),\n",
       "  (('the', 'brahmans'), 11),\n",
       "  (('and', 'said'), 11),\n",
       "  (('me', 'and'), 11),\n",
       "  (('did', 'he'), 11),\n",
       "  (('to', 'know'), 11),\n",
       "  (('he', 'spoke'), 11),\n",
       "  (('go', 'to'), 11),\n",
       "  (('a', 'brahman'), 11),\n",
       "  (('and', 'there'), 11),\n",
       "  (('young', 'man'), 11),\n",
       "  (('would', 'you'), 11),\n",
       "  (('me', 'to'), 11),\n",
       "  (('said', 'siddhartha'), 11),\n",
       "  (('is', 'no'), 11),\n",
       "  (('he', 'learned'), 11),\n",
       "  (('over', 'the'), 11),\n",
       "  (('across', 'the'), 11),\n",
       "  (('his', 'self'), 11),\n",
       "  (('a', 'lot'), 11),\n",
       "  (('a', 'child'), 11),\n",
       "  (('about', 'it'), 11),\n",
       "  (('eyes', 'and'), 11),\n",
       "  (('I', 'would'), 11),\n",
       "  (('to', 'see'), 11),\n",
       "  (('front', 'of'), 11),\n",
       "  (('in', 'their'), 11),\n",
       "  (('has', 'been'), 11),\n",
       "  (('him', 'with'), 11),\n",
       "  (('and', 'from'), 11),\n",
       "  (('was', 'nothing'), 11),\n",
       "  (('nothing', 'but'), 11),\n",
       "  (('again', 'he'), 11),\n",
       "  (('he', 'said'), 11),\n",
       "  (('and', 'this'), 11),\n",
       "  (('I', 'can'), 11),\n",
       "  (('let', 'him'), 11),\n",
       "  (('a', 'bit'), 11),\n",
       "  (('she', 'had'), 11),\n",
       "  (('for', 'this'), 11),\n",
       "  (('electronic', 'work'), 11),\n",
       "  (('archive', 'foundation'), 11),\n",
       "  (('the', 'childlike'), 10),\n",
       "  (('childlike', 'people'), 10),\n",
       "  (('for', 'his'), 10),\n",
       "  (('saw', 'him'), 10),\n",
       "  (('among', 'the'), 10),\n",
       "  (('when', 'siddhartha'), 10),\n",
       "  (('what', 'he'), 10),\n",
       "  (('was', 'his'), 10),\n",
       "  (('and', 'also'), 10),\n",
       "  (('not', 'want'), 10),\n",
       "  (('his', 'heart.'), 10),\n",
       "  (('his', 'mind'), 10),\n",
       "  (('that', 'his'), 10),\n",
       "  (('of', 'their'), 10),\n",
       "  (('it', 'not'), 10),\n",
       "  (('the', 'self'), 10),\n",
       "  (('the', 'holy'), 10),\n",
       "  (('so', 'much'), 10),\n",
       "  (('again', 'and'), 10),\n",
       "  (('and', 'again'), 10),\n",
       "  (('to', 'leave'), 10),\n",
       "  (('to', 'hear'), 10),\n",
       "  (('with', 'him'), 10),\n",
       "  (('have', 'found'), 10),\n",
       "  (('to', 'go'), 10),\n",
       "  (('the', 'great'), 10),\n",
       "  (('where', 'he'), 10),\n",
       "  (('means', 'of'), 10),\n",
       "  (('the', 'two'), 10),\n",
       "  (('that', 'there'), 10),\n",
       "  (('thought', 'about'), 10),\n",
       "  (('who', 'has'), 10),\n",
       "  (('teachings', 'and'), 10),\n",
       "  (('and', 'have'), 10),\n",
       "  (('to', 'listen'), 10),\n",
       "  (('tell', 'me'), 10),\n",
       "  (('to', 'give'), 10),\n",
       "  (('in', 'front'), 10),\n",
       "  (('which', 'the'), 10),\n",
       "  (('of', 'which'), 10),\n",
       "  (('heard', 'the'), 10),\n",
       "  (('a', 'person'), 10),\n",
       "  (('refuge', 'in'), 10),\n",
       "  (('this', 'moment'), 10),\n",
       "  (('this', 'world'), 10),\n",
       "  (('I', 'also'), 10),\n",
       "  (('would', 'have'), 10),\n",
       "  (('from', 'it'), 10),\n",
       "  (('had', 'become'), 10),\n",
       "  (('as', 'the'), 10),\n",
       "  (('a', 'young'), 10),\n",
       "  (('she', 'was'), 10),\n",
       "  (('you', 'dont'), 10),\n",
       "  (('for', 'him'), 10),\n",
       "  (('the', 'work'), 10),\n",
       "  (('access', 'to'), 10),\n",
       "  (('you', 'may'), 9),\n",
       "  (('the', 'son'), 9),\n",
       "  (('son', 'of'), 9),\n",
       "  (('the', 'sun'), 9),\n",
       "  (('one', 'with'), 9),\n",
       "  (('he', 'loved'), 9),\n",
       "  (('for', 'himself'), 9),\n",
       "  (('love', 'and'), 9),\n",
       "  (('started', 'to'), 9),\n",
       "  (('the', 'gods'), 9),\n",
       "  (('it', 'to'), 9),\n",
       "  (('not', 'to'), 9),\n",
       "  (('every', 'day'), 9),\n",
       "  (('to', 'himself'), 9),\n",
       "  (('the', 'evening'), 9),\n",
       "  (('you', 'to'), 9),\n",
       "  (('asked', 'the'), 9),\n",
       "  (('before', 'the'), 9),\n",
       "  (('that', 'siddhartha'), 9),\n",
       "  (('him', 'in'), 9),\n",
       "  (('his', 'hand'), 9),\n",
       "  (('to', 'find'), 9),\n",
       "  (('of', 'my'), 9),\n",
       "  (('any', 'more.'), 9),\n",
       "  (('only', 'a'), 9),\n",
       "  (('a', 'few'), 9),\n",
       "  (('suffering', 'and'), 9),\n",
       "  (('a', 'thousand'), 9),\n",
       "  (('to', 'this'), 9),\n",
       "  (('siddhartha', 'said'), 9),\n",
       "  (('path', 'of'), 9),\n",
       "  (('and', 'I'), 9),\n",
       "  (('one', 'time'), 9),\n",
       "  (('of', 'it'), 9),\n",
       "  (('of', 'gotama'), 9),\n",
       "  (('listen', 'to'), 9),\n",
       "  (('which', 'are'), 9),\n",
       "  (('to', 'take'), 9),\n",
       "  (('down', 'to'), 9),\n",
       "  (('before', 'he'), 9),\n",
       "  (('it', 'has'), 9),\n",
       "  (('he', 'asked'), 9),\n",
       "  (('not', 'be'), 9),\n",
       "  (('and', 'when'), 9),\n",
       "  (('on', 'my'), 9),\n",
       "  (('may', 'be'), 9),\n",
       "  (('to', 'it'), 9),\n",
       "  (('it', 'would'), 9),\n",
       "  (('of', 'your'), 9),\n",
       "  (('they', 'have'), 9),\n",
       "  (('and', 'all'), 9),\n",
       "  (('and', 'looked'), 9),\n",
       "  (('all', 'this'), 9),\n",
       "  (('for', 'many'), 9),\n",
       "  (('many', 'years'), 9),\n",
       "  (('the', 'voice'), 9),\n",
       "  (('from', 'this'), 9),\n",
       "  (('thank', 'you'), 9),\n",
       "  (('to', 'think'), 9),\n",
       "  (('will', 'be'), 9),\n",
       "  (('is', 'also'), 9),\n",
       "  (('but', 'it'), 9),\n",
       "  (('the', 'merchant'), 9),\n",
       "  (('thinking', 'of'), 9),\n",
       "  (('time', 'ago'), 9),\n",
       "  (('the', 'face'), 9),\n",
       "  (('the', 'full'), 9),\n",
       "  (('shade', 'of'), 8),\n",
       "  (('together', 'with'), 8),\n",
       "  (('the', 'wise'), 8),\n",
       "  (('to', 'speak'), 8),\n",
       "  (('wise', 'man'), 8),\n",
       "  (('he', 'wanted'), 8),\n",
       "  (('was', 'to'), 8),\n",
       "  (('in', 'its'), 8),\n",
       "  (('the', 'father'), 8),\n",
       "  (('was', 'in'), 8),\n",
       "  (('he', 'not'), 8),\n",
       "  (('it', 'had'), 8),\n",
       "  (('spoke', 'to'), 8),\n",
       "  (('siddhartha', 'spoke'), 8),\n",
       "  (('went', 'to'), 8),\n",
       "  (('he', 'heard'), 8),\n",
       "  (('face', 'of'), 8),\n",
       "  (('his', 'arms'), 8),\n",
       "  (('back', 'to'), 8),\n",
       "  (('will', 'not'), 8),\n",
       "  (('by', 'means'), 8),\n",
       "  (('of', 'an'), 8),\n",
       "  (('know', 'that'), 8),\n",
       "  (('not', 'know'), 8),\n",
       "  (('been', 'a'), 8),\n",
       "  (('this', 'I'), 8),\n",
       "  (('I', 'know'), 8),\n",
       "  (('time', 'and'), 8),\n",
       "  (('know', 'it'), 8),\n",
       "  (('at', 'this'), 8),\n",
       "  (('at', 'one'), 8),\n",
       "  (('as', 'soon'), 8),\n",
       "  (('soon', 'as'), 8),\n",
       "  (('after', 'all'), 8),\n",
       "  (('when', 'I'), 8),\n",
       "  (('you', 'my'), 8),\n",
       "  (('have', 'you'), 8),\n",
       "  (('this', 'siddhartha'), 8),\n",
       "  (('in', 'which'), 8),\n",
       "  (('old', 'man'), 8),\n",
       "  (('would', 'like'), 8),\n",
       "  (('siddhartha', 'saw'), 8),\n",
       "  (('him', 'a'), 8),\n",
       "  (('of', 'every'), 8),\n",
       "  (('who', 'would'), 8),\n",
       "  (('happened', 'to'), 8),\n",
       "  (('and', 'now'), 8),\n",
       "  (('cannot', 'be'), 8),\n",
       "  (('have', 'not'), 8),\n",
       "  (('you', 'for'), 8),\n",
       "  (('love', 'for'), 8),\n",
       "  (('thoughts', 'and'), 8),\n",
       "  (('him', 'from'), 8),\n",
       "  (('the', 'sky'), 8),\n",
       "  (('he', 'also'), 8),\n",
       "  (('or', 'a'), 8),\n",
       "  (('had', 'felt'), 8),\n",
       "  (('hut', 'and'), 8),\n",
       "  (('have', 'no'), 8),\n",
       "  (('from', 'her'), 8),\n",
       "  (('as', 'well.'), 8),\n",
       "  (('hair', 'and'), 8),\n",
       "  (('you', 'must'), 8),\n",
       "  (('“it', 'is'), 8),\n",
       "  (('a', 'game'), 8),\n",
       "  (('him', 'that'), 8),\n",
       "  (('listening', 'to'), 8),\n",
       "  (('himself', 'with'), 8),\n",
       "  (('he', 'sat'), 8),\n",
       "  (('how', 'he'), 8),\n",
       "  (('a', 'stone'), 8),\n",
       "  (('the', 'foundation'), 8),\n",
       "  (('set', 'forth'), 8),\n",
       "  (('forth', 'in'), 8),\n",
       "  (('this', 'work'), 8),\n",
       "  (('of', 'project'), 8),\n",
       "  (('use', 'of'), 7),\n",
       "  (('the', 'shade'), 7),\n",
       "  (('wise', 'men'), 7),\n",
       "  (('the', 'om'), 7),\n",
       "  (('man', 'and'), 7),\n",
       "  (('of', 'those'), 7),\n",
       "  (('he', 'still'), 7),\n",
       "  (('the', 'stars'), 7),\n",
       "  (('teachings', 'of'), 7),\n",
       "  (('that', 'they'), 7),\n",
       "  (('filled', 'his'), 7),\n",
       "  (('one', 'the'), 7),\n",
       "  (('to', 'make'), 7),\n",
       "  (('was', 'this'), 7),\n",
       "  (('the', 'life'), 7),\n",
       "  (('the', 'way'), 7),\n",
       "  (('the', 'words'), 7),\n",
       "  (('the', 'eternal'), 7),\n",
       "  (('sat', 'down'), 7),\n",
       "  (('had', 'come'), 7),\n",
       "  (('the', 'hour'), 7),\n",
       "  (('hour', 'of'), 7),\n",
       "  (('siddhartha', 'is'), 7),\n",
       "  (('I', 'will'), 7),\n",
       "  (('what', 'you'), 7),\n",
       "  (('spoke', 'the'), 7),\n",
       "  (('left', 'the'), 7),\n",
       "  (('he', 'came'), 7),\n",
       "  (('forest', 'and'), 7),\n",
       "  (('for', 'me'), 7),\n",
       "  (('the', 'last'), 7),\n",
       "  (('“you', 'have'), 7),\n",
       "  (('this', 'day'), 7),\n",
       "  (('stood', 'there'), 7),\n",
       "  (('his', 'hair'), 7),\n",
       "  (('felt', 'the'), 7),\n",
       "  (('the', 'cycle'), 7),\n",
       "  (('one', 'day'), 7),\n",
       "  (('quoth', 'govinda'), 7),\n",
       "  (('being', 'a'), 7),\n",
       "  (('you', 'know'), 7),\n",
       "  (('but', 'that'), 7),\n",
       "  (('time', 'when'), 7),\n",
       "  (('we', 'have'), 7),\n",
       "  (('many', 'a'), 7),\n",
       "  (('and', 'me'), 7),\n",
       "  (('and', 'will'), 7),\n",
       "  (('as', 'much'), 7),\n",
       "  (('his', 'head'), 7),\n",
       "  (('all', 'that'), 7),\n",
       "  (('young', 'men'), 7),\n",
       "  (('gotama', 'the'), 7),\n",
       "  (('to', 'seek'), 7),\n",
       "  (('he', 'remembered'), 7),\n",
       "  (('the', 'samana'), 7),\n",
       "  (('close', 'to'), 7),\n",
       "  (('I', 'want'), 7),\n",
       "  (('and', 'they'), 7),\n",
       "  (('perfected', 'one'), 7),\n",
       "  (('with', 'your'), 7),\n",
       "  (('his', 'thoughts'), 7),\n",
       "  (('face', 'was'), 7),\n",
       "  (('of', 'suffering'), 7),\n",
       "  (('and', 'asked'), 7),\n",
       "  (('you', 'shall'), 7),\n",
       "  (('for', 'you'), 7),\n",
       "  (('had', 'left'), 7),\n",
       "  (('very', 'good'), 7),\n",
       "  (('the', 'morning'), 7),\n",
       "  (('told', 'him'), 7),\n",
       "  (('most', 'of'), 7),\n",
       "  (('is', 'never'), 7),\n",
       "  (('wish', 'to'), 7),\n",
       "  (('a', 'small'), 7),\n",
       "  (('world', 'of'), 7),\n",
       "  (('have', 'a'), 7),\n",
       "  (('one', 'thing'), 7),\n",
       "  (('there', 'are'), 7),\n",
       "  (('the', 'ground'), 7),\n",
       "  (('aware', 'of'), 7),\n",
       "  (('the', 'one'), 7),\n",
       "  (('a', 'deep'), 7),\n",
       "  (('by', 'this'), 7),\n",
       "  (('his', 'path'), 7),\n",
       "  (('sought', 'to'), 7),\n",
       "  (('learn', 'from'), 7),\n",
       "  (('this', 'he'), 7),\n",
       "  (('but', 'now'), 7),\n",
       "  (('moment', 'when'), 7),\n",
       "  (('the', 'time'), 7),\n",
       "  (('deeply', 'he'), 7),\n",
       "  (('a', 'moment'), 7),\n",
       "  (('everything', 'he'), 7),\n",
       "  (('because', 'he'), 7),\n",
       "  (('at', 'that'), 7),\n",
       "  (('that', 'time'), 7),\n",
       "  (('a', 'woman'), 7),\n",
       "  (('everything', 'is'), 7),\n",
       "  (('are', 'like'), 7),\n",
       "  (('they', 'are'), 7),\n",
       "  (('her', 'eyes'), 7),\n",
       "  (('to', 'her'), 7),\n",
       "  (('of', 'love'), 7),\n",
       "  (('and', 'how'), 7),\n",
       "  (('piece', 'of'), 7),\n",
       "  (('she', 'said'), 7),\n",
       "  (('knew', 'that'), 7),\n",
       "  (('bid', 'his'), 7),\n",
       "  (('seems', 'to'), 7),\n",
       "  (('on', 'which'), 7),\n",
       "  (('which', 'a'), 7),\n",
       "  (('from', 'him'), 7),\n",
       "  (('said', 'nothing'), 7),\n",
       "  (('while', 'he'), 7),\n",
       "  (('kamala', 'had'), 7),\n",
       "  (('his', 'farewell'), 7),\n",
       "  (('farewell', 'to'), 7),\n",
       "  (('the', 'bank'), 7),\n",
       "  (('been', 'able'), 7),\n",
       "  (('each', 'one'), 7),\n",
       "  (('copies', 'of'), 7),\n",
       "  (('full', 'project'), 7),\n",
       "  (('agree', 'to'), 7),\n",
       "  (('this', 'ebook'), 6),\n",
       "  (('are', 'not'), 6),\n",
       "  (('up', 'the'), 6),\n",
       "  (('his', 'mother'), 6),\n",
       "  (('surrounded', 'by'), 6),\n",
       "  (('his', 'fathers'), 6),\n",
       "  (('when', 'she'), 6),\n",
       "  (('the', 'town'), 6),\n",
       "  (('siddhartha', 'did'), 6),\n",
       "  (('himself', 'he'), 6),\n",
       "  (('he', 'found'), 6),\n",
       "  (('the', 'soul'), 6),\n",
       "  (('father', 'and'), 6),\n",
       "  (('but', 'in'), 6),\n",
       "  (('to', 'reach'), 6),\n",
       "  (('and', 'not'), 6),\n",
       "  (('the', 'senses'), 6),\n",
       "  (('that', 'one'), 6),\n",
       "  (('there', 'and'), 6),\n",
       "  (('knowledge', 'of'), 6),\n",
       "  (('one', 'who'), 6),\n",
       "  (('had', 'reached'), 6),\n",
       "  (('time', 'to'), 6),\n",
       "  (('a', 'pilgrimage'), 6),\n",
       "  (('he', 'will'), 6),\n",
       "  (('read', 'the'), 6),\n",
       "  (('now', 'siddhartha'), 6),\n",
       "  (('siddhartha', 'looked'), 6),\n",
       "  (('was', 'just'), 6),\n",
       "  (('you', 'siddhartha'), 6),\n",
       "  (('is', 'my'), 6),\n",
       "  (('a', 'samana.'), 6),\n",
       "  (('the', 'small'), 6),\n",
       "  (('are', 'you'), 6),\n",
       "  (('an', 'hour'), 6),\n",
       "  (('over', 'his'), 6),\n",
       "  (('and', 'saw'), 6),\n",
       "  (('saw', 'that'), 6),\n",
       "  (('came', 'back'), 6),\n",
       "  (('light', 'of'), 6),\n",
       "  (('heart', 'with'), 6),\n",
       "  (('the', 'day'), 6),\n",
       "  (('“I', 'have'), 6),\n",
       "  (('than', 'the'), 6),\n",
       "  (('then', 'the'), 6),\n",
       "  (('himself', 'to'), 6),\n",
       "  (('he', 'stood'), 6),\n",
       "  (('turned', 'into'), 6),\n",
       "  (('end', 'of'), 6),\n",
       "  (('his', 'senses'), 6),\n",
       "  (('water', 'and'), 6),\n",
       "  (('you', 'think'), 6),\n",
       "  (('could', 'have'), 6),\n",
       "  (('and', 'by'), 6),\n",
       "  (('how', 'could'), 6),\n",
       "  (('what', 'the'), 6),\n",
       "  (('friend', 'and'), 6),\n",
       "  (('began', 'to'), 6),\n",
       "  (('we', 'are'), 6),\n",
       "  (('have', 'already'), 6),\n",
       "  (('old', 'and'), 6),\n",
       "  (('reach', 'the'), 6),\n",
       "  (('and', 'on'), 6),\n",
       "  (('if', 'I'), 6),\n",
       "  (('kind', 'of'), 6),\n",
       "  (('what', 'would'), 6),\n",
       "  (('himself', 'in'), 6),\n",
       "  (('had', 'said'), 6),\n",
       "  (('had', 'lived'), 6),\n",
       "  (('himself', 'and'), 6),\n",
       "  (('without', 'a'), 6),\n",
       "  (('and', 'as'), 6),\n",
       "  (('on', 'their'), 6),\n",
       "  (('had', 'heard'), 6),\n",
       "  (('lived', 'in'), 6),\n",
       "  (('you', 'like'), 6),\n",
       "  (('from', 'me'), 6),\n",
       "  (('willing', 'to'), 6),\n",
       "  (('but', 'this'), 6),\n",
       "  (('is', 'very'), 6),\n",
       "  (('to', 'walk'), 6),\n",
       "  (('place', 'to'), 6),\n",
       "  (('the', 'very'), 6),\n",
       "  (('to', 'them'), 6),\n",
       "  (('the', 'woman'), 6),\n",
       "  (('the', 'garden'), 6),\n",
       "  (('has', 'come'), 6),\n",
       "  (('an', 'end'), 6),\n",
       "  (('exalted', 'one.'), 6),\n",
       "  (('accustomed', 'to'), 6),\n",
       "  (('was', 'also'), 6),\n",
       "  (('his', 'way'), 6),\n",
       "  (('and', 'then'), 6),\n",
       "  (('to', 'siddhartha'), 6),\n",
       "  (('dont', 'you'), 6),\n",
       "  (('been', 'my'), 6),\n",
       "  (('any', 'other'), 6),\n",
       "  (('you', 'oh'), 6),\n",
       "  (('this', 'path'), 6),\n",
       "  (('time', 'the'), 6),\n",
       "  (('in', 'your'), 6),\n",
       "  (('before', 'this'), 6),\n",
       "  (('about', 'this'), 6),\n",
       "  (('this', 'one'), 6),\n",
       "  (('the', 'goal'), 6),\n",
       "  (('you', 'in'), 6),\n",
       "  (('oh', 'venerable'), 6),\n",
       "  (('many', 'to'), 6),\n",
       "  (('of', 'what'), 6),\n",
       "  (('from', 'all'), 6),\n",
       "  (('this', 'hour'), 6),\n",
       "  (('world', 'and'), 6),\n",
       "  (('I', 'must'), 6),\n",
       "  (('it', 'might'), 6),\n",
       "  (('and', 'is'), 6),\n",
       "  (('he', 'let'), 6),\n",
       "  (('inside', 'of'), 6),\n",
       "  (('his', 'youth'), 6),\n",
       "  (('a', 'part'), 6),\n",
       "  (('him', 'had'), 6),\n",
       "  (('unable', 'to'), 6),\n",
       "  (('he', 'now'), 6),\n",
       "  (('afraid', 'of'), 6),\n",
       "  (('of', 'its'), 6),\n",
       "  (('feeling', 'of'), 6),\n",
       "  (('be', 'my'), 6),\n",
       "  (('should', 'I'), 6),\n",
       "  (('and', 'at'), 6),\n",
       "  (('been', 'the'), 6),\n",
       "  (('looked', 'upon'), 6),\n",
       "  (('became', 'aware'), 6),\n",
       "  (('it', 'he'), 6),\n",
       "  (('he', 'became'), 6),\n",
       "  (('very', 'thing'), 6),\n",
       "  (('had', 'now'), 6),\n",
       "  (('had', 'a'), 6),\n",
       "  (('“I', 'thank'), 6),\n",
       "  (('you', 'too'), 6),\n",
       "  (('so', 'that'), 6),\n",
       "  (('up', 'and'), 6),\n",
       "  (('him', 'at'), 6),\n",
       "  (('had', 'seen'), 6),\n",
       "  (('a', 'word'), 6),\n",
       "  (('into', 'this'), 6),\n",
       "  (('with', 'her'), 6),\n",
       "  (('which', 'you'), 6),\n",
       "  (('coming', 'from'), 6),\n",
       "  (('it', 'as'), 6),\n",
       "  (('like', 'you'), 6),\n",
       "  (('his', 'eyes.'), 6),\n",
       "  (('therefore', 'I'), 6),\n",
       "  (('see', 'you'), 6),\n",
       "  (('when', 'you'), 6),\n",
       "  (('with', 'one'), 6),\n",
       "  (('a', 'rich'), 6),\n",
       "  (('force', 'him'), 6),\n",
       "  (('is', 'good'), 6),\n",
       "  (('for', 'him.'), 6),\n",
       "  (('not', 'in'), 6),\n",
       "  (('as', 'I'), 6),\n",
       "  (('to', 'sleep'), 6),\n",
       "  (('was', 'only'), 6),\n",
       "  (('the', 'people'), 6),\n",
       "  (('do', 'with'), 6),\n",
       "  (('in', 'all'), 6),\n",
       "  (('you', 'can'), 6),\n",
       "  (('the', 'game'), 6),\n",
       "  (('bent', 'over'), 6),\n",
       "  (('at', 'his'), 6),\n",
       "  (('you', 'do'), 6),\n",
       "  (('it', 'happened'), 6),\n",
       "  (('lost', 'his'), 6),\n",
       "  (('his', 'smile'), 6),\n",
       "  (('smile', 'of'), 6),\n",
       "  (('had', 'he'), 6),\n",
       "  (('so', 'he'), 6),\n",
       "  (('he', 'smiled'), 6),\n",
       "  (('the', 'bird'), 6),\n",
       "  (('this', 'river'), 6),\n",
       "  (('to', 'love'), 6),\n",
       "  (('this', 'water'), 6),\n",
       "  (('boat', 'and'), 6),\n",
       "  (('the', 'ferrymans'), 6),\n",
       "  (('river', 'has'), 6),\n",
       "  (('saw', 'his'), 6),\n",
       "  (('of', 'any'), 6),\n",
       "  (('I', 'see'), 6),\n",
       "  (('work', 'in'), 6),\n",
       "  (('in', 'any'), 6),\n",
       "  (('gutenberg™', 'license'), 6),\n",
       "  (('in', 'paragraph'), 6),\n",
       "  (('the', 'copyright'), 6),\n",
       "  (('work', 'or'), 6),\n",
       "  (('the', 'use'), 5),\n",
       "  (('the', 'laws'), 5),\n",
       "  (('laws', 'of'), 5),\n",
       "  (('house', 'in'), 5),\n",
       "  (('near', 'the'), 5),\n",
       "  (('the', 'art'), 5),\n",
       "  (('art', 'of'), 5),\n",
       "  (('he', 'already'), 5),\n",
       "  (('with', 'all'), 5),\n",
       "  (('and', 'what'), 5),\n",
       "  (('become', 'one'), 5),\n",
       "  (('of', 'joy'), 5),\n",
       "  (('the', 'love'), 5),\n",
       "  (('love', 'of'), 5),\n",
       "  (('but', 'they'), 5),\n",
       "  (('off', 'the'), 5),\n",
       "  (('the', 'atman'), 5),\n",
       "  (('and', 'you'), 5),\n",
       "  (('the', 'highest'), 5),\n",
       "  (('innermost', 'part'), 5),\n",
       "  (('this', 'way'), 5),\n",
       "  (('teachers', 'and'), 5),\n",
       "  (('brahmans', 'and'), 5),\n",
       "  (('and', 'their'), 5),\n",
       "  (('spoke', 'of'), 5),\n",
       "  (('in', 'these'), 5),\n",
       "  (('state', 'of'), 5),\n",
       "  (('of', 'being'), 5),\n",
       "  (('were', 'his'), 5),\n",
       "  (('was', 'he'), 5),\n",
       "  (('man', 'a'), 5),\n",
       "  (('over', 'and'), 5),\n",
       "  (('and', 'over'), 5),\n",
       "  (('from', 'a'), 5),\n",
       "  (('the', 'name'), 5),\n",
       "  (('name', 'of'), 5),\n",
       "  (('no', 'one'), 5),\n",
       "  (('with', 'me'), 5),\n",
       "  (('time', 'of'), 5),\n",
       "  (('a', 'very'), 5),\n",
       "  (('a', 'little'), 5),\n",
       "  (('up', 'in'), 5),\n",
       "  (('like', 'the'), 5),\n",
       "  (('his', 'fate'), 5),\n",
       "  (('the', 'chamber'), 5),\n",
       "  (('standing', 'there'), 5),\n",
       "  (('tell', 'you'), 5),\n",
       "  (('you', 'that'), 5),\n",
       "  (('I', 'wish'), 5),\n",
       "  (('he', 'went'), 5),\n",
       "  (('sleep', 'had'), 5),\n",
       "  (('the', 'moon'), 5),\n",
       "  (('it', 'with'), 5),\n",
       "  (('tell', 'him'), 5),\n",
       "  (('up', 'with'), 5),\n",
       "  (('day', 'and'), 5),\n",
       "  (('his', 'mouth'), 5),\n",
       "  (('he', 'walked'), 5),\n",
       "  (('for', 'their'), 5),\n",
       "  (('worthy', 'of'), 5),\n",
       "  (('it', 'all'), 5),\n",
       "  (('with', 'an'), 5),\n",
       "  (('my', 'self'), 5),\n",
       "  (('had', 'died'), 5),\n",
       "  (('and', 'every'), 5),\n",
       "  (('until', 'he'), 5),\n",
       "  (('and', 'learned'), 5),\n",
       "  (('to', 'breathe'), 5),\n",
       "  (('according', 'to'), 5),\n",
       "  (('had', 'tasted'), 5),\n",
       "  (('the', 'end'), 5),\n",
       "  (('of', 'that'), 5),\n",
       "  (('could', 'you'), 5),\n",
       "  (('the', 'pain'), 5),\n",
       "  (('of', 'life'), 5),\n",
       "  (('when', 'they'), 5),\n",
       "  (('true', 'that'), 5),\n",
       "  (('that', 'a'), 5),\n",
       "  (('has', 'not'), 5),\n",
       "  (('have', 'thought'), 5),\n",
       "  (('might', 'be'), 5),\n",
       "  (('we', 'will'), 5),\n",
       "  (('I', 'believe'), 5),\n",
       "  (('single', 'one'), 5),\n",
       "  (('a', 'voice'), 5),\n",
       "  (('suffering', 'of'), 5),\n",
       "  (('nothing', 'to'), 5),\n",
       "  (('starting', 'to'), 5),\n",
       "  (('believe', 'that'), 5),\n",
       "  (('and', 'spoke'), 5),\n",
       "  (('if', 'it'), 5),\n",
       "  (('if', 'there'), 5),\n",
       "  (('this', 'what'), 5),\n",
       "  (('govinda', 'had'), 5),\n",
       "  (('to', 'their'), 5),\n",
       "  (('the', 'suffering'), 5),\n",
       "  (('the', 'yellow'), 5),\n",
       "  (('here', 'and'), 5),\n",
       "  (('good', 'and'), 5),\n",
       "  (('if', 'the'), 5),\n",
       "  (('that', 'in'), 5),\n",
       "  (('would', 'go'), 5),\n",
       "  (('felt', 'a'), 5),\n",
       "  (('see', 'the'), 5),\n",
       "  (('teachings', 'from'), 5),\n",
       "  (('now', 'you'), 5),\n",
       "  (('that', 'my'), 5),\n",
       "  (('my', 'heart'), 5),\n",
       "  (('have', 'heard'), 5),\n",
       "  (('had', 'fallen'), 5),\n",
       "  (('a', 'good'), 5),\n",
       "  (('an', 'old'), 5),\n",
       "  (('said', 'siddhartha.'), 5),\n",
       "  (('to', 'stay'), 5),\n",
       "  (('the', 'rich'), 5),\n",
       "  (('had', 'given'), 5),\n",
       "  (('the', 'door'), 5),\n",
       "  (('the', 'perfected'), 5),\n",
       "  (('and', 'full'), 5),\n",
       "  (('to', 'an'), 5),\n",
       "  (('have', 'seen'), 5),\n",
       "  (('to', 'ask'), 5),\n",
       "  (('and', 'since'), 5),\n",
       "  (('at', 'night'), 5),\n",
       "  (('there', 'were'), 5),\n",
       "  (('those', 'who'), 5),\n",
       "  (('the', 'monks'), 5),\n",
       "  (('if', 'a'), 5),\n",
       "  (('yellow', 'robe'), 5),\n",
       "  (('different', 'from'), 5),\n",
       "  (('hundreds', 'of'), 5),\n",
       "  (('placed', 'his'), 5),\n",
       "  (('face', 'and'), 5),\n",
       "  (('in', 'an'), 5),\n",
       "  (('had', 'just'), 5),\n",
       "  (('by', 'him'), 5),\n",
       "  (('one', 'spoke'), 5),\n",
       "  (('a', 'pilgrim'), 5),\n",
       "  (('and', 'are'), 5),\n",
       "  (('wish', 'that'), 5),\n",
       "  (('refuge', 'with'), 5),\n",
       "  (('he', 'started'), 5),\n",
       "  (('home', 'and'), 5),\n",
       "  (('why', 'he'), 5),\n",
       "  (('them', 'in'), 5),\n",
       "  (('the', 'venerable'), 5),\n",
       "  (('oh', 'exalted'), 5),\n",
       "  (('one', 'I'), 5),\n",
       "  (('this', 'has'), 5),\n",
       "  (('it', 'may'), 5),\n",
       "  (('something', 'which'), 5),\n",
       "  (('not', 'been'), 5),\n",
       "  (('are', 'your'), 5),\n",
       "  (('good', 'for'), 5),\n",
       "  (('let', 'me'), 5),\n",
       "  (('one', 'more'), 5),\n",
       "  (('for', 'I'), 5),\n",
       "  (('my', 'eyes'), 5),\n",
       "  (('life', 'the'), 5),\n",
       "  (('from', 'my'), 5),\n",
       "  (('way', 'he'), 5),\n",
       "  (('saw', 'a'), 5),\n",
       "  (('siddhartha', 'thought'), 5),\n",
       "  (('more', 'he'), 5),\n",
       "  (('in', 'me'), 5),\n",
       "  (('place', 'where'), 5),\n",
       "  (('to', 'accept'), 5),\n",
       "  (('siddhartha', 'has'), 5),\n",
       "  (('path', 'to'), 5),\n",
       "  (('and', 'if'), 5),\n",
       "  (('were', 'not'), 5),\n",
       "  (('siddhartha', 'stopped'), 5),\n",
       "  ...])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = list(filter(lambda x: 1 < int(x[1]), ngrams_up_to_20[0].most_common()))\n",
    "len(l), l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1e69bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_filter(ngrams):\n",
    "    return filter(remove_periods, list(filter(lambda x: 1 < int(x[1]), ngrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c4e1c457",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_to_learn = ngrams_up_to_20[0]\n",
    "X_train_example = [[word_to_index[sent[0][0]]] for sent in my_filter(bigrams_to_learn.most_common())\n",
    "                  if sent[0][0] in word_to_index and sent[0][1] in word_to_index]\n",
    "y_train_example = [[word_to_index[sent[0][1]]] for sent in my_filter(bigrams_to_learn.most_common())\n",
    "                  if sent[0][0] in word_to_index and sent[0][1] in word_to_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "82de2cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[6], [10], [8], [5], [6], [5], [10], [4], [18], [22]],\n",
       " [[1], [1], [11], [1], [9], [31], [9], [1], [29], [7]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_example[0:10], y_train_example[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "841b3de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4836, 4836)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_example), len(y_train_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1bcb7f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'long', 'time'),\n",
       " ('for', 'a', 'long'),\n",
       " ('the', 'project', 'gutenberg'),\n",
       " ('he', 'had', 'been'),\n",
       " ('the', 'exalted', 'one'),\n",
       " ('project', 'gutenberg™', 'electronic'),\n",
       " ('in', 'order', 'to'),\n",
       " ('which', 'he', 'had'),\n",
       " ('in', 'his', 'heart'),\n",
       " ('that', 'he', 'had'),\n",
       " ('in', 'the', 'forest'),\n",
       " ('he', 'did', 'not'),\n",
       " ('he', 'had', 'not'),\n",
       " ('of', 'the', 'world'),\n",
       " ('the', 'terms', 'of'),\n",
       " ('with', 'a', 'smile'),\n",
       " ('seemed', 'to', 'him'),\n",
       " ('long', 'time', 'he'),\n",
       " ('be', 'able', 'to'),\n",
       " ('the', 'project', 'gutenberg™'),\n",
       " ('project', 'gutenberg', 'literary'),\n",
       " ('gutenberg', 'literary', 'archive'),\n",
       " ('all', 'of', 'this'),\n",
       " ('he', 'had', 'to'),\n",
       " ('when', 'he', 'had'),\n",
       " ('literary', 'archive', 'foundation'),\n",
       " ('in', 'the', 'united'),\n",
       " ('the', 'united', 'states'),\n",
       " ('of', 'the', 'project'),\n",
       " ('by', 'the', 'river'),\n",
       " ('to', 'him', 'and'),\n",
       " ('as', 'if', 'he'),\n",
       " ('to', 'become', 'a'),\n",
       " ('in', 'front', 'of'),\n",
       " ('this', 'is', 'what'),\n",
       " ('he', 'had', 'learned'),\n",
       " ('gutenberg™', 'electronic', 'works'),\n",
       " ('terms', 'of', 'this'),\n",
       " ('not', 'want', 'to'),\n",
       " ('had', 'to', 'be'),\n",
       " ('the', 'river', 'and'),\n",
       " ('it', 'seemed', 'to'),\n",
       " ('I', 'do', 'not'),\n",
       " ('that', 'he', 'was'),\n",
       " ('long', 'time', 'ago'),\n",
       " ('the', 'childlike', 'people'),\n",
       " ('he', 'wanted', 'to'),\n",
       " ('it', 'was', 'not'),\n",
       " ('out', 'of', 'the'),\n",
       " ('he', 'could', 'not'),\n",
       " ('of', 'the', 'samanas'),\n",
       " ('by', 'means', 'of'),\n",
       " ('as', 'soon', 'as'),\n",
       " ('the', 'old', 'man'),\n",
       " ('would', 'like', 'to'),\n",
       " ('to', 'him', 'as'),\n",
       " ('set', 'forth', 'in'),\n",
       " ('of', 'this', 'agreement'),\n",
       " ('of', 'project', 'gutenberg™'),\n",
       " ('shade', 'of', 'the'),\n",
       " ('he', 'was', 'a'),\n",
       " ('came', 'to', 'him'),\n",
       " ('there', 'was', 'no'),\n",
       " ('in', 'the', 'same'),\n",
       " ('and', 'in', 'the'),\n",
       " ('away', 'from', 'the'),\n",
       " ('at', 'one', 'time'),\n",
       " ('to', 'listen', 'to'),\n",
       " ('I', 'want', 'to'),\n",
       " ('looked', 'at', 'the'),\n",
       " ('in', 'this', 'moment'),\n",
       " ('it', 'would', 'be'),\n",
       " ('to', 'be', 'able'),\n",
       " ('he', 'saw', 'the'),\n",
       " ('all', 'of', 'these'),\n",
       " ('his', 'farewell', 'to'),\n",
       " ('a', 'young', 'man'),\n",
       " ('there', 'was', 'nothing'),\n",
       " ('been', 'able', 'to'),\n",
       " ('the', 'face', 'of'),\n",
       " ('to', 'the', 'project'),\n",
       " ('terms', 'of', 'the'),\n",
       " ('the', 'shade', 'of'),\n",
       " ('son', 'of', 'a'),\n",
       " ('for', 'his', 'son'),\n",
       " ('was', 'it', 'not'),\n",
       " ('and', 'it', 'was'),\n",
       " ('knowledge', 'of', 'the'),\n",
       " ('again', 'and', 'again'),\n",
       " ('on', 'a', 'pilgrimage'),\n",
       " ('he', 'looked', 'into'),\n",
       " ('looked', 'into', 'the'),\n",
       " ('the', 'path', 'of'),\n",
       " ('one', 'of', 'the'),\n",
       " ('used', 'to', 'be'),\n",
       " ('from', 'the', 'samanas'),\n",
       " ('him', 'and', 'he'),\n",
       " ('him', 'as', 'if'),\n",
       " ('refuge', 'in', 'the'),\n",
       " ('if', 'he', 'had'),\n",
       " ('in', 'the', 'morning'),\n",
       " ('do', 'not', 'want'),\n",
       " ('oh', 'venerable', 'one'),\n",
       " ('the', 'world', 'and'),\n",
       " ('so', 'it', 'seemed'),\n",
       " ('a', 'part', 'of'),\n",
       " ('he', 'had', 'also'),\n",
       " ('to', 'learn', 'from'),\n",
       " ('was', 'no', 'longer'),\n",
       " ('for', 'a', 'moment'),\n",
       " ('at', 'that', 'time'),\n",
       " ('the', 'hut', 'and'),\n",
       " ('across', 'the', 'river'),\n",
       " ('as', 'it', 'is'),\n",
       " ('looked', 'at', 'him'),\n",
       " ('able', 'to', 'do'),\n",
       " ('I', 'have', 'been'),\n",
       " ('he', 'learned', 'from'),\n",
       " ('at', 'the', 'same'),\n",
       " ('as', 'a', 'young'),\n",
       " ('in', 'the', 'hut'),\n",
       " ('the', 'full', 'project'),\n",
       " ('full', 'project', 'gutenberg™'),\n",
       " ('project', 'gutenberg™', 'license'),\n",
       " ('gutenberg™', 'electronic', 'work'),\n",
       " ('the', 'use', 'of'),\n",
       " ('the', 'son', 'of'),\n",
       " ('in', 'the', 'shade'),\n",
       " ('house', 'in', 'the'),\n",
       " ('of', 'the', 'river'),\n",
       " ('the', 'art', 'of'),\n",
       " ('one', 'with', 'the'),\n",
       " ('of', 'the', 'brahmans'),\n",
       " ('the', 'teachings', 'of'),\n",
       " ('teachings', 'of', 'the'),\n",
       " ('of', 'the', 'senses'),\n",
       " ('the', 'name', 'of'),\n",
       " ('all', 'of', 'them'),\n",
       " ('siddhartha', 'did', 'not'),\n",
       " ('in', 'the', 'evening'),\n",
       " ('go', 'to', 'the'),\n",
       " ('that', 'it', 'is'),\n",
       " ('his', 'heart', 'with'),\n",
       " ('him', 'in', 'his'),\n",
       " ('the', 'forest', 'and'),\n",
       " ('to', 'be', 'a'),\n",
       " ('from', 'the', 'self'),\n",
       " ('it', 'is', 'not'),\n",
       " ('it', 'is', 'a'),\n",
       " ('a', 'single', 'one'),\n",
       " ('just', 'as', 'much'),\n",
       " ('in', 'the', 'yellow'),\n",
       " ('here', 'and', 'there'),\n",
       " ('gotama', 'the', 'buddha'),\n",
       " ('who', 'had', 'been'),\n",
       " ('he', 'had', 'heard'),\n",
       " ('and', 'he', 'had'),\n",
       " ('the', 'teachings', 'from'),\n",
       " ('front', 'of', 'the'),\n",
       " ('of', 'the', 'exalted'),\n",
       " ('from', 'the', 'forest'),\n",
       " ('has', 'come', 'to'),\n",
       " ('seemed', 'to', 'be'),\n",
       " ('down', 'to', 'the'),\n",
       " ('long', 'time', 'the'),\n",
       " ('the', 'venerable', 'one'),\n",
       " ('thought', 'about', 'it'),\n",
       " ('in', 'this', 'world'),\n",
       " ('there', 'is', 'no'),\n",
       " ('and', 'the', 'river'),\n",
       " ('on', 'the', 'other'),\n",
       " ('became', 'aware', 'of'),\n",
       " ('he', 'had', 'experienced'),\n",
       " ('this', 'very', 'thing'),\n",
       " ('it', 'in', 'the'),\n",
       " ('he', 'said', 'to'),\n",
       " ('I', 'have', 'learned'),\n",
       " ('“I', 'thank', 'you'),\n",
       " ('it', 'is', 'also'),\n",
       " ('he', 'was', 'still'),\n",
       " ('to', 'the', 'city'),\n",
       " ('a', 'piece', 'of'),\n",
       " ('he', 'was', 'not'),\n",
       " ('said', 'nothing', 'and'),\n",
       " ('you', 'do', 'not'),\n",
       " ('of', 'his', 'youth'),\n",
       " ('part', 'of', 'this'),\n",
       " ('he', 'had', 'felt'),\n",
       " ('bid', 'his', 'farewell'),\n",
       " ('most', 'of', 'all'),\n",
       " ('had', 'been', 'able'),\n",
       " ('did', 'not', 'know'),\n",
       " ('listened', 'to', 'the'),\n",
       " ('by', 'the', 'bank'),\n",
       " ('the', 'river', 'has'),\n",
       " ('the', 'boat', 'and'),\n",
       " ('and', 'of', 'a'),\n",
       " ('the', 'boy', 'had'),\n",
       " ('not', 'protected', 'by'),\n",
       " ('a', 'project', 'gutenberg™'),\n",
       " ('if', 'you', 'are'),\n",
       " ('the', 'laws', 'of'),\n",
       " ('laws', 'of', 'the'),\n",
       " ('where', 'you', 'are'),\n",
       " ('you', 'are', 'located'),\n",
       " ('of', 'the', 'brahman'),\n",
       " ('with', 'the', 'samanas'),\n",
       " ('the', 'wise', 'men'),\n",
       " ('long', 'time', 'siddhartha'),\n",
       " ('of', 'the', 'grove'),\n",
       " ('the', 'grove', 'of'),\n",
       " ('of', 'the', 'soul'),\n",
       " ('the', 'love', 'of'),\n",
       " ('of', 'his', 'father'),\n",
       " ('of', 'his', 'friend'),\n",
       " ('was', 'to', 'be'),\n",
       " ('never', 'he', 'had'),\n",
       " ('the', 'hour', 'of'),\n",
       " ('if', 'he', 'was'),\n",
       " ('he', 'came', 'back'),\n",
       " ('the', 'light', 'of'),\n",
       " ('light', 'of', 'the'),\n",
       " ('to', 'go', 'to'),\n",
       " ('to', 'the', 'river'),\n",
       " ('of', 'his', 'son'),\n",
       " ('he', 'stood', 'there'),\n",
       " ('turned', 'into', 'a'),\n",
       " ('the', 'end', 'of'),\n",
       " ('when', 'he', 'was'),\n",
       " ('you', 'and', 'me'),\n",
       " ('out', 'of', 'all'),\n",
       " ('among', 'so', 'many'),\n",
       " ('path', 'of', 'the'),\n",
       " ('I', 'have', 'asked'),\n",
       " ('long', 'time', 'and'),\n",
       " ('that', 'there', 'is'),\n",
       " ('he', 'thought', 'about'),\n",
       " ('suffering', 'of', 'the'),\n",
       " ('through', 'the', 'land'),\n",
       " ('of', 'gotama', 'the'),\n",
       " ('the', 'young', 'men'),\n",
       " ('there', 'was', 'a'),\n",
       " ('a', 'wise', 'man'),\n",
       " ('had', 'reached', 'the'),\n",
       " ('lived', 'in', 'the'),\n",
       " ('of', 'a', 'brahman'),\n",
       " ('to', 'see', 'the'),\n",
       " ('hear', 'the', 'teachings'),\n",
       " ('he', 'had', 'fallen'),\n",
       " ('the', 'door', 'of'),\n",
       " ('the', 'perfected', 'one'),\n",
       " ('and', 'full', 'of'),\n",
       " ('on', 'his', 'way'),\n",
       " ('his', 'face', 'and'),\n",
       " ('before', 'he', 'had'),\n",
       " ('as', 'much', 'as'),\n",
       " ('was', 'full', 'of'),\n",
       " ('was', 'the', 'world'),\n",
       " ('come', 'to', 'you'),\n",
       " ('an', 'end', 'to'),\n",
       " ('you', 'my', 'dear'),\n",
       " ('with', 'the', 'exalted'),\n",
       " ('him', 'who', 'had'),\n",
       " ('before', 'this', 'has'),\n",
       " ('had', 'not', 'been'),\n",
       " ('that', 'you', 'have'),\n",
       " ('as', 'he', 'was'),\n",
       " ('the', 'place', 'where'),\n",
       " ('he', 'had', 'left'),\n",
       " ('was', 'not', 'able'),\n",
       " ('not', 'able', 'to'),\n",
       " ('to', 'be', 'my'),\n",
       " ('all', 'of', 'it'),\n",
       " ('no', 'longer', 'a'),\n",
       " ('in', 'his', 'chest'),\n",
       " ('for', 'many', 'years'),\n",
       " ('moment', 'when', 'the'),\n",
       " ('had', 'been', 'the'),\n",
       " ('he', 'became', 'aware'),\n",
       " ('now', 'he', 'had'),\n",
       " ('as', 'well', 'as'),\n",
       " ('in', 'the', 'street'),\n",
       " ('the', 'voice', 'of'),\n",
       " ('time', 'he', 'had'),\n",
       " ('at', 'the', 'entrance'),\n",
       " ('a', 'freshly', 'cracked'),\n",
       " ('he', 'had', 'seen'),\n",
       " ('him', 'who', 'was'),\n",
       " ('samana', 'from', 'the'),\n",
       " ('which', 'I', 'have'),\n",
       " ('coming', 'from', 'the'),\n",
       " ('you', 'like', 'to'),\n",
       " ('where', 'he', 'was'),\n",
       " ('youre', 'able', 'to'),\n",
       " ('as', 'he', 'had'),\n",
       " ('perhaps', 'it', 'is'),\n",
       " ('seems', 'to', 'be'),\n",
       " ('learned', 'from', 'her'),\n",
       " ('none', 'of', 'them'),\n",
       " ('I', 'was', 'a'),\n",
       " ('I', 'have', 'found'),\n",
       " ('him', 'many', 'to'),\n",
       " ('had', 'learned', 'from'),\n",
       " ('in', 'those', 'days'),\n",
       " ('which', 'used', 'to'),\n",
       " ('the', 'wheel', 'of'),\n",
       " ('had', 'learned', 'to'),\n",
       " ('while', 'he', 'was'),\n",
       " ('it', 'in', 'his'),\n",
       " ('the', 'city', 'and'),\n",
       " ('the', 'feeling', 'of'),\n",
       " ('and', 'looked', 'at'),\n",
       " ('also', 'had', 'to'),\n",
       " ('I', 'had', 'to'),\n",
       " ('he', 'used', 'to'),\n",
       " ('the', 'river', 'had'),\n",
       " ('ferried', 'across', 'the'),\n",
       " ('listening', 'to', 'the'),\n",
       " ('become', 'one', 'with'),\n",
       " ('the', 'boy', 'was'),\n",
       " ('the', 'same', 'time'),\n",
       " ('saw', 'the', 'face'),\n",
       " ('face', 'of', 'a'),\n",
       " ('this', 'smile', 'of'),\n",
       " ('the', 'phrase', '“project'),\n",
       " ('work', 'or', 'any'),\n",
       " ('project', 'gutenberg™', 'work'),\n",
       " ('project', 'gutenberg', 'ebook'),\n",
       " ('for', 'the', 'use'),\n",
       " ('united', 'states', 'and'),\n",
       " ('project', 'gutenberg', 'license'),\n",
       " ('or', 'online', 'at'),\n",
       " ('located', 'in', 'the'),\n",
       " ('check', 'the', 'laws'),\n",
       " ('son', 'of', 'the'),\n",
       " ('as', 'a', 'boy'),\n",
       " ('his', 'father', 'the'),\n",
       " ('knew', 'how', 'to'),\n",
       " ('with', 'all', 'the'),\n",
       " ('surrounded', 'by', 'the'),\n",
       " ('he', 'saw', 'him'),\n",
       " ('when', 'she', 'saw'),\n",
       " ('she', 'saw', 'him'),\n",
       " ('siddhartha', 'walked', 'through'),\n",
       " ('walked', 'through', 'the'),\n",
       " ('of', 'a', 'king'),\n",
       " ('but', 'more', 'than'),\n",
       " ('did', 'not', 'want'),\n",
       " ('to', 'become', 'one'),\n",
       " ('one', 'of', 'those'),\n",
       " ('sitting', 'in', 'the'),\n",
       " ('of', 'the', 'night'),\n",
       " ('had', 'started', 'to'),\n",
       " ('himself', 'he', 'had'),\n",
       " ('he', 'had', 'started'),\n",
       " ('love', 'of', 'his'),\n",
       " ('his', 'father', 'and'),\n",
       " ('they', 'did', 'not'),\n",
       " ('the', 'world', 'was'),\n",
       " ('it', 'not', 'the'),\n",
       " ('make', 'offerings', 'to'),\n",
       " ('offerings', 'to', 'the'),\n",
       " ('the', 'most', 'important'),\n",
       " ('most', 'important', 'thing'),\n",
       " ('of', 'the', 'holy'),\n",
       " ('the', 'knowledgeable', 'one'),\n",
       " ('over', 'and', 'over'),\n",
       " ('it', 'had', 'to'),\n",
       " ('spoke', 'to', 'himself'),\n",
       " ('name', 'of', 'the'),\n",
       " ('the', 'heavenly', 'world'),\n",
       " ('he', 'had', 'reached'),\n",
       " ('went', 'to', 'the'),\n",
       " ('time', 'of', 'the'),\n",
       " ('his', 'eyes', 'were'),\n",
       " ('when', 'he', 'heard'),\n",
       " ('and', 'with', 'the'),\n",
       " ('the', 'life', 'of'),\n",
       " ('to', 'tell', 'you'),\n",
       " ('it', 'is', 'my'),\n",
       " ('the', 'stars', 'in'),\n",
       " ('stars', 'in', 'the'),\n",
       " ('the', 'small', 'window'),\n",
       " ('his', 'arms', 'folded'),\n",
       " ('an', 'hour', 'he'),\n",
       " ('filled', 'his', 'heart'),\n",
       " ('the', 'young', 'man'),\n",
       " ('he', 'had', 'already'),\n",
       " ('me', 'to', 'be'),\n",
       " ('his', 'mother', 'to'),\n",
       " ('is', 'no', 'longer'),\n",
       " ('the', 'rainy', 'season'),\n",
       " ('any', 'more', 'until'),\n",
       " ('until', 'they', 'were'),\n",
       " ('and', 'learned', 'to'),\n",
       " ('of', 'his', 'heart'),\n",
       " ('was', 'lying', 'on'),\n",
       " ('end', 'of', 'the'),\n",
       " ('out', 'of', 'his'),\n",
       " ('he', 'learned', 'to'),\n",
       " ('he', 'went', 'the'),\n",
       " ('was', 'once', 'again'),\n",
       " ('do', 'you', 'think'),\n",
       " ('a', 'holy', 'man'),\n",
       " ('oh', 'govinda', 'I'),\n",
       " ('how', 'could', 'you'),\n",
       " ('a', 'short', 'numbing'),\n",
       " ('short', 'numbing', 'of'),\n",
       " ('numbing', 'of', 'the'),\n",
       " ('is', 'what', 'the'),\n",
       " ('this', 'is', 'how'),\n",
       " ('is', 'how', 'it'),\n",
       " ('“I', 'do', 'not'),\n",
       " ('that', 'I', 'am'),\n",
       " ('siddhartha', 'began', 'to'),\n",
       " ('not', 'a', 'single'),\n",
       " ('with', 'a', 'quiet'),\n",
       " ('of', 'a', 'samana'),\n",
       " ('have', 'asked', 'the'),\n",
       " ('if', 'I', 'had'),\n",
       " ('there', 'is', 'nothing'),\n",
       " ('on', 'the', 'path'),\n",
       " ('his', 'hands', 'and'),\n",
       " ('if', 'it', 'was'),\n",
       " ('had', 'said', 'to'),\n",
       " ('said', 'to', 'him'),\n",
       " ('the', 'two', 'young'),\n",
       " ('on', 'their', 'way'),\n",
       " ('the', 'buddha', 'the'),\n",
       " ('him', 'he', 'had'),\n",
       " ('him', 'the', 'exalted'),\n",
       " ('the', 'oldest', 'one'),\n",
       " ('and', 'in', 'his'),\n",
       " ('listen', 'to', 'the'),\n",
       " ('where', 'the', 'buddha'),\n",
       " ('walk', 'the', 'path'),\n",
       " ('that', 'I', 'have'),\n",
       " ('this', 'very', 'same'),\n",
       " ('commanded', 'him', 'to'),\n",
       " ('him', 'to', 'do'),\n",
       " ('teachings', 'from', 'his'),\n",
       " ('in', 'the', 'garden'),\n",
       " ('do', 'you', 'know'),\n",
       " ('him', 'with', 'your'),\n",
       " ('I', 'have', 'seen'),\n",
       " ('walking', 'through', 'the'),\n",
       " ('had', 'spent', 'the'),\n",
       " ('the', 'yellow', 'robe'),\n",
       " ('in', 'his', 'thoughts'),\n",
       " ('and', 'placed', 'his'),\n",
       " ('his', 'quietly', 'dangling'),\n",
       " ('quietly', 'dangling', 'hand'),\n",
       " ('he', 'looked', 'at'),\n",
       " ('never', 'before', 'he'),\n",
       " ('have', 'heard', 'the'),\n",
       " ('heard', 'the', 'teachings'),\n",
       " ('it', 'has', 'come'),\n",
       " ('put', 'an', 'end'),\n",
       " ('exalted', 'one', 'and'),\n",
       " ('you', 'want', 'to'),\n",
       " ('often', 'I', 'have'),\n",
       " ('I', 'have', 'thought'),\n",
       " ('of', 'his', 'own'),\n",
       " ('wish', 'that', 'you'),\n",
       " ('that', 'you', 'would'),\n",
       " ('up', 'to', 'its'),\n",
       " ('to', 'its', 'end'),\n",
       " ('that', 'you', 'shall'),\n",
       " ('refuge', 'with', 'the'),\n",
       " ('had', 'left', 'him'),\n",
       " ('that', 'you', 'are'),\n",
       " ('early', 'in', 'the'),\n",
       " ('a', 'follower', 'of'),\n",
       " ('to', 'him', 'who'),\n",
       " ('in', 'the', 'teachings'),\n",
       " ('and', 'when', 'he'),\n",
       " ('so', 'full', 'of'),\n",
       " ('oh', 'exalted', 'one'),\n",
       " ('never', 'before', 'this'),\n",
       " ('it', 'may', 'be'),\n",
       " ('of', 'all', 'things'),\n",
       " ('this', 'world', 'of'),\n",
       " ('the', 'world', 'of'),\n",
       " ('to', 'those', 'who'),\n",
       " ('“I', 'wish', 'that'),\n",
       " ('would', 'not', 'be'),\n",
       " ('means', 'of', 'teachings'),\n",
       " ('will', 'not', 'be'),\n",
       " ('not', 'be', 'able'),\n",
       " ('has', 'happened', 'to'),\n",
       " ('in', 'the', 'hour'),\n",
       " ('for', 'I', 'know'),\n",
       " ('to', 'the', 'ground'),\n",
       " ('and', 'walk', 'this'),\n",
       " ('I', 'would', 'have'),\n",
       " ('but', 'he', 'has'),\n",
       " ('he', 'felt', 'that'),\n",
       " ('he', 'was', 'no'),\n",
       " ('that', 'one', 'thing'),\n",
       " ('part', 'of', 'him'),\n",
       " ('the', 'wish', 'to'),\n",
       " ('on', 'his', 'path'),\n",
       " ('him', 'had', 'to'),\n",
       " ('to', 'accept', 'his'),\n",
       " ('I', 'sought', 'to'),\n",
       " ('of', 'me', 'being'),\n",
       " ('of', 'him', 'and'),\n",
       " ('opened', 'his', 'eyes'),\n",
       " ('his', 'eyes', 'and'),\n",
       " ('him', 'from', 'his'),\n",
       " ('was', 'not', 'long'),\n",
       " ('any', 'kind', 'of'),\n",
       " ('want', 'to', 'be'),\n",
       " ('for', 'the', 'first'),\n",
       " ('the', 'path', 'to'),\n",
       " ('for', 'the', 'sake'),\n",
       " ('the', 'sake', 'of'),\n",
       " ('front', 'of', 'him'),\n",
       " ('had', 'also', 'become'),\n",
       " ('this', 'moment', 'when'),\n",
       " ('moment', 'when', 'he'),\n",
       " ('I', 'am', 'no'),\n",
       " ('I', 'am', 'not'),\n",
       " ('the', 'time', 'of'),\n",
       " ('had', 'been', 'his'),\n",
       " ('now', 'he', 'was'),\n",
       " ('was', 'nothing', 'but'),\n",
       " ('not', 'belong', 'to'),\n",
       " ('belong', 'to', 'the'),\n",
       " ('in', 'which', 'he'),\n",
       " ('out', 'of', 'this'),\n",
       " ('this', 'had', 'been'),\n",
       " ('and', 'his', 'heart'),\n",
       " ('before', 'his', 'eyes'),\n",
       " ('was', 'not', 'the'),\n",
       " ('the', 'other', 'side'),\n",
       " ('side', 'of', 'the'),\n",
       " ('aware', 'of', 'the'),\n",
       " ('of', 'the', 'forest'),\n",
       " ('he', 'had', 'now'),\n",
       " ('he', 'had', 'really'),\n",
       " ('voices', 'of', 'the'),\n",
       " ('and', 'as', 'he'),\n",
       " ('door', 'of', 'the'),\n",
       " ('it', 'is', 'the'),\n",
       " ('had', 'to', 'go'),\n",
       " ('his', 'innermost', 'self'),\n",
       " ('he', 'no', 'longer'),\n",
       " ('he', 'felt', 'the'),\n",
       " ('he', 'had', 'lived'),\n",
       " ('like', 'a', 'freshly'),\n",
       " ('the', 'grove', 'and'),\n",
       " ('about', 'it', 'and'),\n",
       " ('like', 'this', 'I'),\n",
       " ('the', 'flow', 'of'),\n",
       " ('asked', 'him', 'to'),\n",
       " ('a', 'samana', 'and'),\n",
       " ('with', 'his', 'eyes'),\n",
       " ('to', 'tell', 'me'),\n",
       " ('thank', 'you', 'for'),\n",
       " ('I', 'have', 'already'),\n",
       " ('to', 'learn', 'how'),\n",
       " ('learn', 'how', 'to'),\n",
       " ('how', 'to', 'make'),\n",
       " ('like', 'this', 'it'),\n",
       " ('as', 'a', 'gift'),\n",
       " ('it', 'cannot', 'be'),\n",
       " ('no', 'other', 'way'),\n",
       " ('would', 'you', 'like'),\n",
       " ('it', 'for', 'a'),\n",
       " ('he', 'was', 'and'),\n",
       " ('like', 'a', 'child'),\n",
       " ('to', 'him', 'to'),\n",
       " ('do', 'you', 'have'),\n",
       " ('you', 'have', 'a'),\n",
       " ('when', 'I', 'had'),\n",
       " ('into', 'the', 'water'),\n",
       " ('means', 'of', 'the'),\n",
       " ('if', 'he', 'is'),\n",
       " ('he', 'is', 'able'),\n",
       " ('is', 'able', 'to'),\n",
       " ('it', 'seems', 'to'),\n",
       " ('but', 'like', 'this'),\n",
       " ('as', 'if', 'it'),\n",
       " ('had', 'already', 'been'),\n",
       " ('him', 'that', 'he'),\n",
       " ('it', 'might', 'be'),\n",
       " ('in', 'a', 'friendly'),\n",
       " ('to', 'be', 'the'),\n",
       " ('that', 'there', 'was'),\n",
       " ('there', 'was', 'something'),\n",
       " ('he', 'saw', 'them'),\n",
       " ('which', 'a', 'samana'),\n",
       " ('welcome', 'was', 'the'),\n",
       " ('the', 'story', 'of'),\n",
       " ('story', 'of', 'his'),\n",
       " ('him', 'and', 'the'),\n",
       " ('to', 'understand', 'him'),\n",
       " ('away', 'from', 'him'),\n",
       " ('ran', 'and', 'ran'),\n",
       " ('to', 'do', 'with'),\n",
       " ('on', 'account', 'of'),\n",
       " ('he', 'would', 'also'),\n",
       " ('at', 'him', 'with'),\n",
       " ('siddhartha', 'said', 'nothing'),\n",
       " ('years', 'as', 'a'),\n",
       " ('again', 'he', 'had'),\n",
       " ('people', 'of', 'the'),\n",
       " ('years', 'passed', 'by'),\n",
       " ('himself', 'with', 'a'),\n",
       " ('of', 'a', 'child'),\n",
       " ('inside', 'of', 'him'),\n",
       " ('him', 'at', 'that'),\n",
       " ('in', 'the', 'midst'),\n",
       " ('the', 'midst', 'of'),\n",
       " ('face', 'in', 'the'),\n",
       " ('back', 'into', 'the'),\n",
       " ('they', 'had', 'been'),\n",
       " ('she', 'wanted', 'to'),\n",
       " ('then', 'he', 'had'),\n",
       " ('the', 'beginning', 'of'),\n",
       " ('the', 'night', 'in'),\n",
       " ('the', 'just', 'too'),\n",
       " ('of', 'his', 'life'),\n",
       " ('he', 'had', 'gone'),\n",
       " ('world', 'of', 'the'),\n",
       " ('kamala', 'had', 'been'),\n",
       " ('that', 'he', 'could'),\n",
       " ('thinking', 'of', 'his'),\n",
       " ('that', 'she', 'had'),\n",
       " ('through', 'the', 'forest'),\n",
       " ('and', 'that', 'he'),\n",
       " ('the', 'bird', 'in'),\n",
       " ('was', 'nothing', 'left'),\n",
       " ('if', 'there', 'only'),\n",
       " ('there', 'only', 'was'),\n",
       " ('only', 'was', 'a'),\n",
       " ('had', 'not', 'brought'),\n",
       " ('back', 'at', 'him'),\n",
       " ('let', 'him', 'be'),\n",
       " ('of', 'the', 'tree'),\n",
       " ('and', 'he', 'remembered'),\n",
       " ('had', 'come', 'to'),\n",
       " ('had', 'fallen', 'asleep'),\n",
       " ('nothing', 'but', 'a'),\n",
       " ('the', 'friend', 'of'),\n",
       " ('friend', 'of', 'his'),\n",
       " ('been', 'waiting', 'for'),\n",
       " ('and', 'from', 'the'),\n",
       " ('walk', 'to', 'the'),\n",
       " ('to', 'see', 'you'),\n",
       " ('im', 'on', 'a'),\n",
       " ('where', 'is', 'siddhartha'),\n",
       " ('is', 'siddhartha', 'the'),\n",
       " ('I', 'have', 'no'),\n",
       " ('through', 'so', 'much'),\n",
       " ('the', 'thought', 'of'),\n",
       " ('had', 'to', 'become'),\n",
       " ('how', 'I', 'hated'),\n",
       " ('in', 'him', 'was'),\n",
       " ('this', 'was', 'why'),\n",
       " ('I', 'have', 'experienced'),\n",
       " ('good', 'for', 'me'),\n",
       " ('for', 'me', 'to'),\n",
       " ('he', 'was', 'now'),\n",
       " ('his', 'heart', 'he'),\n",
       " ('of', 'all', 'I'),\n",
       " ('enter', 'his', 'mind'),\n",
       " ('how', 'he', 'had'),\n",
       " ('love', 'for', 'the'),\n",
       " ('with', 'a', 'bright'),\n",
       " ('and', 'at', 'the'),\n",
       " ('one', 'and', 'the'),\n",
       " ('though', 'he', 'was'),\n",
       " ('but', 'the', 'boy'),\n",
       " ('looked', 'at', 'her'),\n",
       " ('that', 'it', 'was'),\n",
       " ('and', 'he', 'saw'),\n",
       " ('to', 'the', 'old'),\n",
       " ('to', 'him', 'he'),\n",
       " ('in', 'any', 'way'),\n",
       " ('know', 'that', 'you'),\n",
       " ('siddhartha', 'saw', 'it'),\n",
       " ('to', 'him', 'in'),\n",
       " ('when', 'he', 'looked'),\n",
       " ('face', 'which', 'he'),\n",
       " ('in', 'the', 'river'),\n",
       " ('the', 'image', 'of'),\n",
       " ('has', 'been', 'my'),\n",
       " ('is', 'a', 'stone'),\n",
       " ('and', 'this', 'is'),\n",
       " ('the', 'trademark', 'license'),\n",
       " ('complying', 'with', 'the'),\n",
       " ('free', 'distribution', 'of'),\n",
       " ('of', 'electronic', 'works'),\n",
       " ('all', 'the', 'terms'),\n",
       " ('not', 'agree', 'to'),\n",
       " ('this', 'agreement', 'you'),\n",
       " ('copies', 'of', 'project'),\n",
       " ('electronic', 'works', 'in'),\n",
       " ('the', 'person', 'or'),\n",
       " ('person', 'or', 'entity'),\n",
       " ('as', 'set', 'forth'),\n",
       " ('forth', 'in', 'paragraph'),\n",
       " ('you', 'can', 'do'),\n",
       " ('can', 'do', 'with'),\n",
       " ('if', 'an', 'individual'),\n",
       " ('we', 'do', 'not'),\n",
       " ('project', 'gutenberg™', 'works'),\n",
       " ('this', 'work', 'or'),\n",
       " ('to', 'the', 'full'),\n",
       " ('phrase', '“project', 'gutenberg”'),\n",
       " ('electronic', 'work', 'is'),\n",
       " ('permission', 'of', 'the'),\n",
       " ('of', 'the', 'copyright'),\n",
       " ('project', 'gutenberg™', 'trademark'),\n",
       " ('this', 'electronic', 'work'),\n",
       " ('any', 'project', 'gutenberg™'),\n",
       " ('donations', 'to', 'the'),\n",
       " ('archive', 'foundation', 'the'),\n",
       " ('of', 'replacement', 'or'),\n",
       " ('you', 'received', 'the'),\n",
       " ('received', 'the', 'work'),\n",
       " ('this', 'ebook', 'is'),\n",
       " ('ebook', 'is', 'for'),\n",
       " ('is', 'for', 'the'),\n",
       " ('use', 'of', 'anyone'),\n",
       " ('of', 'anyone', 'anywhere'),\n",
       " ('anyone', 'anywhere', 'in'),\n",
       " ('anywhere', 'in', 'the'),\n",
       " ('states', 'and', 'most'),\n",
       " ('and', 'most', 'other'),\n",
       " ('most', 'other', 'parts'),\n",
       " ('other', 'parts', 'of'),\n",
       " ('parts', 'of', 'the'),\n",
       " ('the', 'world', 'at'),\n",
       " ('world', 'at', 'no'),\n",
       " ('at', 'no', 'cost'),\n",
       " ('no', 'cost', 'and'),\n",
       " ('cost', 'and', 'with'),\n",
       " ('and', 'with', 'almost'),\n",
       " ('with', 'almost', 'no'),\n",
       " ('almost', 'no', 'restrictions'),\n",
       " ('you', 'may', 'copy'),\n",
       " ('may', 'copy', 'it'),\n",
       " ('copy', 'it', 'give'),\n",
       " ('it', 'give', 'it'),\n",
       " ('give', 'it', 'away'),\n",
       " ('it', 'away', 'or'),\n",
       " ('away', 'or', 're-use'),\n",
       " ('or', 're-use', 'it'),\n",
       " ('re-use', 'it', 'under'),\n",
       " ('it', 'under', 'the'),\n",
       " ('under', 'the', 'terms'),\n",
       " ('gutenberg', 'license', 'included'),\n",
       " ('license', 'included', 'with'),\n",
       " ('included', 'with', 'this'),\n",
       " ('with', 'this', 'ebook'),\n",
       " ('this', 'ebook', 'or'),\n",
       " ('ebook', 'or', 'online'),\n",
       " ('you', 'are', 'not'),\n",
       " ('are', 'not', 'located'),\n",
       " ('not', 'located', 'in'),\n",
       " ('united', 'states', 'you'),\n",
       " ('states', 'you', 'will'),\n",
       " ('you', 'will', 'have'),\n",
       " ('will', 'have', 'to'),\n",
       " ('have', 'to', 'check'),\n",
       " ('to', 'check', 'the'),\n",
       " ('of', 'the', 'country'),\n",
       " ('the', 'country', 'where'),\n",
       " ('country', 'where', 'you'),\n",
       " ('are', 'located', 'before'),\n",
       " ('located', 'before', 'using'),\n",
       " ('before', 'using', 'this'),\n",
       " ('gutenberg', 'ebook', 'siddhartha'),\n",
       " ('ebook', 'siddhartha', '***'),\n",
       " ('with', 'the', 'childlike'),\n",
       " ('friend', 'the', 'son'),\n",
       " ('the', 'brahman', 'in'),\n",
       " ('brahman', 'in', 'the'),\n",
       " ('of', 'the', 'house'),\n",
       " ('in', 'the', 'sunshine'),\n",
       " ('of', 'the', 'fig'),\n",
       " ('the', 'fig', 'tree'),\n",
       " ('the', 'brahman', 'the'),\n",
       " ('his', 'friend', 'govinda'),\n",
       " ('time', 'siddhartha', 'had'),\n",
       " ('the', 'service', 'of'),\n",
       " ('he', 'already', 'knew'),\n",
       " ('to', 'speak', 'the'),\n",
       " ('speak', 'the', 'om'),\n",
       " ('to', 'speak', 'it'),\n",
       " ('speak', 'it', 'silently'),\n",
       " ('of', 'his', 'soul'),\n",
       " ('of', 'his', 'being'),\n",
       " ('leapt', 'in', 'his'),\n",
       " ('he', 'who', 'was'),\n",
       " ('the', 'others', 'he'),\n",
       " ('others', 'he', 'was'),\n",
       " ('he', 'loved', 'everything'),\n",
       " ('he', 'would', 'not'),\n",
       " ('want', 'to', 'become'),\n",
       " ('tens', 'of', 'thousands'),\n",
       " ('wanted', 'to', 'follow'),\n",
       " ('to', 'follow', 'him'),\n",
       " ('him', 'as', 'his'),\n",
       " ('a', 'source', 'of'),\n",
       " ('source', 'of', 'joy'),\n",
       " ('of', 'joy', 'for'),\n",
       " ('but', 'he', 'siddhartha'),\n",
       " ('for', 'himself', 'he'),\n",
       " ('paths', 'of', 'the'),\n",
       " ('of', 'the', 'mango'),\n",
       " ('came', 'into', 'his'),\n",
       " ('from', 'the', 'water'),\n",
       " ('of', 'the', 'sun'),\n",
       " ('verses', 'of', 'the'),\n",
       " ('drop', 'by', 'drop'),\n",
       " ('his', 'mother', 'and'),\n",
       " ('for', 'ever', 'and'),\n",
       " ('ever', 'and', 'ever'),\n",
       " ('to', 'him', 'the'),\n",
       " ('that', 'they', 'had'),\n",
       " ('soul', 'was', 'not'),\n",
       " ('of', 'the', 'gods'),\n",
       " ('a', 'happy', 'fortune'),\n",
       " ('the', 'only', 'one'),\n",
       " ('only', 'one', 'the'),\n",
       " ('was', 'it', 'right'),\n",
       " ('it', 'right', 'was'),\n",
       " ('right', 'was', 'it'),\n",
       " ('to', 'the', 'gods'),\n",
       " ('to', 'be', 'found'),\n",
       " ('where', 'did', 'he'),\n",
       " ('in', 'ones', 'own'),\n",
       " ('ones', 'own', 'self'),\n",
       " ('where', 'where', 'was'),\n",
       " ('the', 'wisest', 'ones'),\n",
       " ('and', 'not', 'the'),\n",
       " ('they', 'knew', 'everything'),\n",
       " ('the', 'brahmans', 'and'),\n",
       " ('the', 'world', 'the'),\n",
       " ('the', 'origin', 'of'),\n",
       " ('important', 'thing', 'the'),\n",
       " ('in', 'his', 'sleep'),\n",
       " ('not', 'to', 'be'),\n",
       " ('wise', 'men', 'or'),\n",
       " ('the', 'state', 'of'),\n",
       " ('state', 'of', 'being'),\n",
       " ('of', 'being', 'awake'),\n",
       " ('into', 'the', 'life'),\n",
       " ('every', 'step', 'of'),\n",
       " ('of', 'the', 'way'),\n",
       " ('the', 'most', 'venerable'),\n",
       " ('his', 'father', 'was'),\n",
       " ('did', 'he', 'have'),\n",
       " ('was', 'he', 'not'),\n",
       " ('a', 'thirsty', 'man'),\n",
       " ('did', 'he', 'not'),\n",
       " ('from', 'the', 'offerings'),\n",
       " ('the', 'pristine', 'source'),\n",
       " ('this', 'was', 'his'),\n",
       " ('he', 'spoke', 'to'),\n",
       " ('the', 'brahman', 'is'),\n",
       " ('but', 'never', 'he'),\n",
       " ('had', 'reached', 'it'),\n",
       " ('reached', 'it', 'completely'),\n",
       " ('among', 'all', 'the'),\n",
       " ('one', 'who', 'had'),\n",
       " ('it', 'completely', 'the'),\n",
       " ('siddhartha', 'spoke', 'to'),\n",
       " ('the', 'banyan', 'tree'),\n",
       " ('to', 'perform', 'the'),\n",
       " ('lost', 'in', 'thought'),\n",
       " ('to', 'the', 'world'),\n",
       " ('siddhartha', 'will', 'go'),\n",
       " ('face', 'of', 'his'),\n",
       " ('with', 'the', 'first'),\n",
       " ('and', 'with', 'his'),\n",
       " ('he', 'spoke', 'quietly'),\n",
       " ('life', 'of', 'the'),\n",
       " ('speak', 'no', 'more'),\n",
       " ('no', 'more', 'of'),\n",
       " ('remained', 'standing', 'there'),\n",
       " ('quoth', 'siddhartha', '“with'),\n",
       " ('I', 'came', 'to'),\n",
       " ('tell', 'you', 'that'),\n",
       " ('and', 'go', 'to'),\n",
       " ('fell', 'silent', 'and'),\n",
       " ('for', 'so', 'long'),\n",
       " ('silent', 'and', 'motionless'),\n",
       " ('and', 'the', 'stars'),\n",
       " ('“what', 'are', 'you'),\n",
       " ('are', 'you', 'waiting'),\n",
       " ('you', 'waiting', 'for”'),\n",
       " ('quoth', 'siddhartha', '“you'),\n",
       " ('siddhartha', '“you', 'know'),\n",
       " ('he', 'went', 'to'),\n",
       " ('went', 'to', 'his'),\n",
       " ('his', 'bed', 'and'),\n",
       " ('after', 'an', 'hour'),\n",
       " ('hour', 'since', 'no'),\n",
       " ('since', 'no', 'sleep'),\n",
       " ('no', 'sleep', 'had'),\n",
       " ('sleep', 'had', 'come'),\n",
       " ('had', 'come', 'over'),\n",
       " ('come', 'over', 'his'),\n",
       " ('over', 'his', 'eyes'),\n",
       " ('his', 'eyes', 'the'),\n",
       " ('eyes', 'the', 'brahman'),\n",
       " ('the', 'brahman', 'stood'),\n",
       " ('brahman', 'stood', 'up'),\n",
       " ('paced', 'to', 'and'),\n",
       " ('to', 'and', 'fro'),\n",
       " ('through', 'the', 'small'),\n",
       " ('window', 'of', 'the'),\n",
       " ('of', 'the', 'chamber'),\n",
       " ('the', 'chamber', 'he'),\n",
       " ('chamber', 'he', 'looked'),\n",
       " ('he', 'looked', 'back'),\n",
       " ('looked', 'back', 'inside'),\n",
       " ('saw', 'siddhartha', 'standing'),\n",
       " ('not', 'moving', 'from'),\n",
       " ('moving', 'from', 'his'),\n",
       " ('his', 'heart', 'the'),\n",
       " ('heart', 'the', 'father'),\n",
       " ('and', 'saw', 'that'),\n",
       " ('saw', 'that', 'the'),\n",
       " ('the', 'moon', 'had'),\n",
       " ('and', 'he', 'came'),\n",
       " ('came', 'back', 'after'),\n",
       " ('looked', 'through', 'the'),\n",
       " ('standing', 'in', 'the'),\n",
       " ('of', 'the', 'stars'),\n",
       " ('the', 'day', 'began'),\n",
       " ('“siddhartha”', 'he', 'spoke'),\n",
       " ('“and', 'would', 'you'),\n",
       " ('the', 'first', 'light'),\n",
       " ('first', 'light', 'of'),\n",
       " ('light', 'of', 'day'),\n",
       " ('saw', 'that', 'siddhartha'),\n",
       " ('that', 'siddhartha', 'was'),\n",
       " ('into', 'the', 'forest'),\n",
       " ('he', 'took', 'his'),\n",
       " ('his', 'son', 'and'),\n",
       " ('to', 'his', 'father'),\n",
       " ('in', 'the', 'first'),\n",
       " ('have', 'come”', 'said'),\n",
       " ('said', 'siddhartha', 'and'),\n",
       " ('the', 'samanas', 'in'),\n",
       " ('samanas', 'in', 'the'),\n",
       " ('of', 'this', 'day'),\n",
       " ('up', 'with', 'the'),\n",
       " ('to', 'a', 'poor'),\n",
       " ('once', 'a', 'day'),\n",
       " ('a', 'day', 'and'),\n",
       " ('he', 'fasted', 'for'),\n",
       " ('look', 'from', 'his'),\n",
       " ('it', 'all', 'stank'),\n",
       " ('of', 'joy', 'and'),\n",
       " ('himself', 'not', 'to'),\n",
       " ('self', 'any', 'more'),\n",
       " ('which', 'is', 'no'),\n",
       " ('stood', 'there', 'until'),\n",
       " ('there', 'until', 'he'),\n",
       " ('there', 'in', 'the'),\n",
       " ('in', 'the', 'rainy'),\n",
       " ('could', 'not', 'feel'),\n",
       " ('cold', 'in', 'his'),\n",
       " ('more', 'until', 'nothing'),\n",
       " ('were', 'only', 'a'),\n",
       " ('only', 'a', 'few'),\n",
       " ('according', 'to', 'a'),\n",
       " ('and', 'siddharthas', 'soul'),\n",
       " ('of', 'the', 'cycle'),\n",
       " ('he', 'killed', 'his'),\n",
       " ('of', 'his', 'self'),\n",
       " ('his', 'self', 'into'),\n",
       " ('in', 'the', 'cycle'),\n",
       " ('learned', 'a', 'lot'),\n",
       " ('he', 'was', 'with'),\n",
       " ('went', 'the', 'way'),\n",
       " ('the', 'way', 'of'),\n",
       " ('way', 'of', 'self-denial'),\n",
       " ('of', 'self-denial', 'by'),\n",
       " ('self-denial', 'by', 'means'),\n",
       " ('a', 'thousand', 'times'),\n",
       " ('back', 'to', 'the'),\n",
       " ('fled', 'from', 'the'),\n",
       " ('the', 'hour', 'when'),\n",
       " ('or', 'in', 'the'),\n",
       " ('once', 'again', 'his'),\n",
       " ('the', 'agony', 'of'),\n",
       " ('which', 'had', 'been'),\n",
       " ('went', 'through', 'the'),\n",
       " ('to', 'beg', 'for'),\n",
       " ('for', 'themselves', 'and'),\n",
       " ('“how', 'do', 'you'),\n",
       " ('spoke', 'one', 'day'),\n",
       " ...]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams_to_learn = ngrams_up_to_20[1].copy()\n",
    "[sent[0] for sent in my_filter(trigrams_to_learn.most_common())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f17a8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_example.extend([[word_to_index[w] for w in sent[0][:-1]] for sent in my_filter(trigrams_to_learn.most_common())\n",
    "               if all([w in word_to_index for w in sent[0]])])\n",
    "y_train_example.extend([[word_to_index[w] for w in sent[0][1:]] for sent in my_filter(trigrams_to_learn.most_common())\n",
    "               if all([w in word_to_index for w in sent[0]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7008cc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7213, 7213)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_example), len(y_train_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b444539a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[178], [142], [51], [149], [11], [577], [23], [156], [680], [12]],\n",
       " [[142], [10], [149], [116], [433], [6], [679], [680], [163], [681]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_example[1575:1585], y_train_example[1575:1585]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "259900ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_to_learn = ngrams_up_to_20[0]\n",
    "X_train_2 = [[word_to_index[sent[0][0]]] for sent in my_filter(bigrams_to_learn.most_common())\n",
    "                  if sent[0][0] in word_to_index and sent[0][1] in word_to_index]\n",
    "y_train_2 = [[word_to_index[sent[0][1]]] for sent in my_filter(bigrams_to_learn.most_common())\n",
    "                  if sent[0][0] in word_to_index and sent[0][1] in word_to_index]\n",
    "X_train_2, y_train_2 = fisher_yates(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a521ab13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4836, 4836)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_2), len(y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0abef1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[35], [15], [122], [35], [47], [1129], [130], [22], [54], [7]],\n",
       " [[973], [110], [9], [92], [12], [1130], [232], [48], [13], [225]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2[0:10], y_train_2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d5eb464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.extend(X_train_2)\n",
    "y_train.extend(y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "05f9e659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6150, 6150)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b219c036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([633], [36]),\n",
       " ([630], [9]),\n",
       " ([27], [316]),\n",
       " ([0], [117]),\n",
       " ([254], [904]),\n",
       " ([0], [637]),\n",
       " ([1033], [6]),\n",
       " ([1510], [33]),\n",
       " ([0], [88]),\n",
       " ([145], [26])]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list(zip(X_train, y_train)), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b64cd4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', 'long', 'time'), 56),\n",
       " (('for', 'a', 'long'), 47),\n",
       " (('the', 'project', 'gutenberg'), 20),\n",
       " (('he', 'had', 'been'), 19),\n",
       " (('the', 'exalted', 'one'), 18),\n",
       " (('project', 'gutenberg™', 'electronic'), 18),\n",
       " (('in', 'order', 'to'), 16),\n",
       " (('which', 'he', 'had'), 15),\n",
       " (('in', 'his', 'heart'), 14),\n",
       " (('that', 'he', 'had'), 14)]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_to_learn = ngrams_up_to_20[1]\n",
    "ngrams_to_learn.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8893628b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'long', 'time'),\n",
       " ('for', 'a', 'long'),\n",
       " ('the', 'project', 'gutenberg'),\n",
       " ('he', 'had', 'been'),\n",
       " ('the', 'exalted', 'one'),\n",
       " ('project', 'gutenberg™', 'electronic'),\n",
       " ('in', 'order', 'to'),\n",
       " ('which', 'he', 'had'),\n",
       " ('in', 'his', 'heart'),\n",
       " ('that', 'he', 'had')]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sent[0] for sent in my_filter(ngrams_to_learn.most_common(10))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dfab80fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[82, 13], [23, 1], [150, 37], [398, 10], [145, 26]],\n",
       " [[13, 161], [1, 81], [37, 1], [10, 7], [26, 16]],\n",
       " 2377,\n",
       " 2377)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2 = [[word_to_index[w] for w in sent[0][:-1]] for sent in my_filter(ngrams_to_learn.most_common())\n",
    "               if all([w in word_to_index for w in sent[0]])]\n",
    "y_train_2 = [[word_to_index[w] for w in sent[0][1:]] for sent in my_filter(ngrams_to_learn.most_common())\n",
    "               if all([w in word_to_index for w in sent[0]])]\n",
    "X_train_2, y_train_2 = fisher_yates(X_train_2, y_train_2)\n",
    "X_train_2[0:5], y_train_2[0:5], len(X_train_2), len(y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "fe69858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_filter(ngrams):\n",
    "    return filter(remove_periods, ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4c9f5030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[66, 8], [436, 6], [1, 1911], [1, 1632], [42, 120]],\n",
       " [[8, 11], [6, 1], [1911, 6], [1632, 429], [120, 37]],\n",
       " 2000,\n",
       " 2000)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2 = [[word_to_index[w] for w in sent[0][:-1]] for sent in my_filter(ngrams_to_learn.most_common())\n",
    "               if all([w in word_to_index for w in sent[0]])]\n",
    "y_train_2 = [[word_to_index[w] for w in sent[0][1:]] for sent in my_filter(ngrams_to_learn.most_common())\n",
    "               if all([w in word_to_index for w in sent[0]])]\n",
    "X_train_2 = X_train_2[:2000]\n",
    "y_train_2 = y_train_2[:2000]\n",
    "X_train_2, y_train_2 = fisher_yates(X_train_2, y_train_2)\n",
    "X_train_2[0:5], y_train_2[0:5], len(X_train_2), len(y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4618d9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7, 64], [22, 7], [1, 73], [8, 11], [1, 173]] [[64, 43], [7, 64], [73, 196], [11, 45], [173, 30]] 31793 31793\n"
     ]
    }
   ],
   "source": [
    "ngrams_to_learn = ngrams_up_to_20[1]\n",
    "X_train_2 = [[word_to_index[w] for w in sent[0][:-1]] for sent in my_filter(ngrams_to_learn.most_common())\n",
    "               if all([w in word_to_index for w in sent[0]])]\n",
    "y_train_2 = [[word_to_index[w] for w in sent[0][1:]] for sent in my_filter(ngrams_to_learn.most_common())\n",
    "               if all([w in word_to_index for w in sent[0]])]\n",
    "print(X_train_2[0:5], y_train_2[0:5], len(X_train_2), len(y_train_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "067ad4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['SENTENCE_END']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d8348ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_eos(trigram):\n",
    "    if trigram[1] == word_to_index['SENTENCE_END']:\n",
    "          return True  \n",
    "    return False\n",
    "\n",
    "trigrams_eos = list(filter(check_eos, y_train_2))\n",
    "len(trigrams_eos), trigrams_eos[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "db334d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 18/18 [00:03<00:00,  5.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(1, len(ngrams_up_to_20))):\n",
    "    ngrams_to_learn = ngrams_up_to_20[i]\n",
    "    X_train_2 = [[word_to_index[w] for w in sent[0][:-1]] for sent in my_filter(ngrams_to_learn.most_common())\n",
    "                   if all([w in word_to_index for w in sent[0]])]\n",
    "    y_train_2 = [[word_to_index[w] for w in sent[0][1:]] for sent in my_filter(ngrams_to_learn.most_common())\n",
    "                   if all([w in word_to_index for w in sent[0]])]\n",
    "    X_train_2 = X_train_2[:2000]\n",
    "    y_train_2 = y_train_2[:2000]\n",
    "    X_train_2, y_train_2 = fisher_yates(X_train_2, y_train_2)\n",
    "    X_train.extend(X_train_2)\n",
    "    y_train.extend(y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a8660d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42150, 42150)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "559730e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([7, 73, 125], [73, 125, 238]), ([910], [4]), ([1300, 1688, 12, 2415, 34, 2416, 12, 2417, 34, 2418], [1688, 12, 2415, 34, 2416, 12, 2417, 34, 2418, 214]), ([427, 24, 22, 1], [24, 22, 1, 325]), ([31, 150, 1, 1629, 431, 10, 239, 178, 142, 14, 11, 5, 31], [150, 1, 1629, 431, 10, 239, 178, 142, 14, 11, 5, 31, 897]), ([11], [23]), ([1248], [4]), ([374, 298, 15], [298, 15, 72]), ([6, 1, 95, 1, 1621, 6, 893, 6, 306, 6, 1585, 6, 1586, 1, 2300, 6, 1], [1, 95, 1, 1621, 6, 893, 6, 306, 6, 1585, 6, 1586, 1, 2300, 6, 1, 381]), ([25, 30, 4, 88], [30, 4, 88, 163])]\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(list(zip(X_train, y_train)), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "28a04ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1476"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "eb29ec3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SENTENCE_START',\n",
       " 'SENTENCE_START',\n",
       " 'he',\n",
       " 'saw',\n",
       " 'merchants',\n",
       " 'trading',\n",
       " 'princes',\n",
       " 'hunting',\n",
       " 'mourners',\n",
       " 'wailing',\n",
       " 'for',\n",
       " 'their',\n",
       " 'dead',\n",
       " 'whores',\n",
       " 'offering',\n",
       " 'themselves',\n",
       " 'physicians',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'help',\n",
       " 'the',\n",
       " 'sick',\n",
       " 'priests',\n",
       " 'determining',\n",
       " 'the',\n",
       " 'most',\n",
       " 'suitable',\n",
       " 'day',\n",
       " 'for',\n",
       " 'seeding',\n",
       " 'lovers',\n",
       " 'loving',\n",
       " 'mothers',\n",
       " 'nursing',\n",
       " 'their',\n",
       " 'children—and',\n",
       " 'all',\n",
       " 'of',\n",
       " 'this',\n",
       " 'was',\n",
       " 'not',\n",
       " 'worthy',\n",
       " 'of',\n",
       " 'one',\n",
       " 'look',\n",
       " 'from',\n",
       " 'his',\n",
       " 'eye',\n",
       " 'it',\n",
       " 'all',\n",
       " 'lied',\n",
       " 'it',\n",
       " 'all',\n",
       " 'stank',\n",
       " 'it',\n",
       " 'all',\n",
       " 'stank',\n",
       " 'of',\n",
       " 'lies',\n",
       " 'it',\n",
       " 'all',\n",
       " 'pretended',\n",
       " 'to',\n",
       " 'be',\n",
       " 'meaningful',\n",
       " 'and',\n",
       " 'joyful',\n",
       " 'and',\n",
       " 'beautiful',\n",
       " 'and',\n",
       " 'it',\n",
       " 'all',\n",
       " 'was',\n",
       " 'just',\n",
       " 'concealed',\n",
       " 'putrefaction',\n",
       " 'SENTENCE_EN',\n",
       " 'SENTENCE_END']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "49e2e5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 8,\n",
       " 60,\n",
       " 1297,\n",
       " 1676,\n",
       " 1677,\n",
       " 1298,\n",
       " 2379,\n",
       " 1678,\n",
       " 22,\n",
       " 92,\n",
       " 417,\n",
       " 2380,\n",
       " 1679,\n",
       " 441,\n",
       " 2381,\n",
       " 1299,\n",
       " 5,\n",
       " 545,\n",
       " 1,\n",
       " 1044,\n",
       " 1625,\n",
       " 2382,\n",
       " 1,\n",
       " 156,\n",
       " 1680,\n",
       " 146,\n",
       " 22,\n",
       " 2383,\n",
       " 1681,\n",
       " 1045,\n",
       " 673,\n",
       " 2384,\n",
       " 92,\n",
       " 2385,\n",
       " 33,\n",
       " 6,\n",
       " 13,\n",
       " 12,\n",
       " 23,\n",
       " 911,\n",
       " 6,\n",
       " 30,\n",
       " 312,\n",
       " 26,\n",
       " 9,\n",
       " 1247,\n",
       " 14,\n",
       " 33,\n",
       " 2386,\n",
       " 14,\n",
       " 33,\n",
       " 1300,\n",
       " 14,\n",
       " 33,\n",
       " 1300,\n",
       " 6,\n",
       " 2387,\n",
       " 14,\n",
       " 33,\n",
       " 2388,\n",
       " 5,\n",
       " 31,\n",
       " 1616,\n",
       " 4,\n",
       " 789,\n",
       " 4,\n",
       " 164,\n",
       " 4,\n",
       " 14,\n",
       " 33,\n",
       " 12,\n",
       " 89,\n",
       " 1046,\n",
       " 2389,\n",
       " 2,\n",
       " 3]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[word_to_index[w] for w in sent] for sent in tokenized_sentences if all([w in word_to_index for w in sent])][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "27f32954",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full_sentences = [[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences\n",
    "                         if all([w in word_to_index for w in sent])]\n",
    "y_train_full_sentences = [[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences\n",
    "                         if all([w in word_to_index for w in sent])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4e6a65b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 1, 73, 196, 427, 6, 21, 13, 427, 24, 22, 1, 325, 6, 878, 1570, 10, 1, 374, 298, 4, 156, 104, 1233, 6, 1, 95, 37, 40, 1234, 4, 17, 569, 40, 1571, 1572, 2], [0, 0, 15, 215, 458, 14, 197, 14, 145, 39, 1573, 14, 216, 1, 272, 6, 1, 73, 196, 343, 1235, 17, 13, 427, 39, 1003, 37, 879, 2], [0, 0, 66, 15, 46, 23, 667, 10, 1, 374, 298, 15, 72, 29, 5, 1004, 1, 523, 6, 1, 764, 103, 15, 46, 667, 108, 765, 13, 427, 2], [0, 0, 1574, 21, 2229, 2230, 1575, 2231, 1236, 2232, 766, 1576, 2233, 427, 2234, 2235, 2236, 156, 2237, 1577, 2238, 2239, 2240, 1005, 2241, 2242, 1578, 2243, 2244, 2245, 4, 2246, 2247, 375, 375, 375, 486, 6, 1, 73, 196, 427, 21, 375, 375, 375, 21, 76, 2248, 1237, 34, 2249, 1575, 375, 375, 375, 1006, 246, 186, 1, 136, 6, 1, 137, 17, 1, 147, 177, 668, 669, 186, 91, 17, 1, 286, 86, 428, 34, 1, 54, 1, 187, 1, 136, 217, 42, 246, 186, 5, 2250, 2251, 36, 198, 102, 1, 136, 6, 1, 137, 10, 1, 524, 6, 1, 287, 10, 1, 1579, 6, 1, 2252, 459, 1, 1580, 10, 1, 524, 6, 1, 2253, 166, 10, 1, 524, 6, 1, 767, 429, 24, 103, 21, 1007, 87, 1, 1238, 136, 6, 1, 137, 1, 162, 2254, 376, 17, 9, 102, 42, 136, 6, 7, 137, 2], [0, 0, 1, 487, 2255, 9, 377, 880, 34, 1, 1239, 6, 1, 54, 41, 1581, 1008, 1, 881, 1240, 1, 881, 378, 2]] [[0, 1, 73, 196, 427, 6, 21, 13, 427, 24, 22, 1, 325, 6, 878, 1570, 10, 1, 374, 298, 4, 156, 104, 1233, 6, 1, 95, 37, 40, 1234, 4, 17, 569, 40, 1571, 1572, 2, 3], [0, 15, 215, 458, 14, 197, 14, 145, 39, 1573, 14, 216, 1, 272, 6, 1, 73, 196, 343, 1235, 17, 13, 427, 39, 1003, 37, 879, 2, 3], [0, 66, 15, 46, 23, 667, 10, 1, 374, 298, 15, 72, 29, 5, 1004, 1, 523, 6, 1, 764, 103, 15, 46, 667, 108, 765, 13, 427, 2, 3], [0, 1574, 21, 2229, 2230, 1575, 2231, 1236, 2232, 766, 1576, 2233, 427, 2234, 2235, 2236, 156, 2237, 1577, 2238, 2239, 2240, 1005, 2241, 2242, 1578, 2243, 2244, 2245, 4, 2246, 2247, 375, 375, 375, 486, 6, 1, 73, 196, 427, 21, 375, 375, 375, 21, 76, 2248, 1237, 34, 2249, 1575, 375, 375, 375, 1006, 246, 186, 1, 136, 6, 1, 137, 17, 1, 147, 177, 668, 669, 186, 91, 17, 1, 286, 86, 428, 34, 1, 54, 1, 187, 1, 136, 217, 42, 246, 186, 5, 2250, 2251, 36, 198, 102, 1, 136, 6, 1, 137, 10, 1, 524, 6, 1, 287, 10, 1, 1579, 6, 1, 2252, 459, 1, 1580, 10, 1, 524, 6, 1, 2253, 166, 10, 1, 524, 6, 1, 767, 429, 24, 103, 21, 1007, 87, 1, 1238, 136, 6, 1, 137, 1, 162, 2254, 376, 17, 9, 102, 42, 136, 6, 7, 137, 2, 3], [0, 1, 487, 2255, 9, 377, 880, 34, 1, 1239, 6, 1, 54, 41, 1581, 1008, 1, 881, 1240, 1, 881, 378, 2, 3]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_full_sentences[0:5], y_train_full_sentences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a7104083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['his', 'deep', 'sleep', 'would', 'meet', 'with', 'his', 'innermost', 'part', 'and', 'would', 'reside', 'in', 'the', 'atman', 'SENTENCE_EN', 'SENTENCE_END'], ['how', 'youre', 'able', 'to', 'write', '”', 'the', 'merchant', 'praised', 'him', 'SENTENCE_EN', 'SENTENCE_END'], ['with', 'this', 'trip', 'SENTENCE_EN', 'SENTENCE_END'], ['gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'SENTENCE_EN', 'SENTENCE_END'], ['proper', 'it', 'is', 'for', 'a', 'brahman', 'to', 'speak', 'harsh', 'and', 'angry', 'words', 'SENTENCE_EN', 'SENTENCE_END'], ['loudly', 'and', 'used', 'crude', 'swearwords', 'SENTENCE_EN', 'SENTENCE_END'], ['one', 'in', 'the', 'grove', 'jetavana', '”', '“', 'youre', 'siddhartha', '”', 'govinda', 'exclaimed', 'loudly', 'SENTENCE_EN', 'SENTENCE_END'], ['the', 'beginning', 'and', 'as', 'a', 'child', 'again', 'he', 'had', 'to', 'smile', 'SENTENCE_EN', 'SENTENCE_END'], ['he', 'used', 'to', 'in', 'the', 'spring', 'of', 'his', 'years', 'compare', 'this', 'mouth', 'with', 'a', 'freshly', 'cracked', 'fig', 'SENTENCE_EN', 'SENTENCE_END'], ['time', 'he', 'had', 'not', 'known', 'such', 'a', 'sleep', 'any', 'more', 'SENTENCE_EN', 'SENTENCE_END']]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "last_n_words = []\n",
    "for i in range(3, 20):\n",
    "    tokenized_sentences_400 = random.sample(list(tokenized_sentences), 400)\n",
    "    for s in tokenized_sentences_400:\n",
    "        last_n_words.append(s[::-1][:i][::-1])\n",
    "\n",
    "print(random.sample(last_n_words, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5d3ea085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6800"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(last_n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2e8d4189",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_eos = [[word_to_index[w] for w in sent[:-1]] for sent in last_n_words\n",
    "                         if all([w in word_to_index for w in sent])]\n",
    "y_train_eos = [[word_to_index[w] for w in sent[1:]] for sent in last_n_words\n",
    "                         if all([w in word_to_index for w in sent])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d053eb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6800, 6800)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_eos), len(y_train_eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d15bdfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.extend(X_train_eos)\n",
    "y_train.extend(y_train_eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8320b58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48950, 48950)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "20754601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/X_train_siddhartha.pkl', 'wb') as file:\n",
    "    pickle.dump(X_train, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "33285f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/y_train_siddhartha.pkl', 'wb') as file:\n",
    "    pickle.dump(y_train, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e320653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tokenized_sentences_siddhartha.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenized_sentences, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8cc016d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/word_to_index_siddhartha.pkl', 'wb') as file:\n",
    "    pickle.dump(word_to_index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5ab84bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/index_to_word_siddhartha.pkl', 'wb') as file:\n",
    "    pickle.dump(index_to_word, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f7dac0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = np.asarray(X_train,dtype=object)\n",
    "y_train2 = np.asarray(y_train,dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d20904b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48950,), (48950,))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape, y_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "aef8b661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([427, 39, 1003], [39, 1003, 37]), ([6, 13, 493, 4], [13, 493, 4, 775]), ([1], [387]), ([0, 0, 19, 855, 17, 15, 208, 1037, 20, 67, 21, 2], [0, 19, 855, 17, 15, 208, 1037, 20, 67, 21, 2, 3]), ([186, 13, 775, 186, 14, 12, 23, 1618, 4, 2298, 14, 12, 275], [13, 775, 186, 14, 12, 23, 1618, 4, 2298, 14, 12, 275, 79]), ([45, 120], [120, 307]), ([1, 88, 30, 1, 1615, 30, 80, 1], [88, 30, 1, 1615, 30, 80, 1, 360]), ([122, 1, 222, 902, 6, 1, 688], [1, 222, 902, 6, 1, 688, 8]), ([20, 19, 1, 154, 207, 165, 5, 586, 91, 20, 2896, 21, 2], [19, 1, 154, 207, 165, 5, 586, 91, 20, 2896, 21, 2, 3]), ([0], [105])]\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(list(zip(X_train2, y_train2)), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b1ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fad511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e8876e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95232c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da1b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d0d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e6b32b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4089, 100)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocabulary_size, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "fe873107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#glove_dir = 'data/glove'\n",
    "glove_dir = \"data\"\n",
    "\n",
    "embeddings_index = {} #initialize dictionary\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding='utf8')\n",
    "try:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "except:\n",
    "    print(line)\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "217b251c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4089"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9efa65c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
    "for word, i in vocab:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < vocabulary_size:\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c6be09a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4089, 100)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1d449abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('joy', 31)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "10d6e5a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.39159   ,  0.22118001,  0.81884003, -0.48398   , -0.57314003,\n",
       "        0.083019  , -0.20906   , -0.074538  ,  0.049359  , -0.55949998,\n",
       "       -0.32308999,  0.57011998, -0.21456   , -0.41084999,  0.29183   ,\n",
       "        0.17476   , -0.96956998,  0.048109  ,  0.47062999,  0.74265999,\n",
       "        0.74690002,  1.02139997, -0.13095   , -0.67132002,  0.37097999,\n",
       "        0.43346   , -0.079043  , -0.53241998,  0.16960999,  0.28220001,\n",
       "       -0.40671   ,  0.40191999, -0.23286   , -0.44812   ,  0.16073   ,\n",
       "        0.266     , -0.57449001,  0.17587   ,  0.60320997, -0.29776999,\n",
       "        0.17654   , -0.76122999,  0.10279   , -0.47314   , -0.76828998,\n",
       "       -0.29628   ,  0.51100999,  0.59928   ,  0.64578998, -1.18060005,\n",
       "        0.084544  , -0.59182   ,  0.1964    ,  0.88892001, -0.34691   ,\n",
       "       -2.38919997, -0.12136   , -0.17922001,  0.87950999, -0.08393   ,\n",
       "        0.21187   ,  1.3937    , -1.33019996,  0.54578   ,  0.18774   ,\n",
       "       -0.27192   ,  0.50072998, -0.10156   ,  0.20821001,  0.21246   ,\n",
       "       -0.23405001, -0.16276   ,  0.27208999,  0.14825   ,  0.63217998,\n",
       "        0.44681001, -0.41562   ,  0.08081   ,  0.094626  ,  0.12711   ,\n",
       "       -0.23082   , -0.56564999, -0.66153997, -0.21696   , -1.58570004,\n",
       "       -0.045807  ,  0.29936001,  0.14169   , -0.65140003, -0.38797   ,\n",
       "        0.077916  ,  0.43711999, -0.47536001,  0.0043289 ,  0.18086   ,\n",
       "       -0.42750999,  0.027146  , -0.33048001,  0.010488  ,  0.54263002])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fa48b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def find_closest_embeddings(embedding):\n",
    "    return sorted(embeddings_index.keys(), key=lambda word: spatial.distance.euclidean(embeddings_index[word], embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "736be635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prince', 'queen', 'monarch', 'brother', 'uncle']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest_embeddings(embeddings_index[\"king\"])[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5fd3df52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flashlight', 'twig', 'clipboard', 'shove', 'hand', 'fingers', 'clutching', 'clutched', 'tossing', 'stroking']\n"
     ]
    }
   ],
   "source": [
    "print(find_closest_embeddings(\n",
    "    embeddings_index[\"twig\"] - embeddings_index[\"branch\"] + embeddings_index[\"hand\"]\n",
    ")[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "04c0f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# tsne = TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c303e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words =  list(embeddings_index.keys())[:500]\n",
    "# vectors = [embeddings_index[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "3e357fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = tsne.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3937adac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4089, 100)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size, embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2560833",
   "metadata": {},
   "source": [
    "### 4. Model Architecture\n",
    "We define the architecture of our RNN model:\n",
    "\n",
    "Embedding Layer: Maps input indices to dense vectors of fixed size.\n",
    "SimpleRNN Layer: A simple recurrent layer that learns dependencies from the sequences.\n",
    "Dense Layer: Outputs the predicted word by applying a softmax over the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "69f9c68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        # Randomly initialize the network parameters\n",
    "        #self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        #self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, embedding_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        \n",
    "        # Set GLOVE embeddings matrix\n",
    "        self.G = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "55fa9d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    # sometimes, may want to do this first:\n",
    "    #x = np.vectorize(round)(x)\n",
    "    \n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ef282db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(self, x):\n",
    "    # The total number of time steps\n",
    "    T = len(x)\n",
    "    \n",
    "    # During forward propagation we save all hidden states in s because need them later.\n",
    "    # We add one additional element for the initial hidden, which we set to 0\n",
    "    s = np.zeros((T + 1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    \n",
    "    # The outputs at each time step. Again, we save them for later.\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    \n",
    "    # For each time step...\n",
    "    for t in np.arange(T):\n",
    "        # embedding of x[t]:\n",
    "        e_t = self.G[x[t]]\n",
    "                             \n",
    "        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "        #s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        s[t] = np.tanh(self.U.dot(e_t) + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "        \n",
    "    return [o, s]\n",
    "\n",
    "RNN.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "f3a1b4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (4089, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4089,),\n",
       " array([0.00024456, 0.00024456, 0.00024456, ..., 0.00024456, 0.00024456,\n",
       "        0.00024456]))"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dim = vocabulary_size\n",
    "hidden_dim = 100\n",
    "embedding_dim = 100\n",
    "U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, embedding_dim))\n",
    "W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "x = np.random.randint(0, high=3000, size=word_dim)\n",
    "T = len(x)\n",
    "s = np.zeros((T + 1, hidden_dim))\n",
    "s_m1 = np.zeros(hidden_dim)\n",
    "o = np.zeros((T, word_dim))\n",
    "e_0 = embedding_matrix[x[0]]\n",
    "s_0 = np.tanh(U.dot(e_0) + W.dot(s_m1))\n",
    "print(s_0.shape, V.shape)\n",
    "o_0 = softmax(V.dot(s_0))\n",
    "o_0.shape, o_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9f25cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o[-1], axis=1)\n",
    "\n",
    "RNN.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6a106d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "\n",
    "RNN.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "933ebe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print (\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in X_train2[1000]]), X_train2[1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "fd2de6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "melting from the beams of the sun dreams\n",
      "[2285, 26, 1, 2286, 6, 1, 487, 773]\n"
     ]
    }
   ],
   "source": [
    "print (\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in X_train2[20000]]), X_train2[20000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "decbd19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "it silently out of himself while exhaling with all the concentration of his\n",
      "[14, 326, 111, 6, 59, 209, 1586, 17, 33, 1, 2259, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "print (\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in X_train2[30000]]), X_train2[30000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ebeff454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4089, [9, 570, 4])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size, X_train2[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d5409819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4089) [[0.00024803 0.00024104 0.00023871 ... 0.00024342 0.00022586 0.0002446 ]\n",
      " [0.00024418 0.00025014 0.000246   ... 0.00023526 0.00024535 0.00024223]\n",
      " [0.00024463 0.00023684 0.00023286 ... 0.00024458 0.00024566 0.0002521 ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(17)\n",
    "model = RNN(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train2[10000])\n",
    "print (o.shape, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "47c33dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "970"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(o[-1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d979b12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18,) [3342 3114 1447 3096 3114 2041  359  431 1447 3096 3114 1727 3096 2567\n",
      "  143  431 1447 3096]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train2[40000])\n",
    "print(predictions.shape, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e40516e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "delude company bottom rot company manifestations high source bottom rot company murky rot represented same source bottom rot\n"
     ]
    }
   ],
   "source": [
    "print (\"x:\\n%s\" % (\" \".join([index_to_word[x] for x in predictions])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "54625244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    # For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    "\n",
    "def calculate_loss(self, x, y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N\n",
    "\n",
    "RNN.calculate_total_loss = calculate_total_loss\n",
    "RNN.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "68565d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 8.316056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jm/3w1dz5w55nj8bqyr7t76_qr00000gn/T/ipykernel_55450/1857118698.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  N = np.sum((len(y_i) for y_i in y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual loss: 8.316056\n"
     ]
    }
   ],
   "source": [
    "# Limit to 1000 examples to save time\n",
    "print (\"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print (\"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0ee5e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    \n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    \n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    \n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        \n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        \n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            \n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            #dLdU[:,x[bptt_step]] += delta_t\n",
    "            dLdU += np.outer(delta_t, self.G[x[bptt_step]]) \n",
    "            \n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "            \n",
    "    return [dLdU, dLdV, dLdW]\n",
    "\n",
    "RNN.bptt = bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f0cbe55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    \n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = model.bptt(x, y)\n",
    "    \n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    \n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        \n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print(\"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape)))\n",
    "               \n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "               \n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "               \n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = model.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = model.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "               \n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "               \n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "               \n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient) / (\n",
    "                                np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            \n",
    "               # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print( \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix))\n",
    "                print( \"+h Loss: %f\" % gradplus)\n",
    "                print( \"-h Loss: %f\" % gradminus)\n",
    "                print( \"Estimated_gradient: %f\" % estimated_gradient)\n",
    "                print( \"Backpropagation gradient: %f\" % backprop_gradient)\n",
    "                print( \"Relative Error: %f\" % relative_error)\n",
    "                return \n",
    "            it.iternext()\n",
    "               \n",
    "        print( \"Gradient check for parameter %s passed.\" % (pname))\n",
    "\n",
    "RNN.gradient_check = gradient_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "d0fcf292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNN(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "1fdafb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs one step of SGD.\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    \n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "\n",
    "RNN.sgd_step = numpy_sdg_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "67d32552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    \n",
    "    for epoch in range(nepoch):\n",
    "        \n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print (\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "            \n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5  \n",
    "                print (\"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            \n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "724c4ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4089"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "89dca235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.84 ms ± 1.15 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(17)\n",
    "model = RNN(vocabulary_size)\n",
    "%timeit model.sgd_step(X_train2[10000], y_train2[10000], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b9d8fad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-16 06:58:02: Loss after num_examples_seen=0 epoch=0: 8.317129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jm/3w1dz5w55nj8bqyr7t76_qr00000gn/T/ipykernel_55450/1857118698.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  N = np.sum((len(y_i) for y_i in y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-16 06:58:03: Loss after num_examples_seen=100 epoch=1: 8.227105\n",
      "2024-10-16 06:58:04: Loss after num_examples_seen=200 epoch=2: 8.092797\n",
      "2024-10-16 06:58:05: Loss after num_examples_seen=300 epoch=3: 7.845445\n",
      "2024-10-16 06:58:06: Loss after num_examples_seen=400 epoch=4: 7.435504\n",
      "2024-10-16 06:58:06: Loss after num_examples_seen=500 epoch=5: 6.996474\n",
      "2024-10-16 06:58:07: Loss after num_examples_seen=600 epoch=6: 6.589343\n",
      "2024-10-16 06:58:08: Loss after num_examples_seen=700 epoch=7: 6.282422\n",
      "2024-10-16 06:58:09: Loss after num_examples_seen=800 epoch=8: 6.043619\n",
      "2024-10-16 06:58:10: Loss after num_examples_seen=900 epoch=9: 5.848634\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(17)\n",
    "\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNN(vocabulary_size)\n",
    "losses = train_with_sgd(model, X_train2[10000:10100], y_train2[10000:10100], nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ba56f313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4090"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "93f951d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, senten_max_length):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    \n",
    "    # Repeat until we get an end token and keep our sentences to less than senten_max_length words for now\n",
    "    while (not new_sentence[-1] == word_to_index[sentence_end_token]) and len(new_sentence) < senten_max_length:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        \n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            \n",
    "            # correcting for abnormalities\n",
    "            #abs_v = [-i if i <0 else i for i in next_word_probs[-1][0]] \n",
    "            #nrm_v = [i/sum(abs_v) for i in abs_v] \n",
    "            #abs_v = [0 if i <0 else i for i in next_word_probs[-1][0]] \n",
    "            #abs_v = [0 if i <0 else i for i in next_word_probs[0][-1]] \n",
    "            #nrm_v = [i/sum(abs_v) for i in abs_v] \n",
    "            #samples = np.random.multinomial(1, nrm_v)\n",
    "            #sampled_word = np.argmax(samples)\n",
    "            \n",
    "            # the secret sauce of creativity\n",
    "            samples = np.random.multinomial(1, next_word_probs[0][-1])\n",
    "            \n",
    "            sampled_word = np.argmax(samples)\n",
    "            \n",
    "        new_sentence.append(sampled_word)\n",
    "\n",
    "    print(new_sentence)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    #print(sentence_str)\n",
    "    return sentence_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "39d72aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2112, 4036, 3822, 2563, 3310, 2228, 470, 2603, 1158, 2260, 423, 2105, 513, 957, 2765, 4041, 273, 3329, 3473]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['displayed',\n",
       " 'widespread',\n",
       " 'vision',\n",
       " 'untouchable',\n",
       " 'non-eternal',\n",
       " 'wwwgutenbergorg/donate',\n",
       " 'wont',\n",
       " 'afar',\n",
       " 'instead',\n",
       " 'glow',\n",
       " 'head',\n",
       " 'healing',\n",
       " 'divine',\n",
       " 'thin',\n",
       " 'drank',\n",
       " 'outdated',\n",
       " 'night',\n",
       " 'disappointments']"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senten_max_length = 20\n",
    "generate_sentence(model, senten_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "3af50938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1799, 2368, 1543, 527, 9, 3976, 23, 4, 3118, 4, 289, 10, 2847, 968, 7, 1105, 6, 9, 2790]\n",
      "enticed thighs general magic his remaining not and disease and walk in approached writing a pondered of his\n",
      "[0, 3311, 910, 442, 3629, 3635, 4062, 611, 2672, 3916, 1543, 808, 2511, 958, 2397, 3642, 2686, 1908, 1463, 3056]\n",
      "enchantment beard nirvana immensely loyalty swamp including seeks e-mail general behold eagerness owned dripping unrelenting anew thinks seeking\n",
      "[0, 3010, 530, 3792, 3300, 924, 3298, 2803, 2308, 4026, 98, 1, 351, 3528, 3539, 449, 12, 14, 1725, 6]\n",
      "attempts verses blow sakyamunI mocking apparently shuddering wove 1500 very the along spare heed past was it ears\n",
      "[0, 2615, 2047, 207, 1330, 2119, 2993, 3043, 754, 1247, 560, 3657, 817, 3543, 2778, 2585, 3270, 2481, 1367, 1726]\n",
      "demonstrated wonderfully youre spreading tenderness profit complain suffered eye thank odds mute accusation guests patiently prayers rebirths politely\n",
      "[0, 647, 2970, 1917, 2926, 1051, 2714, 1673, 3171, 802, 2834, 2098, 1920, 3996, 1020, 912, 2480, 3491, 2327, 209]\n",
      "bid pit allow opening slipped clouds shaggy thoughtful fast stairs hurriedly praised formats pure skin halted willingly teeth\n",
      "[0, 182, 3353, 1181, 3985, 1337, 2167, 2888, 2226, 1781, 20, 1827, 1502, 1818, 7, 4, 9, 176, 7, 1477]\n",
      "such frightened distorted harmless self-castigation soil lured solicit encompassed ” glistening spit book a and his goal a\n",
      "[0, 1544, 1918, 3336, 3278, 2597, 404, 3871, 127, 3651, 3131, 1046, 459, 1650, 2493, 2101, 1889, 2614, 1488, 2953]\n",
      "complying scroll smelled struck parents taught indicating thus flame ugliness concealed near o devil scream difficult invaded aged\n",
      "[0, 1016, 3394, 2498, 1877, 3990, 2254, 2233, 3216, 2485, 210, 3774, 2285, 12, 3754, 3542, 1134, 4021, 10, 14]\n",
      "created rebuke comforting lots alteration falcon [ uncertain defamation among silly melting was interested compared laughing deductible in\n",
      "[0, 3143, 2383, 3432, 1237, 3166, 2201, 2113, 3171, 2033, 517, 3632, 2867, 1907, 1012, 2083, 1891, 2019, 1432, 2749]\n",
      "demonstrate seeding deadly tale mirror specified worrying thoughtful drowned servants achieving supposed rock official pause sing annihilate depth\n",
      "[0, 648, 2805, 429, 2920, 2433, 1473, 436, 62, 3581, 6, 149, 1105, 9, 3596, 6, 1, 327, 6, 1152]\n",
      "deception charms tree lesson whorehouses leaves name them foamed of knew pondered his save of the walking of\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    "senten_max_length = 20\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = generate_sentence(model, senten_max_length)\n",
    "    print (\" \".join(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb1282c",
   "metadata": {},
   "source": [
    "### 5. Model Training\n",
    "In this section, we train the model using categorical cross-entropy loss and the Adam optimizer. The goal is to minimize the loss over multiple epochs and improve the accuracy.\n",
    "\n",
    "Training Details:\n",
    "Loss function: Categorical Cross-Entropy\n",
    "Optimizer: Adam\n",
    "Batch Size: 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ba63888b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jm/3w1dz5w55nj8bqyr7t76_qr00000gn/T/ipykernel_55450/1857118698.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  N = np.sum((len(y_i) for y_i in y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-16 06:59:49: Loss after num_examples_seen=0 epoch=0: 8.317905\n",
      "2024-10-16 07:17:05: Loss after num_examples_seen=48950 epoch=1: 5.360205\n",
      "2024-10-16 07:35:34: Loss after num_examples_seen=97900 epoch=2: 5.340448\n",
      "2024-10-16 07:52:52: Loss after num_examples_seen=146850 epoch=3: 5.297493\n",
      "2024-10-16 08:10:56: Loss after num_examples_seen=195800 epoch=4: 5.294335\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(17)\n",
    "\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNN(vocabulary_size)\n",
    "losses = train_with_sgd(model, X_train2, y_train2, nepoch=5, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "648e7ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jm/3w1dz5w55nj8bqyr7t76_qr00000gn/T/ipykernel_55450/1857118698.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  N = np.sum((len(y_i) for y_i in y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-16 08:31:39: Loss after num_examples_seen=0 epoch=0: 8.317905\n",
      "2024-10-16 08:48:06: Loss after num_examples_seen=48950 epoch=1: 5.360205\n",
      "2024-10-16 09:04:23: Loss after num_examples_seen=97900 epoch=2: 5.340448\n",
      "2024-10-16 09:21:47: Loss after num_examples_seen=146850 epoch=3: 5.297493\n",
      "2024-10-16 09:38:32: Loss after num_examples_seen=195800 epoch=4: 5.294335\n"
     ]
    }
   ],
   "source": [
    "losses = train_with_sgd(model, X_train2, y_train2, nepoch=5, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a1d58eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jm/3w1dz5w55nj8bqyr7t76_qr00000gn/T/ipykernel_55450/1857118698.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  N = np.sum((len(y_i) for y_i in y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-16 09:55:45: Loss after num_examples_seen=0 epoch=0: 5.152209\n",
      "2024-10-16 10:12:25: Loss after num_examples_seen=48950 epoch=1: 5.154802\n",
      "Setting learning rate to 0.002500\n",
      "2024-10-16 10:30:41: Loss after num_examples_seen=97900 epoch=2: 4.991009\n",
      "2024-10-16 10:48:20: Loss after num_examples_seen=146850 epoch=3: 4.963550\n",
      "2024-10-16 11:08:07: Loss after num_examples_seen=195800 epoch=4: 4.919193\n"
     ]
    }
   ],
   "source": [
    "losses = train_with_sgd(model, X_train2, y_train2, nepoch=5, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "20400640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jm/3w1dz5w55nj8bqyr7t76_qr00000gn/T/ipykernel_55450/1857118698.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  N = np.sum((len(y_i) for y_i in y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-16 11:24:53: Loss after num_examples_seen=0 epoch=0: 4.936287\n",
      "2024-10-16 11:39:36: Loss after num_examples_seen=48950 epoch=1: 5.093147\n",
      "Setting learning rate to 0.002500\n",
      "2024-10-16 12:35:15: Loss after num_examples_seen=97900 epoch=2: 4.892396\n",
      "2024-10-16 12:49:06: Loss after num_examples_seen=146850 epoch=3: 4.876551\n",
      "2024-10-16 13:05:37: Loss after num_examples_seen=195800 epoch=4: 4.888263\n",
      "Setting learning rate to 0.001250\n"
     ]
    }
   ],
   "source": [
    "losses = train_with_sgd(model, X_train2, y_train2, nepoch=5, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "bc4b1885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jm/3w1dz5w55nj8bqyr7t76_qr00000gn/T/ipykernel_55450/1857118698.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  N = np.sum((len(y_i) for y_i in y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-16 13:30:10: Loss after num_examples_seen=0 epoch=0: 4.777440\n",
      "2024-10-16 13:43:46: Loss after num_examples_seen=48950 epoch=1: 4.988670\n",
      "Setting learning rate to 0.002500\n",
      "2024-10-16 14:00:59: Loss after num_examples_seen=97900 epoch=2: 4.861099\n",
      "2024-10-16 14:33:38: Loss after num_examples_seen=146850 epoch=3: 4.863277\n",
      "Setting learning rate to 0.001250\n",
      "2024-10-16 15:35:27: Loss after num_examples_seen=195800 epoch=4: 4.771211\n"
     ]
    }
   ],
   "source": [
    "losses = train_with_sgd(model, X_train2, y_train2, nepoch=5, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb632e2",
   "metadata": {},
   "source": [
    "We ran the below cell but by mistake it was converted to mardown and we lost the running tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329abb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train_with_sgd(model, X_train2, y_train2, nepoch=40, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caf9c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train_with_sgd(model, X_train2, y_train2, nepoch=5, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163e09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jm/3w1dz5w55nj8bqyr7t76_qr00000gn/T/ipykernel_55450/1857118698.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  N = np.sum((len(y_i) for y_i in y))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for i in range(10):\n",
    "    losses = train_with_sgd(model, X_train2, y_train2, nepoch=5, evaluate_loss_after=1)\n",
    "    time.sleep(15 * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train_with_sgd(model, X_train2, y_train2, nepoch=5, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f5af9",
   "metadata": {},
   "source": [
    "### 6. Generating Text\n",
    "Once the model is trained, we can use it to generate text. By providing a seed (e.g., a phrase), the model predicts the next word iteratively to form a sequence.\n",
    "\n",
    "Generation Process:\n",
    "Provide a seed text.\n",
    "Generate the next word using the model’s prediction.\n",
    "Append the new word to the seed and continue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "98d5b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, senten_max_length):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    \n",
    "    # Repeat until we get an end token and keep our sentences to less than senten_max_length words for now\n",
    "    while (not new_sentence[-1] == word_to_index[sentence_end_token]) and len(new_sentence) < senten_max_length:\n",
    "        next_words_probs = model.forward_propagation(new_sentence)\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        \n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            \n",
    "            #print(next_word_probs[0][-1])\n",
    "            samples = np.random.multinomial(1, next_words_probs[0][-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "            \n",
    "        new_sentence.append(sampled_word)\n",
    "\n",
    "    #print(new_sentence)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    #print(sentence_str)\n",
    "    return sentence_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "b271ea2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "displayed widespread vision untouchable non-eternal wwwgutenbergorg/donate wont afar instead glow head healing divine thin drank outdated night disappointments\n",
      "enticed thighs general magic ferried worries letter once his fate and like become being the woman back SENTENCE_EN\n",
      "simpler branches sink visible cheek enchantment beard nirvana immensely loyalty swamp including seeks e-mail general behold eagerness owned\n",
      "unrelenting anew thinks seeking balls attempts verses blow sakyamunI mocking apparently shuddering wove 1500 very all about or\n",
      "spare fragrant isolated collect discard wonderfully youre spreading tenderness profit complain suffered eye thank odds mute accusation guests\n",
      "prayers rebirths politely believers bid pit allow opening slipped clouds shaggy thoughtful fast stairs hurriedly praised formats pure\n",
      "halted willingly teeth while such frightened distorted harmless self-castigation soil lured solicit encompassed ” “ little do heard\n",
      "ablutions spit book soul dreamt the community thirst in know SENTENCE_EN\n",
      "master smelled struck parents taught indicating thus flame ugliness concealed near o devil scream difficult invaded aged besiege\n",
      "rebuke comforting lots alteration falcon [ uncertain defamation among was cant for SENTENCE_EN\n",
      "pure safe yenco pit canes lacks carp banana-trees seeding deadly tale mirror specified worrying thoughtful drowned servants achieving\n",
      "rock official pause sing annihilate depth enlightenment—it deception charms tree lesson whorehouses leaves name them oh two the\n",
      "humiliate petrified dreamt roughly piled rigidly liked fighting weve straightening flourished seasons took eightfold resolution patience still now\n",
      "repeated recently important happen flocked devoted smarter werent handsome listening satyam—verily amusedly rice price delights city-house beckoned are\n",
      "defect pointlessness openly unbelievable sighing wise killed consisted humans purified void noise - teacher transported forties penitent shut\n",
      "wondrously developing 1e bizarre command showed alien hut presented white hut foreign disappearance eager taste but vasudeva friend\n",
      "alleys redeemed liberated downwards knock meditations decayed international pleasure-garden exclusion precious achieving plunging precisely crude interested jackals oneself\n",
      "developing little exercise pointed lamented files important rivers city worn sit state appearance injustice bright nearly sounded large\n",
      "able eyes hasnt lack redeemed dissect lighting these blood desperate occupied universe reminiscent corpses hips saying stone satisfy\n",
      "stubbornly accessed arcs pleased slurred garments lovely woven bloody unsuccessfully spell twitched any person worrying of my every\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 20\n",
    "senten_min_length = 7\n",
    "senten_max_length = 20\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model, senten_max_length)\n",
    "    print (\" \".join(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4399d4",
   "metadata": {},
   "source": [
    "### 7. Conclusion\n",
    "We successfully built and trained an RNN model for text generation using trigrams. The model generates text based on the patterns it learned from the input corpus. Further improvements can be made by experimenting with different architectures and hyperparameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c4a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
