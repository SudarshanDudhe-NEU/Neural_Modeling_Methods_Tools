{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e13ba44-2051-41f7-84c2-7f0882edf080",
   "metadata": {},
   "source": [
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b85864fe-7765-4aa5-8774-0e4ce58943bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries for the entire notebook\n",
    "import re\n",
    "import numpy as np\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt  # Optional: for plotting, if needed\n",
    "\n",
    "# Ensure NLTK resources are downloaded (if not already done)\n",
    "nltk.download('book')  # For tokenizing text into sentences and words\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Check TensorFlow version, important for compatibility with certain functionalities\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70970ef2-a165-4482-8068-2d3d90ebcdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b16042ee-b4f6-4125-b2ba-e04ed7c74b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/kaushikkaranam/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af9b7fa-5426-4898-b575-e0246cd35c22",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c92a58e-d25c-4462-a931-f6bca81824c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading txt file...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk import tokenize\n",
    "\n",
    "#alphabets= \"([A-Za-z])\"\n",
    "#prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "#suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "#starters = \"(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "#acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "#websites = \"[.](com|net|org|io|gov|edu|me)\"\n",
    "#digits = \"([0-9])\"\n",
    "\n",
    "# If you want to restrict the size of the voabulary\n",
    "# Right now, we set it in the next cell to be the entire vocabular: vocabulary_size = len(word_freq.items())\n",
    "#vocabulary_size = 3000\n",
    "\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "text = ''\n",
    "print( \"Reading txt file...\")\n",
    "with open(r'data/Mahabharat.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "8aaa0636-d6cf-4a35-bba3-b1c760f4c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text data from a file\n",
    "text_file_path = 'data/Mahabharat.txt'\n",
    "with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Clean and preprocess the text\n",
    "text = text.lower().replace('\\n', ' ').replace('\\r', '').replace('.', ' .')\n",
    "\n",
    "# Initialize the tokenizer and fit it on the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "\n",
    "# Convert text to sequence of tokens\n",
    "sequences = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "# Create training sequences\n",
    "train_length = 50\n",
    "step = 3\n",
    "sentences = []\n",
    "next_words = []\n",
    "\n",
    "for i in range(0, len(sequences) - train_length, step):\n",
    "    sentences.append(sequences[i: i + train_length])\n",
    "    next_words.append(sequences[i + train_length])\n",
    "\n",
    "# Prepare the input and output data\n",
    "X = pad_sequences(sentences, maxlen=train_length)\n",
    "y = tf.keras.utils.to_categorical(next_words, num_classes=len(tokenizer.word_index) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "751a5588-bc42-4961-8db1-ceae3f14f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()\n",
    "text = text.replace('i ', 'I ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "404b3c09-90a8-4206-a290-856b05833d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 2145\n",
      "Sentence 100: one of the names of vishnu is purushottama.\n",
      "Sentence 101: poor urvasi, when called upon to confess on whom her heart was set, forgetting the part she had to act, says \"I love pururavas,\" instead of \"I love purushottama.\"\n",
      "Sentence 102: her teacher bharata, the author of the play, is so much exasperated by this mistake, that he pronounces a curse upon urvasi.\n",
      "Sentence 103: \"you must lose your divine knowledge.\"\n",
      "Sentence 104: after the close of the performance, indra, observing her as she stood apart, ashamed and disconsolate, calls her and says:—\n",
      "\n",
      "\"the mortal, who engrosses your thoughts, has been my friend in the days of adversity; he has helped me in the conflict with the enemies of the gods, and is entitled to my acknowledgements.\n",
      "Sentence 105: you must, accordingly, repair to him and remain with him till he beholds the offspring you shall bear him.\"\n",
      "Sentence 106: the god thus permits her to marry the mortal hero.\n",
      "Sentence 107: after transacting public business, the king retires to the garden of the palace as the evening approaches.\n",
      "Sentence 108: a messenger arrives from the queen, apprising his majesty that she desires to see him on the terrace of the pavilion.\n",
      "Sentence 109: the king obeys and ascends the crystal steps while the moon is just about to rise, and the east is tinged with red.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the cleaned text into sentences\n",
    "sentences = tokenize.sent_tokenize(text)\n",
    "\n",
    "# Check the length of sentences\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "\n",
    "# If there are at least 110 sentences, print some for inspection\n",
    "if len(sentences) > 100:\n",
    "    for i in range(100, min(110, len(sentences))):  # Ensure index doesn't go out of range\n",
    "        print(f\"Sentence {i}: {sentences[i]}\")\n",
    "else:\n",
    "    print(\"There are fewer than 100 sentences in the text.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbecdbd-8969-4a6d-9428-36017739273a",
   "metadata": {},
   "source": [
    "### 2. creating word mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e0536884-5f78-4393-a4ea-95ba08bd5aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 2145 sentences.\n",
      "Found 6445 unique word tokens.\n",
      "The least frequent word in our vocabulary is 'newsletter' and appeared 1 times.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import itertools\n",
    "\n",
    "# Append SENTENCE_START and SENTENCE_END\n",
    "sentences = [\"%s %s %s\" % (sentence_start_token, x[:-1].replace(\"&\", \"\"), sentence_end_token) for x in sentences] \n",
    "print(\"Parsed %d sentences.\" % len(sentences))\n",
    "\n",
    "# Tokenize the sentences into words, making sure to remove end-of-sentence period\n",
    "tokenized_sentences = [nltk.word_tokenize(sent.replace('.', '')) for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"Found %d unique word tokens.\" % len(word_freq.items()))\n",
    "\n",
    "# Set vocabulary_size here, either to a specific value or based on word frequencies\n",
    "vocabulary_size = 300000  \n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size - 1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w, i) for i, w in enumerate(index_to_word)])\n",
    "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "30cfd444-e073-4248-982e-d74342007c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3354),\n",
       " (',', 2912),\n",
       " ('SENTENCE_START', 2145),\n",
       " ('SENTENCE_END', 2145),\n",
       " ('of', 1708),\n",
       " ('and', 1251),\n",
       " ('to', 1225),\n",
       " ('a', 768),\n",
       " ('is', 675),\n",
       " ('in', 610),\n",
       " ('his', 582),\n",
       " ('her', 448),\n",
       " ('with', 407),\n",
       " ('by', 389),\n",
       " ('he', 357),\n",
       " ('king', 317),\n",
       " ('that', 259),\n",
       " ('or', 227),\n",
       " ('as', 220),\n",
       " ('for', 217)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5cb21f04-8009-4358-bd68-428b2c864d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SENTENCE_START \\nsakuntala or the lost ring SENTENCE_END',\n",
       " 'SENTENCE_START in ancient days, there was a mighty king of the lunar dynasty by name dushyanta SENTENCE_END',\n",
       " 'SENTENCE_START he was the king of hastinapur SENTENCE_END',\n",
       " 'SENTENCE_START he once goes out a-hunting and in the pursuit of a deer comes near the hermitage of the sage kanwa, the chief of the hermits, where some anchorites request him not to kill the deer SENTENCE_END',\n",
       " 'SENTENCE_START the king feels thirsty and was seeking water when he saw certain maidens of the hermits watering the favourite plants SENTENCE_END',\n",
       " 'SENTENCE_START one of them, an exquisitely beautiful and bashful maiden, named sakuntala, received him SENTENCE_END',\n",
       " 'SENTENCE_START she was the daughter of the celestial nymph menaka by the celebrated sage viswamitra and foster-child of the hermit kanwa SENTENCE_END',\n",
       " 'SENTENCE_START she is smitten with love at the first sight of the king, standing confused at the change of her own feeling SENTENCE_END',\n",
       " 'SENTENCE_START the love at first sight which the king conceives for her is of too deep a nature to be momentary SENTENCE_END',\n",
       " 'SENTENCE_START struck by her beauty he exclaims:—\\n\\n\"her lip is ruddy as an opening bud; her graceful arms resemble tender shoots; attractive as the bloom upon the tree, the glow of youth is spread on all her limbs. SENTENCE_END',\n",
       " 'SENTENCE_START seizing an opportunity of addressing her, he soon feels that it is impossible for him to return to his capital SENTENCE_END',\n",
       " 'SENTENCE_START his limbs move forward, while his heart flies back, like a silken standard borne against the breeze SENTENCE_END',\n",
       " 'SENTENCE_START he seeks for opportunities for seeing her SENTENCE_END',\n",
       " 'SENTENCE_START with the thought about her haunting him by day and night, he finds no rest, and no pleasure even in his favourite recreation—sporting SENTENCE_END',\n",
       " 'SENTENCE_START mathavya, the jester, friend and companion of the king, however, breaks the dull monotony of his anxious time SENTENCE_END',\n",
       " 'SENTENCE_START the opportunity which the king seeks offers itself SENTENCE_END',\n",
       " 'SENTENCE_START the hermits send an embassy to the king asking him to come over to the hermitage to guard their sacrifices SENTENCE_END',\n",
       " 'SENTENCE_START as he was making preparations for departure to the hermitage, karavaka, a messenger from the queen-mother, arrives asking his presence at the city of hastinapur SENTENCE_END',\n",
       " 'SENTENCE_START he is at first at a loss to extricate himself from this difficulty but a thought strikes him and he acts upon it SENTENCE_END',\n",
       " 'SENTENCE_START he sends the jester as his substitute to the city SENTENCE_END',\n",
       " 'SENTENCE_START he is now at leisure to seek out the love-sick sakuntala who is drooping on account of her love for the king and is discovered lying on a bed of flowers in an arbour SENTENCE_END',\n",
       " 'SENTENCE_START he comes to the hermitage, overhears her conversation with her two friends, shows himself and offers to wed her SENTENCE_END',\n",
       " 'SENTENCE_START for a second time, the lovers thus meet SENTENCE_END',\n",
       " 'SENTENCE_START he enquires of her parentage to see if there is any obstacle to their being united in marriage; whereupon sakuntala asks her companion priyambada to satisfy the king with an account of her birth SENTENCE_END',\n",
       " 'SENTENCE_START the king hearing the story of her birth asks the companion to get the consent of sakuntala to be married to him according to the form known as gandharva SENTENCE_END']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a176657-7b43-43eb-8d4f-604c24551457",
   "metadata": {},
   "source": [
    "### 3. Preparing Trigrams and Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8d5c7af0-9d7e-4863-aabe-a3a210d4e838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.8 ms, sys: 3.63 ms, total: 30.4 ms\n",
      "Wall time: 28.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 449),\n",
       " (('the', 'king'), 220),\n",
       " (('to', 'the'), 211),\n",
       " (('in', 'the'), 167),\n",
       " (('and', 'the'), 112),\n",
       " (('by', 'the'), 97),\n",
       " (('with', 'the'), 89),\n",
       " (('of', 'his'), 74),\n",
       " (('the', 'queen'), 73),\n",
       " (('to', 'be'), 70)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "bigram_counts = Counter(ngrams(text.split(), 2))\n",
    "bigram_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "39b80d01-586d-4243-9b44-a5c6ee8ad5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.1 ms, sys: 3.6 ms, total: 25.7 ms\n",
      "Wall time: 25 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 449),\n",
       " (('the', 'king'), 220),\n",
       " (('to', 'the'), 211),\n",
       " (('in', 'the'), 167),\n",
       " (('and', 'the'), 112),\n",
       " (('by', 'the'), 97),\n",
       " (('with', 'the'), 89),\n",
       " (('of', 'his'), 74),\n",
       " (('the', 'queen'), 73),\n",
       " (('to', 'be'), 70)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import collections\n",
    "def ngrams(text, n=2):\n",
    "    return zip(*[text[i:] for i in range(n)])\n",
    "bigram_counts = collections.Counter(ngrams(text.split(), 2))\n",
    "bigram_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5e4c51e9-a7c2-4546-b435-a8c091bd4eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsakuntala or the lost ring.\\n\\n\\nin ancient days, there was a mighty king of the lunar dynasty by name dushyanta. he was the king of hastinapur. he once goes out a-hunting and in the pursuit of a deer comes near the hermitage of the sage kanwa, the chief of the hermits, where some anchorites request him not to kill the deer. the king feels thirsty and was seeking water when he saw certain maidens of the hermits watering the favourite plants. one of them, an exquisitely beautiful and bashful maiden, named sakuntala, received him. she was the daughter of the celestial nymph menaka by the celebrated sage viswamitra and foster-child of the hermit kanwa. she is smitten with love at the first sight of the king, standing confused at the change of her own feeling. the love at first sight which the king conceives for her is of too deep a nature to be momentary. struck by her beauty he exclaims:—\\n\\n\"her lip is ruddy as an opening bud; her graceful arms resemble tender shoots; attractive as the bloo'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6064772d-5309-4929-9ad6-347026fa491c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 272),\n",
       " ('he', 122),\n",
       " ('.\"', 96),\n",
       " ('I', 47),\n",
       " ('.\"\\n\\nthe', 37),\n",
       " ('she', 34),\n",
       " ('his', 29),\n",
       " ('it', 26),\n",
       " ('rama', 26),\n",
       " ('a', 24)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_word_counts = Counter([ p.replace('. ', '') for p in re.findall('\\..[^\" \"]*', text)])\n",
    "first_word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3297d07e-c0c1-45be-9e83-f7af41f9d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = [[sentence_start_token] for sent,times in first_word_counts if sent != 'o.']\n",
    "#y_train = [sent for sent in first_word_counts if sent != 'o.']\n",
    "X_train = [[sentence_start_token]*c for sent,c in first_word_counts.items() if sent != 'o.']\n",
    "y_train = [[sent]*c for sent,c in first_word_counts.items() if sent != 'o.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1b67f742-c4eb-4dd7-8f08-4f0f624bbe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [item for sublist in X_train for item in sublist]\n",
    "y_train = [item for sublist in y_train for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fbdbefc8-1755-44c9-9c13-23ebb16a368b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SENTENCE_START',\n",
       " 'SENTENCE_START',\n",
       " 'SENTENCE_START',\n",
       " 'SENTENCE_START',\n",
       " 'SENTENCE_START',\n",
       " 'SENTENCE_START',\n",
       " 'SENTENCE_START',\n",
       " 'SENTENCE_START',\n",
       " 'SENTENCE_START',\n",
       " 'SENTENCE_START']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f1eabc78-0ade-49b0-9269-0ddfd70086e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'one', 'one', 'one', 'one', 'one', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'struck', '.\"\\n\\nseizing', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'mathavya,', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'for', 'for', 'for', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'next', 'next', 'upon', 'priyambada', 'sakuntala', '.\"\\n\\non', '.\"\\n\\non', 'at', 'at', 'at', 'at', 'at', 'at', 'at', 'at', 'at', 'at', 'at', 'whereupon', 'sarnagarva', 'after', 'after', 'after', 'after', 'after', 'after', 'after', 'after', 'after', 'after', 'after', 'after', 'after', 'after', 'after', 'everyone', 'while', 'while', 'while', 'while', 'while', 'taking', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.)\\n\\nsoon', 'king', 'urvasI', 'urvasI', 'urvasI', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', '.\"\\n\\nher', '', '', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', '.\"\\n\\nshe', '.\"\\n\\nshe', '.\"\\n\\nshe', 'her', 'her', 'her', 'her', 'her', 'her', 'her', 'her', 'her', 'her', 'her', 'her', 'her', 'her', 'her', 'her', 'mournful', 'clouds', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', '.\"\\n\\nat', '.\"\\n\\nat', '.\"\\n\\nat', '.\"\\n\\nat', '.\"\\n\\nat', '.\"\\n\\nat', '.\"\\n\\nat', 'thus', 'thus', 'an', 'an', 'an', 'an', 'orders', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'now', 'now', 'now', 'now', 'now', 'so', 'so', 'so', 'so', 'so', 'all', 'all', 'all', 'all', 'all', 'all', 'all', '.\"\\n\\npuspamitra', '.c.', '.c.', '.c.', 'malavika', 'malavika', 'yajnasena', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', \"malavika's\", 'unluckily', 'presently', 'presently', 'vakulavalI', 'amongst', 'amongst', 'that', 'that', 'sumatI', 'kausika', 'whilst', 'whilst', 'whilst', 'whilst', 'iravatI', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'rama', 'kusadhwaja,', 'meanwhile', 'rama,', 'rama,', 'viswamitra', 'parasurama', 'parasurama', 'parasurama', 'thy', '.\"\\n\\nrama', '.\"\\n\\nrama', '.\"\\n\\nrama', '.\"\\n\\nrama', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', '.\"\\n\\nthe', 'ramachandra', 'jatayu', 'jatayu', 'jatayu', 'lakshmana', 'lakshmana', 'lakshmana', 'lakshmana', '.)\\n\\nrama,', 'kabandha', 'balI', 'balI', 'balI', 'trijata,', 'angada', 'angada', 'angada', 'ravana,', 'ravana,', 'indra', 'alaka,', 'lanka,', 'then', 'then', 'then', 'then', 'then', 'then', 'then', '.\"\\n\\nin', '.\"\\n\\nin', '.\"\\n\\nin', 'sita', 'sita', 'sita', 'sita', 'sita', 'sita', 'people', 'people', 'evidently', 'is', 'among', 'lava—for', 'soldiers', \"lakshmana's\", 'lava', \"lava's\", 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'ganga', 'ganga', '.\"\\n\\nbut', 'earth', 'earth', 'oh!', '.\"\\n\\n\\n\\n*', '.\"\\n\\n\\n\\n*', 'madhava', 'madhava', 'bhurivasu', 'devarata', 'malatI', 'malatI', 'malatI', 'by', 'by', 'by', 'by', 'by', 'by', 'by', 'by', 'by', 'by', 'even', 'even', '.\"\\n\\navalokita', 'how', 'how', 'how', 'though', '.\"\\n\\nkamandakI', '.\"\\n\\nkamandakI', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'let', 'let', 'let', 'let', 'let', 'let', 'avalokita', 'can', 'may', 'whatever', 'whatever', 'love', 'love', '.\"\\n\\nmakranda', 'behold!', 'what', 'what', 'what', 'what', 'what', 'being', 'words', 'vain', 'requested', 'bowing', '.\"\\n\\nmakaranda', 'maidens', 'and', 'and', '.\"\\n\\nkalahansa,', 'mandarika', 'come,', 'come,', 'portray', '.\"\\n\\nmadhava,', '.\"\\n\\n\\n\\n\\n\\nbeing', 'alas!', 'alas!', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'every', 'resistless', 'its', 'its', 'its', 'its', 'no', 'no', 'no', 'no', 'no', 'no', 'neither', 'life', 'life', 'forgive', 'death', 'death', 'death', '.\"\\n\\n[this', '.]\\n\\nthe', 'your', 'for,', 'their', 'their', 'now,', '.\"\\n\\nby', 'instantly,', 'mandayantika', 'another', 'voices', 'nandana,', 'fiery', 'fiery', 'meteors', 'around,', 'unseasonable', 'day', 'maricha,', 'jatayu,', 'consequently', 'knowest', '.\"\\n\\nangada,', \"sita's\", \"sita's\", 'ravana', 'ravana', 'ravana', 'ravana', 'ravana', 'ravana', '.\"\\n\\nthus', 'sita,', 'kumbhakarna,', 'kumbhakarna,', 'these', 'these', 'these', 'these', 'these', 'these', 'confiding', '.\"\\n\\nkumbhakarna\\'s', 'hanumana', 'hanumana', 'vasishtha', 'we', 'we', 'we', 'we', 'we', 'we', 'we', 'we', 'we', 'we', 'or', '.\"\\n\\nravana', 'mandodarI', 'each', 'sanshkala,', 'sanshkala', 'malyavan', 'malyavan', 'malyavan', '.\"\\n\\na', '.\"\\n\\na', \"rama's\", '.\"\\n\\n\\ndraupadi,', 'ferocious', 'sahadeva', 'duryodhana', 'duryodhana', 'duryodhana', 'duryodhana', 'duryodhana', 'duryodhana', 'duryodhana', 'bhanumatI', 'up', 'aswatthama,', \"aswatthama's\", 'kripa', 'kerna', 'dark', '.\"\\n\\nkripa', 'bhima', 'kerna,', 'sundaraka,', 'duryodhana,', 'aswatthama', 'dhritarashtra', 'yudhisthira', 'draupadI', 'draupadI', 'draupadI', 'supposing', 'arjuna', 'apprehending', 'be', 'be', 'be', 'be', 'just', 'addressing', 'hunting', 'bighna', 'so,', 'they,', 'seeing', 'seeing', 'seeing', 'please', 'please', '.\"\\n\\nthen', '.\"\\n\\nthen', '.\"\\n\\nthen', 'collect', 'collect', 'not', 'not', 'not', 'alms', 'alms', 'i,', 'oh', 'oh', 'oh', 'oh,', 'accept', 'see', 'see', 'see', 'why', 'why', 'why', 'virtue', 'virtue', 'do', 'do', 'do', 'hideous', 'dogs', 'vultures', '.\"\\n\\nno', '.\"\\n\\nas', 'apply', 'rohitashya', 'parvatI', 'parvatI', 'usha', 'nareda', 'krishna,', 'notwithstanding', 'siva', 'siva', 'krishna', 'sridama,', 'balarama', 'devayanI', 'devayanI', 'devayani,', 'yayatI', '.\"\\n\\nsatyacharya', '.\"\\n\\nsatyacharya', '.\"\\n\\nsatyacharya', '.\"\\n\\nsatyacharya', 'yet', 'yet', 'our', 'better', 'yudhishthira,', '.\"\\n\\n\\nreligion', 'aryuna', 'mahayatrika,', 'daksha', \"nareda's\", 'dadhichi,', '.\"\\n\\nsiva', 'those', 'those', 'virabhadra,', 'shaking', 'some', 'some', 'some', 'during', 'before', 'nanda,', 'chandragupta', 'nanda', 'rakshasa', 'saileswara', 'vairodhaka,', 'bhagurayana', 'charayana', 'chandraverma,', '.\"\\n\\n', 'has', 'mekhala,', 'mekhala', 'mrigankavalI', 'man', 'vikramabahu', 'vikramabahu,', 'from', 'from', 'wearied', 'attended', 'kausambI', 'noticing', 'kanchanmala', 'sagarika', 'sagarika', 'sagarika', 'ah', 'lord', 'susangata', 'susangata', 'susangata', 'susangata', 'afraid', 'lead', '.\"\\n\\nvasantaka', '.\"\\n\\nvasantaka', 'take', 'vasantaka', 'vasantaka', 'vasantaka', 'vasantaka', 'vasantaka', 'looking', 'looking', 'looking', 'look', '.\"\\n\\nsagarika', 'kanchanmala,', '.\"\\n\\nto', 'come', 'cannot', 'asked', 'impatient', 'issuing', 'fires', 'rumanwat', 'brahma', 'indra,', 'around', 'vasubhutI', 'vasubhutI', 'yaugandharayana', 'vikrambahu', 'ratnavalI', 'nothing', 'particular', 'wherever,', 'both', 'indian', 'mohammedan', '.\"\\n\\n\\n\\n\\n\\nthe', '.\"\\n\\n\\n\\n\\n\\nthe', 'nor', 'nor', 'indeed,', 'either', 'but,', 'but,', 'hence', 'prominent', 'of', 'of', 'behind', 'weapons,', '.\"\\n\\n\\n\\n\\n\\nsakuntala.\\n\\n\\nof', \"thus—'urvasI\", \".'\\n\\nthe\", 'throughout', '.e.', '.e.', '.d.—it', 'justice', \"chanakya's\", 'and,', '.\"\\n\\n\\n\\n\\n\\nvisakhadatta.\\n\\n\\nvisakhadatta', 'thence', 'impressed', '.,\\n\\nrev.', 'a.', 'a.', '.a.', '.a.', 'george', 'a.\\n\\n10', '.—1910\\n\\n\\n\\n*', 'jnan', 'a.,', 'l.', 's.', 'r.', '.—1910', 'c.', 'linton.', '.a.,', 'dutt', 'were', 'h.', 'a.\\n\\nprof.', '5th—1910', 'n.', 'headland', '3.', '.head', '.s.', '.s.', '.s.', '.s.', '.s.', '.s.', 'special', 'project', 'project', 'redistribution', '.gutenberg.org/license.\\n\\nsection', 'general', 'general', '.e.8.\\n\\n1.b.', '.c', '.e', 'nearly', '.d.', 'copyright', '.e.1.', '.gutenberg.org.', '.e.2.', '.e.1', '.e.1', '.e.1', '.e.7', '.e.7', '.e.8', '.e.8', '.e.9.\\n\\n1.e.3.', 'additional', '.e.4.', '.e.5.', '.e.6.', 'however,', '.gutenberg.org),', 'any', '.e.1.\\n\\n1.e.7.', '.e.9.\\n\\n1.e.8.', 'royalty', 'royalty', '.”\\n\\n•', '.f.3,', '.f.3,', '.f.3,', '.e.9.', 'contact', '.f.\\n\\n1.f.1.', 'despite', '.f.2.', '.f.3.', '.f.3.', '.f.4.', '.f.5.', '.f.6.', 'information', 'information', 'information', '.gutenberg.org.\\n\\nsection', 'contributions', 'email', '.gutenberg.org/contact\\n\\nsection', 'many', 'compliance', '.gutenberg.org/donate.\\n\\nwhile', 'u.s.', 'donations', '.gutenberg.org/donate.\\n\\nsection', 'hart', 'thus,', '.gutenberg.org.\\n\\nthis']\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d0778236-141a-4eb1-b6f2-2fbc479cda03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1562, 1562)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "580c3f9a-3cff-432b-a5da-f33fec05d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def fisher_yates (arr1, arr2):\n",
    "     \n",
    "    # We will Start from the last element\n",
    "    # and swap one by one.\n",
    "    n = len(arr1)\n",
    "    if n != len(arr2):\n",
    "        return None\n",
    "    \n",
    "    for i in range(n - 1, 0, -1):\n",
    "\n",
    "        # Pick a random index from 0 to i\n",
    "        j = random.randint(0, i)\n",
    "        #print(i, j)\n",
    "\n",
    "        # Swap arr[i] with the element at random index\n",
    "        arr1[i], arr1[j] = arr1[j], arr1[i]\n",
    "        arr2[i], arr2[j] = arr2[j], arr2[i]\n",
    "        \n",
    "    return arr1, arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1b1ec789-c3bb-4667-9870-9d6e2105ff10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a', 'b', 'c'], ['1', '2', '3'])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random as rd\n",
    "one = ['a', 'b', 'c']\n",
    "two = ['1', '2', '3']\n",
    "one, two = fisher_yates(one, two)\n",
    "one, two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e7fc9228-b590-4709-bdca-95673130fc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['b'], ['c'], ['a']], [['2'], ['3'], ['1']])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one = [['a'], ['b'], ['c']]\n",
    "two = [['1'], ['2'], ['3']]\n",
    "one, two = fisher_yates(one, two)\n",
    "one, two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f4c68456-a157-4e6f-a63d-d9bef1abd537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1562, 1562)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = fisher_yates(X_train, y_train)\n",
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4a6afb3a-ded5-49c1-b8cd-e02f6145b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens = [[word_to_index[symbol]] for symbol,word in zip(X_train, y_train) if word in word_to_index]\n",
    "y_tokens = [[word_to_index[word]] for symbol,word in zip(X_train, y_train) if word in word_to_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bc865cc7-4d4b-4e7b-9eb7-c5a0c5efe383",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_tokens\n",
    "y_train = y_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cc43980a-601e-434b-aac7-9efbd9318cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1222, 1222)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "af842927-0204-4d92-abec-8c049519ef07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[2], [2], [2], [2], [2]], [[0], [0], [46], [27], [14]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:5], y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c727e318-2dcc-4947-836d-d6efe112ebf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram- 2 length: 27343\n",
      "ngram- 3 length: 36204\n",
      "ngram- 4 length: 38104\n",
      "ngram- 5 length: 38535\n",
      "ngram- 6 length: 38666\n",
      "ngram- 7 length: 38716\n",
      "ngram- 8 length: 38736\n",
      "ngram- 9 length: 38742\n",
      "ngram- 10 length: 38744\n",
      "ngram- 11 length: 38745\n",
      "ngram- 12 length: 38746\n",
      "ngram- 13 length: 38747\n",
      "ngram- 14 length: 38746\n",
      "ngram- 15 length: 38745\n",
      "ngram- 16 length: 38744\n",
      "ngram- 17 length: 38743\n",
      "ngram- 18 length: 38742\n",
      "ngram- 19 length: 38741\n",
      "ngram- 20 length: 38740\n"
     ]
    }
   ],
   "source": [
    "ngrams_up_to_20 = []\n",
    "for i in range(2, 21):\n",
    "    ngram_counts = Counter(ngrams(text.split(), i))\n",
    "    print('ngram-', i, 'length:', len(ngram_counts))\n",
    "    ngrams_up_to_20.append(ngram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b264ebcf-3527-4607-a7ed-ad4409dcf407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_periods(ngram):\n",
    "    for wrd in ngram[0]:\n",
    "        if '.' in wrd or \"’\" in wrd or \"‘\" in wrd:\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "def my_filter(ngrams):\n",
    "    return filter(remove_periods, ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a4f53518-cc47-42ab-8174-a94e75b33be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3798,\n",
       " [(('of', 'the'), 449),\n",
       "  (('the', 'king'), 220),\n",
       "  (('to', 'the'), 211),\n",
       "  (('in', 'the'), 167),\n",
       "  (('and', 'the'), 112),\n",
       "  (('by', 'the'), 97),\n",
       "  (('with', 'the'), 89),\n",
       "  (('of', 'his'), 74),\n",
       "  (('the', 'queen'), 73),\n",
       "  (('to', 'be'), 70),\n",
       "  (('of', 'a'), 68),\n",
       "  (('*', '*'), 66),\n",
       "  (('from', 'the'), 64),\n",
       "  (('on', 'the'), 63),\n",
       "  (('at', 'the'), 61),\n",
       "  (('of', 'her'), 56),\n",
       "  (('project', 'gutenberg™'), 54),\n",
       "  (('king', 'of'), 52),\n",
       "  (('for', 'the'), 51),\n",
       "  (('he', 'is'), 48),\n",
       "  (('is', 'the'), 48),\n",
       "  (('that', 'the'), 47),\n",
       "  (('as', 'the'), 42),\n",
       "  (('to', 'his'), 41),\n",
       "  (('in', 'a'), 40),\n",
       "  (('it', 'is'), 37),\n",
       "  (('with', 'his'), 37),\n",
       "  (('by', 'a'), 36),\n",
       "  (('in', 'his'), 34),\n",
       "  (('the', 'queen,'), 34),\n",
       "  (('the', 'sage'), 31),\n",
       "  (('the', 'project'), 30),\n",
       "  (('and', 'his'), 29),\n",
       "  (('is', 'a'), 29),\n",
       "  (('or', 'the'), 28),\n",
       "  (('as', 'a'), 28),\n",
       "  (('do', 'not'), 28),\n",
       "  (('of', 'this'), 28),\n",
       "  (('son', 'of'), 27),\n",
       "  (('him', 'to'), 26),\n",
       "  (('to', 'her'), 26),\n",
       "  (('is', 'now'), 25),\n",
       "  (('who', 'is'), 25),\n",
       "  (('and', 'is'), 25),\n",
       "  (('has', 'been'), 25),\n",
       "  (('into', 'the'), 25),\n",
       "  (('with', 'a'), 25),\n",
       "  (('is', 'not'), 24),\n",
       "  (('the', 'king,'), 23),\n",
       "  (('comes', 'to'), 23),\n",
       "  (('king', 'and'), 22),\n",
       "  (('they', 'are'), 22),\n",
       "  (('project', 'gutenberg'), 22),\n",
       "  (('her', 'to'), 21),\n",
       "  (('through', 'the'), 21),\n",
       "  (('of', 'my'), 21),\n",
       "  (('I', 'have'), 21),\n",
       "  (('after', 'a'), 20),\n",
       "  (('I', 'will'), 20),\n",
       "  (('one', 'of'), 19),\n",
       "  (('she', 'is'), 19),\n",
       "  (('which', 'the'), 19),\n",
       "  (('that', 'he'), 19),\n",
       "  (('the', 'king.'), 19),\n",
       "  (('his', 'own'), 19),\n",
       "  (('he', 'has'), 19),\n",
       "  (('about', 'to'), 19),\n",
       "  (('rama', 'and'), 19),\n",
       "  (('all', 'the'), 19),\n",
       "  (('upon', 'the'), 18),\n",
       "  (('with', 'her'), 18),\n",
       "  (('by', 'his'), 18),\n",
       "  (('to', 'a'), 18),\n",
       "  (('on', 'his'), 18),\n",
       "  (('the', 'two'), 18),\n",
       "  (('gutenberg™', 'electronic'), 18),\n",
       "  (('by', 'her'), 17),\n",
       "  (('in', 'which'), 17),\n",
       "  (('for', 'his'), 17),\n",
       "  (('of', 'which'), 17),\n",
       "  (('the', 'son'), 17),\n",
       "  (('I', 'am'), 17),\n",
       "  (('if', 'you'), 17),\n",
       "  (('terms', 'of'), 17),\n",
       "  (('was', 'the'), 16),\n",
       "  (('daughter', 'of'), 16),\n",
       "  (('to', 'have'), 16),\n",
       "  (('the', 'god'), 16),\n",
       "  (('have', 'been'), 16),\n",
       "  (('he', 'was'), 15),\n",
       "  (('king', 'is'), 15),\n",
       "  (('the', \"king's\"), 15),\n",
       "  (('who', 'has'), 15),\n",
       "  (('may', 'be'), 15),\n",
       "  (('the', 'daughter'), 14),\n",
       "  (('at', 'a'), 14),\n",
       "  (('at', 'last'), 14),\n",
       "  (('her', 'friend'), 14),\n",
       "  (('on', 'which'), 14),\n",
       "  (('you', 'have'), 14),\n",
       "  (('the', 'picture'), 14),\n",
       "  (('the', 'terms'), 14),\n",
       "  (('in', 'this'), 14),\n",
       "  (('the', 'work'), 14),\n",
       "  (('and', 'in'), 13),\n",
       "  (('when', 'he'), 13),\n",
       "  (('as', 'he'), 13),\n",
       "  (('to', 'give'), 13),\n",
       "  (('of', 'their'), 13),\n",
       "  (('her', 'and'), 13),\n",
       "  (('the', 'goddess'), 13),\n",
       "  (('come', 'to'), 13),\n",
       "  (('it', 'was'), 13),\n",
       "  (('the', 'minister'), 13),\n",
       "  (('will', 'be'), 13),\n",
       "  (('the', 'full'), 13),\n",
       "  (('gutenberg', 'literary'), 13),\n",
       "  (('literary', 'archive'), 13),\n",
       "  (('on', 'a'), 12),\n",
       "  (('to', 'see'), 12),\n",
       "  (('and', 'her'), 12),\n",
       "  (('must', 'be'), 12),\n",
       "  (('before', 'the'), 12),\n",
       "  (('is', 'in'), 12),\n",
       "  (('is', 'to'), 12),\n",
       "  (('between', 'the'), 12),\n",
       "  (('the', 'vidushaka'), 12),\n",
       "  (('it', 'to'), 12),\n",
       "  (('the', 'latter'), 12),\n",
       "  (('when', 'the'), 12),\n",
       "  (('of', 'rama'), 12),\n",
       "  (('a', 'very'), 12),\n",
       "  (('you', 'are'), 12),\n",
       "  (('at', 'this'), 12),\n",
       "  (('will', 'not'), 12),\n",
       "  (('the', 'united'), 12),\n",
       "  (('him', 'and'), 11),\n",
       "  (('for', 'a'), 11),\n",
       "  (('the', 'story'), 11),\n",
       "  (('but', 'he'), 11),\n",
       "  (('does', 'not'), 11),\n",
       "  (('cause', 'of'), 11),\n",
       "  (('and', 'goes'), 11),\n",
       "  (('goes', 'to'), 11),\n",
       "  (('whom', 'he'), 11),\n",
       "  (('a', 'great'), 11),\n",
       "  (('part', 'of'), 11),\n",
       "  (('accompanied', 'by'), 11),\n",
       "  (('a', 'female'), 11),\n",
       "  (('the', 'queen.'), 11),\n",
       "  (('the', 'author'), 11),\n",
       "  (('is', 'about'), 11),\n",
       "  (('god', 'of'), 11),\n",
       "  (('from', 'his'), 11),\n",
       "  (('and', 'other'), 11),\n",
       "  (('and', 'that'), 11),\n",
       "  (('rama', 'is'), 11),\n",
       "  (('the', 'gods'), 11),\n",
       "  (('such', 'a'), 11),\n",
       "  (('to', 'make'), 11),\n",
       "  (('electronic', 'works'), 11),\n",
       "  (('her', 'own'), 10),\n",
       "  (('for', 'her'), 10),\n",
       "  (('but', 'the'), 10),\n",
       "  (('her', 'in'), 10),\n",
       "  (('which', 'he'), 10),\n",
       "  (('the', 'most'), 10),\n",
       "  (('see', 'the'), 10),\n",
       "  (('this', 'is'), 10),\n",
       "  (('author', 'of'), 10),\n",
       "  (('enters', 'the'), 10),\n",
       "  (('under', 'the'), 10),\n",
       "  (('out', 'of'), 10),\n",
       "  (('the', 'raja'), 10),\n",
       "  (('his', 'friend'), 10),\n",
       "  (('as', 'well'), 10),\n",
       "  (('the', 'world'), 10),\n",
       "  (('his', 'brother'), 10),\n",
       "  (('a', 'son'), 10),\n",
       "  (('of', 'your'), 10),\n",
       "  (('the', 'same'), 10),\n",
       "  (('but', 'in'), 10),\n",
       "  (('not', 'be'), 10),\n",
       "  (('the', 'hindu'), 10),\n",
       "  (('and', 'was'), 9),\n",
       "  (('that', 'it'), 9),\n",
       "  (('return', 'to'), 9),\n",
       "  (('against', 'the'), 9),\n",
       "  (('and', 'he'), 9),\n",
       "  (('there', 'is'), 9),\n",
       "  (('who', 'had'), 9),\n",
       "  (('she', 'has'), 9),\n",
       "  (('over', 'the'), 9),\n",
       "  (('when', 'they'), 9),\n",
       "  (('the', 'friend'), 9),\n",
       "  (('they', 'have'), 9),\n",
       "  (('that', 'they'), 9),\n",
       "  (('that', 'you'), 9),\n",
       "  (('his', 'son'), 9),\n",
       "  (('to', 'take'), 9),\n",
       "  (('well', 'as'), 9),\n",
       "  (('the', 'young'), 9),\n",
       "  (('he', 'had'), 9),\n",
       "  (('the', 'great'), 9),\n",
       "  (('and', 'of'), 9),\n",
       "  (('then', 'the'), 9),\n",
       "  (('be', 'a'), 9),\n",
       "  (('the', 'people'), 9),\n",
       "  (('form', 'of'), 9),\n",
       "  (('are', 'the'), 9),\n",
       "  (('king', 'replies,'), 9),\n",
       "  (('the', \"queen's\"), 9),\n",
       "  (('access', 'to'), 9),\n",
       "  (('was', 'a'), 8),\n",
       "  (('the', 'hermitage'), 8),\n",
       "  (('not', 'to'), 8),\n",
       "  (('the', 'hermit'), 8),\n",
       "  (('a', 'messenger'), 8),\n",
       "  (('the', 'form'), 8),\n",
       "  (('which', 'she'), 8),\n",
       "  (('be', 'the'), 8),\n",
       "  (('the', 'following'), 8),\n",
       "  (('on', 'their'), 8),\n",
       "  (('king.', 'the'), 8),\n",
       "  (('the', 'cause'), 8),\n",
       "  (('while', 'the'), 8),\n",
       "  (('of', 'that'), 8),\n",
       "  (('that', 'she'), 8),\n",
       "  (('which', 'is'), 8),\n",
       "  (('sent', 'to'), 8),\n",
       "  (('which', 'they'), 8),\n",
       "  (('the', 'prince'), 8),\n",
       "  (('and', 'then'), 8),\n",
       "  (('to', 'bring'), 8),\n",
       "  (('they', 'were'), 8),\n",
       "  (('brother', 'of'), 8),\n",
       "  (('the', 'bow'), 8),\n",
       "  (('shall', 'be'), 8),\n",
       "  (('to', 'go'), 8),\n",
       "  (('aid', 'of'), 8),\n",
       "  (('the', 'sun'), 8),\n",
       "  (('with', 'this'), 8),\n",
       "  (('order', 'to'), 8),\n",
       "  (('the', 'world.'), 8),\n",
       "  (('agree', 'to'), 8),\n",
       "  (('in', 'my'), 8),\n",
       "  (('the', 'priestess'), 8),\n",
       "  (('the', 'temple'), 8),\n",
       "  (('temple', 'of'), 8),\n",
       "  (('of', 'all'), 8),\n",
       "  (('as', 'if'), 8),\n",
       "  (('no', 'one'), 8),\n",
       "  (('you', 'can'), 8),\n",
       "  (('allowed', 'to'), 8),\n",
       "  (('the', 'court'), 8),\n",
       "  (('krishna', 'and'), 8),\n",
       "  (('is', 'very'), 8),\n",
       "  (('to', 'your'), 8),\n",
       "  (('set', 'forth'), 8),\n",
       "  (('forth', 'in'), 8),\n",
       "  (('of', 'project'), 8),\n",
       "  (('electronic', 'work'), 8),\n",
       "  (('this', 'agreement'), 8),\n",
       "  (('archive', 'foundation'), 8),\n",
       "  (('hermitage', 'of'), 7),\n",
       "  (('the', 'chief'), 7),\n",
       "  (('she', 'was'), 7),\n",
       "  (('the', 'city'), 7),\n",
       "  (('the', 'lovers'), 7),\n",
       "  (('story', 'of'), 7),\n",
       "  (('to', 'him'), 7),\n",
       "  (('the', 'sight'), 7),\n",
       "  (('is', 'so'), 7),\n",
       "  (('from', 'a'), 7),\n",
       "  (('and', 'asks'), 7),\n",
       "  (('the', 'hero'), 7),\n",
       "  (('off', 'by'), 7),\n",
       "  (('that', 'a'), 7),\n",
       "  (('her.', 'the'), 7),\n",
       "  (('is', 'no'), 7),\n",
       "  (('a', 'few'), 7),\n",
       "  (('repair', 'to'), 7),\n",
       "  (('the', 'palace'), 7),\n",
       "  (('she', 'now'), 7),\n",
       "  (('consequence', 'of'), 7),\n",
       "  (('has', 'seen'), 7),\n",
       "  (('in', 'her'), 7),\n",
       "  (('on', 'her'), 7),\n",
       "  (('a', 'short'), 7),\n",
       "  (('compliance', 'with'), 7),\n",
       "  (('the', 'general'), 7),\n",
       "  (('the', 'last'), 7),\n",
       "  (('order', 'of'), 7),\n",
       "  (('to', 'effect'), 7),\n",
       "  (('queen', 'is'), 7),\n",
       "  (('whom', 'the'), 7),\n",
       "  (('the', 'person'), 7),\n",
       "  (('the', 'hands'), 7),\n",
       "  (('hands', 'of'), 7),\n",
       "  (('the', 'brother'), 7),\n",
       "  (('the', 'father'), 7),\n",
       "  (('arrival', 'of'), 7),\n",
       "  (('determines', 'to'), 7),\n",
       "  (('go', 'to'), 7),\n",
       "  (('the', 'way'), 7),\n",
       "  (('resolves', 'to'), 7),\n",
       "  (('the', 'monkey'), 7),\n",
       "  (('forth', 'to'), 7),\n",
       "  (('the', 'aid'), 7),\n",
       "  (('up', 'the'), 7),\n",
       "  (('him', 'that'), 7),\n",
       "  (('have', 'no'), 7),\n",
       "  (('after', 'some'), 7),\n",
       "  (('in', 'all'), 7),\n",
       "  (('in', 'their'), 7),\n",
       "  (('the', 'object'), 7),\n",
       "  (('and', 'a'), 7),\n",
       "  (('picture', 'and'), 7),\n",
       "  (('and', 'says,'), 7),\n",
       "  (('\"it', 'is'), 7),\n",
       "  (('court', 'of'), 7),\n",
       "  (('of', 'ravana,'), 7),\n",
       "  (('him', 'in'), 7),\n",
       "  (('up', 'to'), 7),\n",
       "  (('are', 'not'), 7),\n",
       "  (('work', 'in'), 7),\n",
       "  (('in', 'order'), 7),\n",
       "  (('enamoured', 'of'), 7),\n",
       "  (('copies', 'of'), 7),\n",
       "  (('you', 'may'), 7),\n",
       "  (('full', 'project'), 7),\n",
       "  (('this', 'work'), 7),\n",
       "  (('at', 'first'), 6),\n",
       "  (('like', 'a'), 6),\n",
       "  (('him', 'by'), 6),\n",
       "  (('is', 'at'), 6),\n",
       "  (('but', 'a'), 6),\n",
       "  (('account', 'of'), 6),\n",
       "  (('married', 'to'), 6),\n",
       "  (('the', 'marriage'), 6),\n",
       "  (('from', 'her'), 6),\n",
       "  (('king', 'then'), 6),\n",
       "  (('returns', 'to'), 6),\n",
       "  (('this,', 'the'), 6),\n",
       "  (('sent', 'by'), 6),\n",
       "  (('her', 'by'), 6),\n",
       "  (('her', 'husband,'), 6),\n",
       "  (('the', 'gods,'), 6),\n",
       "  (('the', 'garden'), 6),\n",
       "  (('to', 'meet'), 6),\n",
       "  (('to', 'him,'), 6),\n",
       "  (('queen,', 'who'), 6),\n",
       "  (('she', 'had'), 6),\n",
       "  (('her', 'as'), 6),\n",
       "  (('behind', 'the'), 6),\n",
       "  (('takes', 'the'), 6),\n",
       "  (('into', 'a'), 6),\n",
       "  (('at', 'her'), 6),\n",
       "  (('restored', 'to'), 6),\n",
       "  (('the', 'forest'), 6),\n",
       "  (('to', 'embrace'), 6),\n",
       "  (('I', 'shall'), 6),\n",
       "  (('the', 'messenger'), 6),\n",
       "  (('to', 'pay'), 6),\n",
       "  (('favour', 'of'), 6),\n",
       "  (('that', 'is'), 6),\n",
       "  (('his', 'queen'), 6),\n",
       "  (('by', 'order'), 6),\n",
       "  (('the', 'party'), 6),\n",
       "  (('is', 'highly'), 6),\n",
       "  (('with', 'their'), 6),\n",
       "  (('disturbed', 'by'), 6),\n",
       "  (('command', 'of'), 6),\n",
       "  (('off', 'the'), 6),\n",
       "  (('and', 'all'), 6),\n",
       "  (('life', 'of'), 6),\n",
       "  (('father', 'of'), 6),\n",
       "  (('of', 'rama.'), 6),\n",
       "  (('rama.', 'the'), 6),\n",
       "  (('ravana', 'to'), 6),\n",
       "  (('some', 'of'), 6),\n",
       "  (('of', 'siva,'), 6),\n",
       "  (('rama', 'to'), 6),\n",
       "  (('in', 'its'), 6),\n",
       "  (('the', 'arrival'), 6),\n",
       "  (('endeavour', 'to'), 6),\n",
       "  (('the', 'earth'), 6),\n",
       "  (('to', 'assist'), 6),\n",
       "  (('he', 'then'), 6),\n",
       "  (('appearance', 'of'), 6),\n",
       "  (('the', 'whole'), 6),\n",
       "  (('or', 'other'), 6),\n",
       "  (('his', 'wife'), 6),\n",
       "  (('the', 'inner'), 6),\n",
       "  (('protected', 'by'), 6),\n",
       "  (('care', 'of'), 6),\n",
       "  (('the', 'hand'), 6),\n",
       "  (('and', 'they'), 6),\n",
       "  (('and', 'with'), 6),\n",
       "  (('object', 'of'), 6),\n",
       "  (('you', 'to'), 6),\n",
       "  (('house', 'of'), 6),\n",
       "  (('a', 'picture'), 6),\n",
       "  (('at', 'once'), 6),\n",
       "  (('the', 'princess'), 6),\n",
       "  (('to', 'whom'), 6),\n",
       "  (('to', 'our'), 6),\n",
       "  (('the', 'brahmin'), 6),\n",
       "  (('sons', 'of'), 6),\n",
       "  (('in', 'vain.'), 6),\n",
       "  (('overcome', 'with'), 6),\n",
       "  (('of', 'one'), 6),\n",
       "  (('lord', 'of'), 6),\n",
       "  (('is', 'also'), 6),\n",
       "  (('is', 'my'), 6),\n",
       "  (('with', 'which'), 6),\n",
       "  (('you', 'must'), 6),\n",
       "  (('I', 'must'), 6),\n",
       "  (('his', 'daughter'), 6),\n",
       "  (('upon', 'his'), 6),\n",
       "  (('the', 'brahmans'), 6),\n",
       "  (('the', 'drama'), 6),\n",
       "  (('of', 'any'), 6),\n",
       "  (('distribution', 'of'), 6),\n",
       "  (('the', 'play'), 6),\n",
       "  (('united', 'states'), 6),\n",
       "  (('gutenberg™', 'license'), 6),\n",
       "  (('this', 'agreement,'), 6),\n",
       "  (('in', 'paragraph'), 6),\n",
       "  (('the', 'copyright'), 6),\n",
       "  (('near', 'the'), 5),\n",
       "  (('the', 'first'), 5),\n",
       "  (('sight', 'of'), 5),\n",
       "  (('as', 'an'), 5),\n",
       "  (('for', 'him'), 5),\n",
       "  (('to', 'return'), 5),\n",
       "  (('about', 'her'), 5),\n",
       "  (('friend', 'and'), 5),\n",
       "  (('city', 'of'), 5),\n",
       "  (('to', 'their'), 5),\n",
       "  (('asks', 'her'), 5),\n",
       "  (('with', 'an'), 5),\n",
       "  (('to', 'get'), 5),\n",
       "  (('according', 'to'), 5),\n",
       "  (('king', 'to'), 5),\n",
       "  (('then', 'goes'), 5),\n",
       "  (('the', 'sage,'), 5),\n",
       "  (('if', 'the'), 5),\n",
       "  (('at', 'his'), 5),\n",
       "  (('the', 'end'), 5),\n",
       "  (('end', 'of'), 5),\n",
       "  (('the', 'boy'), 5),\n",
       "  (('as', 'to'), 5),\n",
       "  (('the', 'name'), 5),\n",
       "  (('and', 'on'), 5),\n",
       "  (('in', 'love'), 5),\n",
       "  (('him,', 'and'), 5),\n",
       "  (('goes', 'off'), 5),\n",
       "  (('goddess', 'of'), 5),\n",
       "  (('days', 'of'), 5),\n",
       "  (('to', 'my'), 5),\n",
       "  (('with', 'him'), 5),\n",
       "  (('to', 'marry'), 5),\n",
       "  (('the', 'moon'), 5),\n",
       "  (('his', 'eyes'), 5),\n",
       "  (('the', 'divine'), 5),\n",
       "  (('in', 'consequence'), 5),\n",
       "  (('the', 'air.'), 5),\n",
       "  (('have', 'seen'), 5),\n",
       "  (('during', 'the'), 5),\n",
       "  (('he', 'thinks'), 5),\n",
       "  (('now', 'comes'), 5),\n",
       "  (('the', 'female'), 5),\n",
       "  (('who', 'was'), 5),\n",
       "  (('her', 'husband'), 5),\n",
       "  (('is', 'thus'), 5),\n",
       "  (('what', 'is'), 5),\n",
       "  (('the', 'pupil'), 5),\n",
       "  (('that', 'this'), 5),\n",
       "  (('to', 'put'), 5),\n",
       "  (('to', 'try'), 5),\n",
       "  (('to', 'do'), 5),\n",
       "  (('king,', 'who'), 5),\n",
       "  (('a', 'visit'), 5),\n",
       "  (('visit', 'to'), 5),\n",
       "  (('the', 'only'), 5),\n",
       "  (('time', 'to'), 5),\n",
       "  (('derived', 'from'), 5),\n",
       "  (('about', 'the'), 5),\n",
       "  (('to', 'which'), 5),\n",
       "  (('way', 'to'), 5),\n",
       "  (('the', 'character'), 5),\n",
       "  (('character', 'of'), 5),\n",
       "  (('a', 'letter'), 5),\n",
       "  (('for', 'which'), 5),\n",
       "  (('the', 'news'), 5),\n",
       "  (('and', 'lakshmana'), 5),\n",
       "  (('wife', 'of'), 5),\n",
       "  (('the', 'demon'), 5),\n",
       "  (('mother', 'of'), 5),\n",
       "  (('with', 'all'), 5),\n",
       "  (('bow', 'of'), 5),\n",
       "  (('have', 'heard'), 5),\n",
       "  (('of', \"rama's\"), 5),\n",
       "  (('me', 'a'), 5),\n",
       "  (('to', 'rama,'), 5),\n",
       "  (('her', 'father'), 5),\n",
       "  (('off', 'to'), 5),\n",
       "  (('and', 'to'), 5),\n",
       "  (('means', 'of'), 5),\n",
       "  (('the', 'feet'), 5),\n",
       "  (('feet', 'of'), 5),\n",
       "  (('then', 'enters'), 5),\n",
       "  (('one', 'or'), 5),\n",
       "  (('they', 'go'), 5),\n",
       "  (('\"I', 'am'), 5),\n",
       "  (('give', 'up'), 5),\n",
       "  (('orders', 'his'), 5),\n",
       "  (('out', 'to'), 5),\n",
       "  (('himself', 'to'), 5),\n",
       "  (('sita.', 'he'), 5),\n",
       "  (('the', 'banks'), 5),\n",
       "  (('banks', 'of'), 5),\n",
       "  (('when', 'a'), 5),\n",
       "  (('are', 'a'), 5),\n",
       "  (('had', 'a'), 5),\n",
       "  (('he', 'now'), 5),\n",
       "  (('addresses', 'her'), 5),\n",
       "  (('the', 'world,'), 5),\n",
       "  (('could', 'not'), 5),\n",
       "  (('life', 'and'), 5),\n",
       "  (('work', 'of'), 5),\n",
       "  (('says,', '\"I'), 5),\n",
       "  (('the', 'house'), 5),\n",
       "  (('\"I', 'have'), 5),\n",
       "  (('for', 'your'), 5),\n",
       "  (('give', 'me'), 5),\n",
       "  (('beauty', 'of'), 5),\n",
       "  (('my', 'heart'), 5),\n",
       "  (('her', 'for'), 5),\n",
       "  (('received', 'the'), 5),\n",
       "  (('your', 'majesty'), 5),\n",
       "  (('the', 'presence'), 5),\n",
       "  (('presence', 'of'), 5),\n",
       "  (('all', 'this'), 5),\n",
       "  (('the', 'public'), 5),\n",
       "  (('the', 'magician'), 5),\n",
       "  (('such', 'as'), 5),\n",
       "  (('in', 'vain'), 5),\n",
       "  (('his', \"father's\"), 5),\n",
       "  (('the', 'secret'), 5),\n",
       "  (('a', 'voice'), 5),\n",
       "  (('voice', 'from'), 5),\n",
       "  (('from', 'heaven,'), 5),\n",
       "  (('slain', 'by'), 5),\n",
       "  (('I', 'can'), 5),\n",
       "  (('on', 'this'), 5),\n",
       "  (('with', 'great'), 5),\n",
       "  (('his', 'wife,'), 5),\n",
       "  (('the', 'three'), 5),\n",
       "  (('how', 'can'), 5),\n",
       "  (('can', 'I'), 5),\n",
       "  (('me.', 'I'), 5),\n",
       "  (('not', 'a'), 5),\n",
       "  (('this', 'time,'), 5),\n",
       "  (('need', 'not'), 5),\n",
       "  (('can', 'be'), 5),\n",
       "  (('be', 'found'), 5),\n",
       "  (('the', 'former'), 5),\n",
       "  (('of', 'krishna'), 5),\n",
       "  (('and', 'its'), 5),\n",
       "  (('mission', 'of'), 5),\n",
       "  (('become', 'the'), 5),\n",
       "  (('king', 'observes,'), 5),\n",
       "  (('your', \"majesty's\"), 5),\n",
       "  (('it', 'must'), 5),\n",
       "  (('the', 'sanskrit'), 5),\n",
       "  (('state', 'of'), 5),\n",
       "  (('in', 'any'), 5),\n",
       "  (('was', 'composed'), 5),\n",
       "  (('of', 'english'), 5),\n",
       "  (('not', 'protected'), 5),\n",
       "  (('by', 'u.s.'), 5),\n",
       "  (('the', 'foundation'), 5),\n",
       "  (('the', 'trademark'), 5),\n",
       "  (('use', 'of'), 5),\n",
       "  (('comply', 'with'), 5),\n",
       "  (('works', 'in'), 5),\n",
       "  (('a', 'project'), 5),\n",
       "  (('work', 'is'), 5),\n",
       "  (('or', 'any'), 5),\n",
       "  (('to', 'you'), 5),\n",
       "  (('information', 'about'), 5),\n",
       "  (('the', 'lunar'), 4),\n",
       "  (('to', 'kill'), 4),\n",
       "  (('kill', 'the'), 4),\n",
       "  (('the', 'favourite'), 4),\n",
       "  (('sage', 'viswamitra'), 4),\n",
       "  (('he', 'soon'), 4),\n",
       "  (('his', 'heart'), 4),\n",
       "  (('he', 'finds'), 4),\n",
       "  (('to', 'come'), 4),\n",
       "  (('preparations', 'for'), 4),\n",
       "  (('messenger', 'from'), 4),\n",
       "  (('he', 'sends'), 4),\n",
       "  (('as', 'his'), 4),\n",
       "  (('out', 'the'), 4),\n",
       "  (('on', 'account'), 4),\n",
       "  (('love', 'for'), 4),\n",
       "  (('is', 'discovered'), 4),\n",
       "  (('in', 'an'), 4),\n",
       "  (('conversation', 'with'), 4),\n",
       "  (('and', 'offers'), 4),\n",
       "  (('her', 'companion'), 4),\n",
       "  (('to', 'satisfy'), 4),\n",
       "  (('issue', 'of'), 4),\n",
       "  (('the', 'throne'), 4),\n",
       "  (('his', 'spiritual'), 4),\n",
       "  (('absorbed', 'in'), 4),\n",
       "  (('comes', 'and'), 4),\n",
       "  (('and', 'by'), 4),\n",
       "  (('the', 'curse'), 4),\n",
       "  (('is', 'sent'), 4),\n",
       "  (('his', 'two'), 4),\n",
       "  (('friends.', 'the'), 4),\n",
       "  (('heart', 'is'), 4),\n",
       "  (('of', 'an'), 4),\n",
       "  (('an', 'old'), 4),\n",
       "  (('then', 'he'), 4),\n",
       "  (('may', 'the'), 4),\n",
       "  (('the', 'dust'), 4),\n",
       "  (('arrive', 'at'), 4),\n",
       "  (('a', 'while,'), 4),\n",
       "  (('his', 'palace'), 4),\n",
       "  (('appears', 'and'), 4),\n",
       "  (('him', 'with'), 4),\n",
       "  (('the', 'race'), 4),\n",
       "  (('race', 'of'), 4),\n",
       "  (('his', 'son.'), 4),\n",
       "  (('name', 'of'), 4),\n",
       "  (('is', 'said'), 4),\n",
       "  (('said', 'to'), 4),\n",
       "  (('carried', 'off'), 4),\n",
       "  (('the', 'nymph'), 4),\n",
       "  (('back', 'to'), 4),\n",
       "  (('love', 'with'), 4),\n",
       "  (('wishes', 'to'), 4),\n",
       "  (('the', 'eyes'), 4),\n",
       "  (('eyes', 'of'), 4),\n",
       "  (('those', 'of'), 4),\n",
       "  (('garden', 'of'), 4),\n",
       "  (('is', 'his'), 4),\n",
       "  (('and', 'carries'), 4),\n",
       "  (('the', 'meantime,'), 4),\n",
       "  (('of', 'love.'), 4),\n",
       "  (('and', 'at'), 4),\n",
       "  (('messenger', 'of'), 4),\n",
       "  (('this', 'the'), 4),\n",
       "  (('and,', 'after'), 4),\n",
       "  (('off', 'in'), 4),\n",
       "  (('her', 'husband.'), 4),\n",
       "  (('the', 'play,'), 4),\n",
       "  (('as', 'she'), 4),\n",
       "  (('my', 'friend'), 4),\n",
       "  (('me', 'in'), 4),\n",
       "  (('arrives', 'from'), 4),\n",
       "  (('his', 'majesty'), 4),\n",
       "  (('king', 'as'), 4),\n",
       "  (('no', 'other'), 4),\n",
       "  (('queen', 'in'), 4),\n",
       "  (('king', 'enters'), 4),\n",
       "  (('enters', 'a'), 4),\n",
       "  (('away', 'his'), 4),\n",
       "  (('where', 'she'), 4),\n",
       "  (('the', 'mountain'), 4),\n",
       "  (('the', 'mighty'), 4),\n",
       "  (('are', 'thus'), 4),\n",
       "  (('and', 'return'), 4),\n",
       "  (('away', 'the'), 4),\n",
       "  (('by', 'which'), 4),\n",
       "  (('female', 'ascetic'), 4),\n",
       "  (('was', 'not'), 4),\n",
       "  (('and', 'tells'), 4),\n",
       "  (('that', 'I'), 4),\n",
       "  (('me', 'to'), 4),\n",
       "  (('in', 'compliance'), 4),\n",
       "  (('prepares', 'to'), 4),\n",
       "  (('announce', 'that'), 4),\n",
       "  (('to', 'remain'), 4),\n",
       "  (('having', 'been'), 4),\n",
       "  (('general', 'of'), 4),\n",
       "  (('belongs', 'to'), 4),\n",
       "  (('picture', 'of'), 4),\n",
       "  (('the', 'latter,'), 4),\n",
       "  (('him', 'the'), 4),\n",
       "  (('her.', 'she'), 4),\n",
       "  (('induced', 'to'), 4),\n",
       "  (('engaged', 'in'), 4),\n",
       "  (('the', 'parivrajaka,'), 4),\n",
       "  (('queen,', 'and'), 4),\n",
       "  (('in', 'great'), 4),\n",
       "  (('her', 'lover'), 4),\n",
       "  (('amongst', 'the'), 4),\n",
       "  (('the', 'sister'), 4),\n",
       "  (('sister', 'of'), 4),\n",
       "  (('seized', 'by'), 4),\n",
       "  (('that', 'of'), 4),\n",
       "  (('which', 'it'), 4),\n",
       "  (('have', 'a'), 4),\n",
       "  (('only', 'by'), 4),\n",
       "  (('had', 'been'), 4),\n",
       "  (('the', 'command'), 4),\n",
       "  (('as', 'their'), 4),\n",
       "  (('bharata', 'and'), 4),\n",
       "  (('sita', 'and'), 4),\n",
       "  (('influence', 'of'), 4),\n",
       "  (('and', 'gives'), 4),\n",
       "  (('to', 'rama'), 4),\n",
       "  (('to', 'invite'), 4),\n",
       "  (('of', 'whom'), 4),\n",
       "  (('is', 'killed'), 4),\n",
       "  (('goes', 'away'), 4),\n",
       "  (('the', 'sages'), 4),\n",
       "  (('he', 'hears'), 4),\n",
       "  (('anxious', 'for'), 4),\n",
       "  (('the', 'kshatriya'), 4),\n",
       "  (('which', 'I'), 4),\n",
       "  (('his', 'father'), 4),\n",
       "  (('father', 'to'), 4),\n",
       "  (('him', 'of'), 4),\n",
       "  (('maternal', 'uncle'), 4),\n",
       "  (('and', 'sita'), 4),\n",
       "  (('which', 'his'), 4),\n",
       "  (('a', 'holy'), 4),\n",
       "  (('approaches', 'the'), 4),\n",
       "  (('he', 'goes'), 4),\n",
       "  (('the', 'monkeys'), 4),\n",
       "  (('they', 'then'), 4),\n",
       "  (('instigated', 'by'), 4),\n",
       "  (('for', 'their'), 4),\n",
       "  (('tells', 'him'), 4),\n",
       "  (('to', 'restore'), 4),\n",
       "  (('him.', 'he'), 4),\n",
       "  (('he', 'orders'), 4),\n",
       "  (('were', 'not'), 4),\n",
       "  (('goes', 'forth'), 4),\n",
       "  (('ravana,', 'and'), 4),\n",
       "  (('sita', 'is'), 4),\n",
       "  (('the', 'fiery'), 4),\n",
       "  (('which', 'was'), 4),\n",
       "  (('the', 'capital'), 4),\n",
       "  (('ready', 'to'), 4),\n",
       "  (('sita', 'to'), 4),\n",
       "  (('wife', 'and'), 4),\n",
       "  (('the', 'boys'), 4),\n",
       "  (('body', 'is'), 4),\n",
       "  (('of', 'such'), 4),\n",
       "  (('and', 'an'), 4),\n",
       "  (('the', 'favour'), 4),\n",
       "  (('an', 'end'), 4),\n",
       "  (('that', 'his'), 4),\n",
       "  (('he', 'disregarded'), 4),\n",
       "  (('daughter', 'to'), 4),\n",
       "  (('the', 'real'), 4),\n",
       "  (('people', 'are'), 4),\n",
       "  (('satisfied', 'with'), 4),\n",
       "  (('the', 'husband'), 4),\n",
       "  (('I', 'could'), 4),\n",
       "  (('malatI', 'and'), 4),\n",
       "  (('and', 'madhava'), 4),\n",
       "  (('malati.', 'the'), 4),\n",
       "  (('did', 'not'), 4),\n",
       "  (('festival', 'of'), 4),\n",
       "  (('to', 'join'), 4),\n",
       "  (('were', 'the'), 4),\n",
       "  (('the', 'flowery'), 4),\n",
       "  (('it', 'with'), 4),\n",
       "  (('of', 'malatI'), 4),\n",
       "  (('is', 'your'), 4),\n",
       "  (('by', 'their'), 4),\n",
       "  (('and', 'you'), 4),\n",
       "  (('as', 'you'), 4),\n",
       "  (('sovereign', 'of'), 4),\n",
       "  (('the', 'burden'), 4),\n",
       "  (('burden', 'of'), 4),\n",
       "  (('is', 'brought'), 4),\n",
       "  (('arrives', 'to'), 4),\n",
       "  (('the', 'shrine'), 4),\n",
       "  (('in', 'search'), 4),\n",
       "  (('search', 'of'), 4),\n",
       "  (('the', 'place'), 4),\n",
       "  (('the', 'death'), 4),\n",
       "  (('death', 'of'), 4),\n",
       "  (('in', 'whose'), 4),\n",
       "  (('purpose', 'of'), 4),\n",
       "  (('at', 'this,'), 4),\n",
       "  (('the', 'sons'), 4),\n",
       "  (('the', 'prince,'), 4),\n",
       "  (('to', 'any'), 4),\n",
       "  (('the', 'dead'), 4),\n",
       "  (('of', 'our'), 4),\n",
       "  (('from', 'their'), 4),\n",
       "  (('orders', 'the'), 4),\n",
       "  (('the', 'other'), 4),\n",
       "  (('destruction', 'of'), 4),\n",
       "  (('a', 'new'), 4),\n",
       "  (('I', 'was'), 4),\n",
       "  (('or', 'a'), 4),\n",
       "  (('the', 'pandavas,'), 4),\n",
       "  (('queen', 'of'), 4),\n",
       "  (('to', 'prepare'), 4),\n",
       "  (('from', 'which'), 4),\n",
       "  (('he', 'will'), 4),\n",
       "  (('ensues', 'between'), 4),\n",
       "  (('is', 'slain'), 4),\n",
       "  (('vision', 'of'), 4),\n",
       "  (('my', 'dear'), 4),\n",
       "  (('king', 'addresses'), 4),\n",
       "  (('made', 'a'), 4),\n",
       "  (('with', 'these'), 4),\n",
       "  (('lord', 'kausika!'), 4),\n",
       "  (('a', 'fee'), 4),\n",
       "  (('the', 'fee'), 4),\n",
       "  (('way', 'of'), 4),\n",
       "  (('queen', 'and'), 4),\n",
       "  (('her', 'thus:—'), 4),\n",
       "  (('if', 'a'), 4),\n",
       "  (('no', 'longer'), 4),\n",
       "  (('do', 'with'), 4),\n",
       "  (('sun', 'is'), 4),\n",
       "  (('me', 'with'), 4),\n",
       "  (('sense', 'of'), 4),\n",
       "  (('to', 'burn'), 4),\n",
       "  (('the', 'lord'), 4),\n",
       "  (('you', 'will'), 4),\n",
       "  (('you', 'need'), 4),\n",
       "  (('specimen', 'of'), 4),\n",
       "  (('a', 'long'), 4),\n",
       "  (('thinks', 'of'), 4),\n",
       "  (('to', 'me,'), 4),\n",
       "  (('hand', 'is'), 4),\n",
       "  (('is', 'never'), 4),\n",
       "  (('grandson', 'of'), 4),\n",
       "  (('of', 'krishna,'), 4),\n",
       "  (('seen', 'in'), 4),\n",
       "  (('surrounded', 'by'), 4),\n",
       "  (('his', 'minister'), 4),\n",
       "  (('point', 'of'), 4),\n",
       "  (('power', 'of'), 4),\n",
       "  (('where', 'they'), 4),\n",
       "  (('consented', 'to'), 4),\n",
       "  (('the', 'time'), 4),\n",
       "  (('to', 'offer'), 4),\n",
       "  (('of', 'chanakya'), 4),\n",
       "  (('the', 'deity'), 4),\n",
       "  (('deity', 'of'), 4),\n",
       "  (('called', 'the'), 4),\n",
       "  (('the', 'damsel'), 4),\n",
       "  (('drama', 'is'), 4),\n",
       "  (('take', 'place'), 4),\n",
       "  (('to', 'behold'), 4),\n",
       "  (('queen', 'now'), 4),\n",
       "  (('the', 'sea,'), 4),\n",
       "  (('whom', 'I'), 4),\n",
       "  (('it', 'seems'), 4),\n",
       "  (('as', 'in'), 4),\n",
       "  (('are', 'in'), 4),\n",
       "  (('as', 'I'), 4),\n",
       "  (('we', 'have'), 4),\n",
       "  (('number', 'of'), 4),\n",
       "  (('of', 'sanskrit'), 4),\n",
       "  (('the', 'dramatis'), 4),\n",
       "  (('there', 'are'), 4),\n",
       "  (('composed', 'by'), 4),\n",
       "  (('\"the', \"boy's\"), 4),\n",
       "  (('m.', 'a.'), 4),\n",
       "  (('professor', 'of'), 4),\n",
       "  (('english', 'literature,'), 4),\n",
       "  (('matriculation', 'students.'), 4),\n",
       "  (('of', 'matriculation'), 4),\n",
       "  (('u.s.', 'copyright'), 4),\n",
       "  (('copyright', 'law'), 4),\n",
       "  (('you', 'do'), 4),\n",
       "  (('any', 'other'), 4),\n",
       "  (('the', 'phrase'), 4),\n",
       "  (('phrase', '“project'), 4),\n",
       "  (('you', 'agree'), 4),\n",
       "  (('“project', 'gutenberg”'), 4),\n",
       "  (('united', 'states.'), 4),\n",
       "  (('associated', 'with'), 4),\n",
       "  (('laws', 'of'), 4),\n",
       "  (('the', 'laws'), 4),\n",
       "  (('work', 'or'), 4),\n",
       "  (('and', 'any'), 4),\n",
       "  (('provide', 'a'), 4),\n",
       "  (('•', 'you'), 4),\n",
       "  (('donations', 'to'), 4),\n",
       "  (('foundation,', 'the'), 4),\n",
       "  (('of', 'hastinapur.'), 3),\n",
       "  (('goes', 'out'), 3),\n",
       "  (('pursuit', 'of'), 3),\n",
       "  (('chief', 'of'), 3),\n",
       "  (('the', 'hermits'), 3),\n",
       "  (('beautiful', 'and'), 3),\n",
       "  (('the', 'celestial'), 3),\n",
       "  (('the', 'celebrated'), 3),\n",
       "  (('the', 'love'), 3),\n",
       "  (('an', 'opportunity'), 3),\n",
       "  (('even', 'in'), 3),\n",
       "  (('the', 'hermitage,'), 3),\n",
       "  (('the', 'jester'), 3),\n",
       "  (('he', 'comes'), 3),\n",
       "  (('offers', 'to'), 3),\n",
       "  (('a', 'second'), 3),\n",
       "  (('time,', 'the'), 3),\n",
       "  (('see', 'if'), 3),\n",
       "  (('obstacle', 'to'), 3),\n",
       "  (('united', 'in'), 3),\n",
       "  (('to', 'wait'), 3),\n",
       "  (('wait', 'till'), 3),\n",
       "  (('give', 'his'), 3),\n",
       "  (('throne', 'of'), 3),\n",
       "  (('of', 'what'), 3),\n",
       "  (('worthy', 'of'), 3),\n",
       "  (('in', 'every'), 3),\n",
       "  (('a', 'curse'), 3),\n",
       "  (('it', 'and'), 3),\n",
       "  (('sage,', 'who'), 3),\n",
       "  (('of', 'some'), 3),\n",
       "  (('ornament', 'of'), 3),\n",
       "  (('the', 'seventh'), 3),\n",
       "  (('the', 'hermitage.'), 3),\n",
       "  (('takes', 'leave'), 3),\n",
       "  (('calls', 'upon'), 3),\n",
       "  (('the', 'ring'), 3),\n",
       "  (('asks', 'his'), 3),\n",
       "  (('his', 'family'), 3),\n",
       "  (('family', 'priest'), 3),\n",
       "  (('them', 'the'), 3),\n",
       "  (('the', 'priest'), 3),\n",
       "  (('informs', 'the'), 3),\n",
       "  (('it.', 'the'), 3),\n",
       "  (('curse', 'of'), 3),\n",
       "  (('the', 'conduct'), 3),\n",
       "  (('conduct', 'of'), 3),\n",
       "  (('go', 'away'), 3),\n",
       "  (('have', 'to'), 3),\n",
       "  (('then', 'comes'), 3),\n",
       "  (('following', 'the'), 3),\n",
       "  (('the', 'royal'), 3),\n",
       "  (('to', 'him.'), 3),\n",
       "  (('upon', 'him'), 3),\n",
       "  (('of', 'sakuntala,'), 3),\n",
       "  (('appears', 'to'), 3),\n",
       "  (('makes', 'his'), 3),\n",
       "  (('his', 'aerial'), 3),\n",
       "  (('while', 'he'), 3),\n",
       "  (('of', 'indra,'), 3),\n",
       "  (('he', 'sees'), 3),\n",
       "  (('be', 'his'), 3),\n",
       "  (('son,', 'he'), 3),\n",
       "  (('the', 'touch'), 3),\n",
       "  (('touch', 'of'), 3),\n",
       "  (('boy', 'is'), 3),\n",
       "  (('a', 'scion'), 3),\n",
       "  (('scion', 'of'), 3),\n",
       "  (('for', 'him.'), 3),\n",
       "  (('son.', 'the'), 3),\n",
       "  (('from', 'whom'), 3),\n",
       "  (('he', 'learns'), 3),\n",
       "  (('and', 'from'), 3),\n",
       "  (('to', 'this'), 3),\n",
       "  (('hero', 'and'), 3),\n",
       "  (('from', 'an'), 3),\n",
       "  (('his', 'chariot,'), 3),\n",
       "  (('on', 'hearing'), 3),\n",
       "  (('and', 'restores'), 3),\n",
       "  (('and', 'she'), 3),\n",
       "  (('being', 'summoned'), 3),\n",
       "  (('the', 'modern'), 3),\n",
       "  (('a', 'secret'), 3),\n",
       "  (('himself', 'in'), 3),\n",
       "  (('servant', 'of'), 3),\n",
       "  (('can', 'no'), 3),\n",
       "  (('his', 'return'), 3),\n",
       "  (('the', 'battle'), 3),\n",
       "  (('the', 'air'), 3),\n",
       "  (('invisible', 'to'), 3),\n",
       "  (('gods,', 'and'), 3),\n",
       "  (('but', 'it'), 3),\n",
       "  (('carried', 'away'), 3),\n",
       "  (('picked', 'up'), 3),\n",
       "  (('up', 'by'), 3),\n",
       "  (('the', 'garden.'), 3),\n",
       "  (('on', 'whom'), 3),\n",
       "  (('instead', 'of'), 3),\n",
       "  (('after', 'the'), 3),\n",
       "  (('friend', 'in'), 3),\n",
       "  (('the', 'terrace'), 3),\n",
       "  (('terrace', 'of'), 3),\n",
       "  ...])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = list(filter(lambda x: 1 < int(x[1]), ngrams_up_to_20[0].most_common()))\n",
    "len(l), l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "29435c00-ed5c-4082-a9f1-5e3925b7b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_filter(ngrams):\n",
    "    return filter(remove_periods, list(filter(lambda x: 1 < int(x[1]), ngrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "427c4cc2-f2a5-4552-8bc1-58b09e213de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_to_learn = ngrams_up_to_20[0]\n",
    "X_train_example = [[word_to_index[sent[0][0]]] for sent in my_filter(bigrams_to_learn.most_common())\n",
    "                  if sent[0][0] in word_to_index and sent[0][1] in word_to_index]\n",
    "y_train_example = [[word_to_index[sent[0][1]]] for sent in my_filter(bigrams_to_learn.most_common())\n",
    "                  if sent[0][0] in word_to_index and sent[0][1] in word_to_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "aa739a96-778d-4bcf-a89d-bad415d6eae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[4], [0], [6], [9], [5], [13], [12], [4], [0], [6]],\n",
       " [[0], [15], [0], [0], [0], [0], [0], [10], [36], [31]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_example[0:10], y_train_example[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9473d95e-7af5-4d05-b248-5b119c8c42ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3127, 3127)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_example), len(y_train_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "532376c7-51f2-4bd0-9611-4581f5023247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'king', 'of'),\n",
       " ('*', '*', '*'),\n",
       " ('the', 'king', 'and'),\n",
       " ('project', 'gutenberg™', 'electronic'),\n",
       " ('the', 'son', 'of'),\n",
       " ('the', 'project', 'gutenberg'),\n",
       " ('the', 'king', 'is'),\n",
       " ('the', 'daughter', 'of'),\n",
       " ('the', 'project', 'gutenberg™'),\n",
       " ('project', 'gutenberg', 'literary'),\n",
       " ('gutenberg', 'literary', 'archive'),\n",
       " ('of', 'the', 'king'),\n",
       " ('the', 'terms', 'of'),\n",
       " ('is', 'about', 'to'),\n",
       " ('the', 'god', 'of'),\n",
       " ('king', 'of', 'the'),\n",
       " ('one', 'of', 'the'),\n",
       " ('terms', 'of', 'this'),\n",
       " ('as', 'well', 'as'),\n",
       " ('the', 'king', 'replies,'),\n",
       " ('of', 'the', 'king,'),\n",
       " ('to', 'the', 'king'),\n",
       " ('the', 'cause', 'of'),\n",
       " ('the', 'author', 'of'),\n",
       " ('of', 'the', 'queen,'),\n",
       " ('to', 'be', 'a'),\n",
       " ('of', 'the', 'project'),\n",
       " ('in', 'the', 'united'),\n",
       " ('set', 'forth', 'in'),\n",
       " ('gutenberg™', 'electronic', 'works'),\n",
       " ('of', 'project', 'gutenberg™'),\n",
       " ('literary', 'archive', 'foundation'),\n",
       " ('the', 'hermitage', 'of'),\n",
       " ('to', 'the', 'queen,'),\n",
       " ('the', 'hands', 'of'),\n",
       " ('the', 'brother', 'of'),\n",
       " ('the', 'aid', 'of'),\n",
       " ('the', 'temple', 'of'),\n",
       " ('the', 'court', 'of'),\n",
       " ('in', 'order', 'to'),\n",
       " ('to', 'have', 'been'),\n",
       " ('to', 'the', 'project'),\n",
       " ('that', 'it', 'is'),\n",
       " ('the', 'story', 'of'),\n",
       " ('the', 'king', 'then'),\n",
       " ('him', 'to', 'be'),\n",
       " ('the', 'queen,', 'who'),\n",
       " ('by', 'order', 'of'),\n",
       " ('the', 'queen', 'is'),\n",
       " ('the', 'father', 'of'),\n",
       " ('the', 'arrival', 'of'),\n",
       " ('the', 'form', 'of'),\n",
       " ('king', 'and', 'the'),\n",
       " ('the', 'full', 'project'),\n",
       " ('full', 'project', 'gutenberg™'),\n",
       " ('project', 'gutenberg™', 'license'),\n",
       " ('of', 'this', 'agreement'),\n",
       " ('daughter', 'of', 'the'),\n",
       " ('the', 'city', 'of'),\n",
       " ('comes', 'to', 'the'),\n",
       " ('the', 'king', 'to'),\n",
       " ('of', 'the', 'gods,'),\n",
       " ('to', 'see', 'the'),\n",
       " ('of', 'the', 'queen'),\n",
       " ('of', 'the', 'goddess'),\n",
       " ('in', 'consequence', 'of'),\n",
       " ('after', 'a', 'short'),\n",
       " ('the', 'king,', 'who'),\n",
       " ('the', 'character', 'of'),\n",
       " ('rama', 'and', 'lakshmana'),\n",
       " ('the', 'bow', 'of'),\n",
       " ('and', 'that', 'the'),\n",
       " ('a', 'son', 'of'),\n",
       " ('the', 'feet', 'of'),\n",
       " ('of', 'rama', 'and'),\n",
       " ('the', 'banks', 'of'),\n",
       " ('banks', 'of', 'the'),\n",
       " ('the', 'work', 'of'),\n",
       " ('the', 'house', 'of'),\n",
       " ('the', 'presence', 'of'),\n",
       " ('to', 'the', 'court'),\n",
       " ('a', 'voice', 'from'),\n",
       " ('and', 'the', 'queen'),\n",
       " ('at', 'this', 'time,'),\n",
       " ('the', 'king', 'observes,'),\n",
       " ('not', 'protected', 'by'),\n",
       " ('the', 'united', 'states'),\n",
       " ('terms', 'of', 'the'),\n",
       " ('a', 'project', 'gutenberg™'),\n",
       " ('gutenberg™', 'electronic', 'work'),\n",
       " ('and', 'in', 'the'),\n",
       " ('of', 'the', 'sage'),\n",
       " ('of', 'her', 'own'),\n",
       " ('to', 'return', 'to'),\n",
       " ('a', 'messenger', 'from'),\n",
       " ('he', 'is', 'at'),\n",
       " ('on', 'account', 'of'),\n",
       " ('the', 'sight', 'of'),\n",
       " ('after', 'a', 'while,'),\n",
       " ('the', 'race', 'of'),\n",
       " ('the', 'name', 'of'),\n",
       " ('carried', 'off', 'by'),\n",
       " ('in', 'love', 'with'),\n",
       " ('in', 'the', 'meantime,'),\n",
       " ('part', 'of', 'the'),\n",
       " ('the', 'goddess', 'of'),\n",
       " ('author', 'of', 'the'),\n",
       " ('the', 'king', 'as'),\n",
       " ('the', 'king', 'enters'),\n",
       " ('her', 'in', 'the'),\n",
       " ('now', 'comes', 'to'),\n",
       " ('that', 'you', 'have'),\n",
       " ('in', 'compliance', 'with'),\n",
       " ('to', 'the', 'queen'),\n",
       " ('of', 'the', \"king's\"),\n",
       " ('when', 'he', 'is'),\n",
       " ('a', 'visit', 'to'),\n",
       " ('the', 'queen,', 'and'),\n",
       " ('king', 'and', 'his'),\n",
       " ('when', 'they', 'are'),\n",
       " ('the', 'sister', 'of'),\n",
       " ('into', 'the', 'hands'),\n",
       " ('the', 'command', 'of'),\n",
       " ('of', 'the', 'great'),\n",
       " ('by', 'the', 'sage'),\n",
       " ('there', 'is', 'no'),\n",
       " ('go', 'to', 'the'),\n",
       " ('of', 'the', 'two'),\n",
       " ('come', 'to', 'the'),\n",
       " ('favour', 'of', 'the'),\n",
       " ('on', 'the', 'banks'),\n",
       " ('the', 'people', 'are'),\n",
       " ('the', 'object', 'of'),\n",
       " ('the', 'burden', 'of'),\n",
       " ('in', 'search', 'of'),\n",
       " ('the', 'death', 'of'),\n",
       " ('at', 'this,', 'the'),\n",
       " ('by', 'the', 'hand'),\n",
       " ('the', 'sons', 'of'),\n",
       " ('voice', 'from', 'heaven,'),\n",
       " ('the', 'queen', 'of'),\n",
       " ('then', 'the', 'king'),\n",
       " ('form', 'of', 'a'),\n",
       " ('the', 'king', 'addresses'),\n",
       " ('how', 'can', 'I'),\n",
       " ('feet', 'of', 'the'),\n",
       " ('I', 'will', 'not'),\n",
       " ('the', 'queen', 'and'),\n",
       " ('the', 'lord', 'of'),\n",
       " ('you', 'need', 'not'),\n",
       " ('that', 'the', 'queen'),\n",
       " ('return', 'to', 'the'),\n",
       " ('is', 'the', 'daughter'),\n",
       " ('of', 'the', 'gods'),\n",
       " ('to', 'be', 'the'),\n",
       " ('the', 'queen', 'now'),\n",
       " ('it', 'is', 'not'),\n",
       " ('the', 'picture', 'and'),\n",
       " ('if', 'you', 'are'),\n",
       " ('was', 'composed', 'by'),\n",
       " ('professor', 'of', 'english'),\n",
       " ('of', 'english', 'literature,'),\n",
       " ('part', 'of', 'this'),\n",
       " ('the', 'phrase', '“project'),\n",
       " ('the', 'chief', 'of'),\n",
       " ('to', 'kill', 'the'),\n",
       " ('to', 'the', 'hermitage,'),\n",
       " ('account', 'of', 'her'),\n",
       " ('he', 'comes', 'to'),\n",
       " ('according', 'to', 'the'),\n",
       " ('the', 'throne', 'of'),\n",
       " ('returns', 'to', 'the'),\n",
       " ('of', 'the', 'sage,'),\n",
       " ('in', 'which', 'she'),\n",
       " ('informs', 'the', 'king'),\n",
       " ('the', 'curse', 'of'),\n",
       " ('the', 'conduct', 'of'),\n",
       " ('the', 'end', 'of'),\n",
       " ('to', 'be', 'his'),\n",
       " ('the', 'touch', 'of'),\n",
       " ('the', 'boy', 'is'),\n",
       " ('a', 'scion', 'of'),\n",
       " ('scion', 'of', 'the'),\n",
       " ('is', 'said', 'to'),\n",
       " ('said', 'to', 'have'),\n",
       " ('who', 'has', 'been'),\n",
       " ('off', 'by', 'a'),\n",
       " ('with', 'her', 'and'),\n",
       " ('one', 'of', 'her'),\n",
       " ('the', 'eyes', 'of'),\n",
       " ('the', 'garden', 'of'),\n",
       " ('the', 'gods,', 'and'),\n",
       " ('by', 'the', 'queen,'),\n",
       " ('and,', 'after', 'a'),\n",
       " ('off', 'in', 'a'),\n",
       " ('of', 'the', 'play,'),\n",
       " ('to', 'him', 'and'),\n",
       " ('to', 'the', 'garden'),\n",
       " ('arrives', 'from', 'the'),\n",
       " ('the', 'terrace', 'of'),\n",
       " ('as', 'he', 'is'),\n",
       " ('his', 'desire', 'for'),\n",
       " ('and', 'comes', 'to'),\n",
       " ('god', 'of', 'the'),\n",
       " ('the', 'queen', 'in'),\n",
       " ('they', 'have', 'seen'),\n",
       " ('of', 'the', 'lotus,'),\n",
       " ('the', 'forest', 'and'),\n",
       " ('the', 'king', 'who'),\n",
       " ('compliance', 'with', 'the'),\n",
       " ('the', 'messenger', 'of'),\n",
       " ('the', 'general', 'of'),\n",
       " ('order', 'of', 'the'),\n",
       " ('of', 'the', 'picture'),\n",
       " ('are', 'about', 'to'),\n",
       " ('to', 'repair', 'to'),\n",
       " ('the', 'foot', 'of'),\n",
       " ('to', 'try', 'the'),\n",
       " ('the', 'effect', 'of'),\n",
       " ('with', 'his', 'friend'),\n",
       " ('visit', 'to', 'the'),\n",
       " ('he', 'has', 'been'),\n",
       " ('be', 'sent', 'to'),\n",
       " ('and', 'the', 'vidushaka'),\n",
       " ('her', 'friend', 'and'),\n",
       " ('when', 'he', 'was'),\n",
       " ('with', 'the', 'news'),\n",
       " ('the', 'king', 'now'),\n",
       " ('charita', 'or', 'the'),\n",
       " ('the', 'life', 'of'),\n",
       " ('the', 'sage', 'to'),\n",
       " ('rama', 'and', 'lakshmana,'),\n",
       " ('consequence', 'of', 'which'),\n",
       " ('between', 'the', 'two'),\n",
       " ('who', 'is', 'the'),\n",
       " ('the', 'palace', 'of'),\n",
       " ('anxious', 'for', 'the'),\n",
       " ('his', 'father', 'to'),\n",
       " ('with', 'his', 'brother'),\n",
       " ('then', 'goes', 'to'),\n",
       " ('goes', 'to', 'the'),\n",
       " ('determines', 'to', 'go'),\n",
       " ('to', 'go', 'to'),\n",
       " ('the', 'residence', 'of'),\n",
       " ('in', 'the', 'way'),\n",
       " ('the', 'appearance', 'of'),\n",
       " ('goes', 'forth', 'to'),\n",
       " ('and', 'his', 'son'),\n",
       " ('to', 'the', 'aid'),\n",
       " ('the', 'dandaka', 'forest,'),\n",
       " ('dandaka', 'forest,', 'the'),\n",
       " ('rama', 'is', 'now'),\n",
       " ('to', 'give', 'up'),\n",
       " ('in', 'accordance', 'with'),\n",
       " ('the', 'care', 'of'),\n",
       " ('care', 'of', 'the'),\n",
       " ('himself', 'to', 'his'),\n",
       " ('the', 'favour', 'of'),\n",
       " ('the', 'mother', 'of'),\n",
       " ('in', 'honour', 'of'),\n",
       " ('is', 'the', 'only'),\n",
       " ('hero', 'of', 'the'),\n",
       " ('the', 'approach', 'of'),\n",
       " ('of', 'the', 'ganges'),\n",
       " ('was', 'about', 'to'),\n",
       " ('the', 'wife', 'of'),\n",
       " ('malatI', 'and', 'madhava'),\n",
       " ('house', 'of', 'the'),\n",
       " ('\"I', 'have', 'heard'),\n",
       " ('temple', 'of', 'the'),\n",
       " ('of', 'the', 'god'),\n",
       " ('the', 'beauty', 'of'),\n",
       " ('beauty', 'of', 'the'),\n",
       " ('a', 'picture', 'and'),\n",
       " ('is', 'the', 'work'),\n",
       " ('in', 'the', 'presence'),\n",
       " ('in', 'this', 'world'),\n",
       " ('her', 'to', 'the'),\n",
       " ('to', 'the', 'house'),\n",
       " ('appearance', 'of', 'the'),\n",
       " ('of', 'his', 'own'),\n",
       " ('him', 'to', 'the'),\n",
       " ('to', 'the', 'place'),\n",
       " ('hands', 'of', 'the'),\n",
       " ('the', 'purpose', 'of'),\n",
       " ('of', 'siva,', 'and'),\n",
       " ('this,', 'the', 'king'),\n",
       " ('it', 'is', 'the'),\n",
       " ('the', 'result', 'of'),\n",
       " ('her', 'by', 'the'),\n",
       " ('brother', 'of', 'ravana,'),\n",
       " ('not', 'to', 'be'),\n",
       " ('with', 'his', 'own'),\n",
       " ('the', 'dead', 'body'),\n",
       " ('overcome', 'with', 'shame'),\n",
       " ('the', 'shrine', 'of'),\n",
       " ('the', 'capital', 'of'),\n",
       " ('of', 'the', 'pandavas,'),\n",
       " ('krishna', 'and', 'his'),\n",
       " ('and', 'do', 'not'),\n",
       " ('the', 'mind', 'of'),\n",
       " ('that', 'he', 'has'),\n",
       " ('the', 'entrance', 'of'),\n",
       " ('he', 'has', 'seen'),\n",
       " ('is', 'slain', 'by'),\n",
       " ('the', 'inner', 'apartments'),\n",
       " ('of', 'which', 'he'),\n",
       " ('when', 'the', 'king'),\n",
       " ('assumes', 'the', 'form'),\n",
       " ('of', 'the', 'world'),\n",
       " ('he', 'will', 'not'),\n",
       " ('to', 'give', 'you'),\n",
       " ('he', 'resolves', 'to'),\n",
       " ('to', 'the', 'terms'),\n",
       " ('the', 'sun', 'is'),\n",
       " ('as', 'soon', 'as'),\n",
       " ('one', 'or', 'two'),\n",
       " ('it', 'to', 'the'),\n",
       " ('lord', 'of', 'the'),\n",
       " ('at', 'once', 'to'),\n",
       " ('he', 'thinks', 'of'),\n",
       " ('addresses', 'her', 'thus:—'),\n",
       " ('you', 'are', 'not'),\n",
       " ('be', 'found', 'in'),\n",
       " ('the', 'grandson', 'of'),\n",
       " ('seen', 'in', 'his'),\n",
       " ('of', 'all', 'the'),\n",
       " ('his', 'daughter', 'to'),\n",
       " ('some', 'of', 'the'),\n",
       " ('the', 'garden,', 'where'),\n",
       " ('this', 'is', 'a'),\n",
       " ('the', 'destruction', 'of'),\n",
       " ('the', 'time', 'of'),\n",
       " ('of', 'the', 'people'),\n",
       " ('of', 'krishna', 'and'),\n",
       " ('of', 'the', 'full'),\n",
       " ('the', 'contrivances', 'of'),\n",
       " ('to', 'the', 'prince'),\n",
       " ('the', 'neck', 'of'),\n",
       " ('neck', 'of', 'the'),\n",
       " ('to', 'do', 'so'),\n",
       " ('the', 'king', 'takes'),\n",
       " ('becomes', 'enamoured', 'of'),\n",
       " ('enamoured', 'of', 'the'),\n",
       " ('consequence', 'of', 'the'),\n",
       " ('son', 'of', 'a'),\n",
       " ('in', 'which', 'the'),\n",
       " ('the', 'husband', 'of'),\n",
       " ('the', 'princesses', 'of'),\n",
       " ('should', 'become', 'the'),\n",
       " ('become', 'the', 'emperor'),\n",
       " ('at', 'the', 'request'),\n",
       " ('the', 'request', 'of'),\n",
       " ('your', 'majesty', 'to'),\n",
       " ('the', 'worship', 'of'),\n",
       " ('shows', 'it', 'to'),\n",
       " ('the', 'queen', 'observes,'),\n",
       " ('the', \"queen's\", 'anger'),\n",
       " ('explains', 'his', 'conduct'),\n",
       " ('the', 'number', 'of'),\n",
       " ('the', 'queen', 'who'),\n",
       " ('it', 'must', 'be'),\n",
       " ('of', 'which', 'the'),\n",
       " ('the', 'sanskrit', 'drama'),\n",
       " ('the', 'hindu', 'drama'),\n",
       " ('with', 'which', 'the'),\n",
       " ('the', 'course', 'of'),\n",
       " ('as', 'in', 'the'),\n",
       " ('the', 'story', 'is'),\n",
       " ('the', 'influence', 'of'),\n",
       " ('is', 'the', 'author'),\n",
       " ('is', 'one', 'of'),\n",
       " ('the', 'beginning', 'of'),\n",
       " ('the', 'chouhan', 'prince'),\n",
       " ('prithu', 'raI', 'of'),\n",
       " ('examiner', 'to', 'the'),\n",
       " ('to', 'the', 'university'),\n",
       " ('the', 'university', 'of'),\n",
       " ('university', 'of', 'calcutta'),\n",
       " ('I', 'have', 'looked'),\n",
       " ('to', 'the', 'capacity'),\n",
       " ('the', 'capacity', 'of'),\n",
       " ('capacity', 'of', 'matriculation'),\n",
       " ('in', 'its', 'issue'),\n",
       " ('its', 'issue', 'of'),\n",
       " ('you', 'do', 'not'),\n",
       " ('complying', 'with', 'the'),\n",
       " ('free', 'distribution', 'of'),\n",
       " ('all', 'the', 'terms'),\n",
       " ('not', 'agree', 'to'),\n",
       " ('this', 'agreement,', 'you'),\n",
       " ('copies', 'of', 'project'),\n",
       " ('electronic', 'works', 'in'),\n",
       " ('the', 'person', 'or'),\n",
       " ('person', 'or', 'entity'),\n",
       " ('as', 'set', 'forth'),\n",
       " ('forth', 'in', 'paragraph'),\n",
       " ('you', 'can', 'do'),\n",
       " ('can', 'do', 'with'),\n",
       " ('if', 'an', 'individual'),\n",
       " ('you', 'are', 'located'),\n",
       " ('the', 'united', 'states,'),\n",
       " ('we', 'do', 'not'),\n",
       " ('project', 'gutenberg™', 'works'),\n",
       " ('laws', 'of', 'the'),\n",
       " ('the', 'laws', 'of'),\n",
       " ('phrase', '“project', 'gutenberg”'),\n",
       " ('the', 'use', 'of'),\n",
       " ('electronic', 'work', 'is'),\n",
       " ('permission', 'of', 'the'),\n",
       " ('of', 'the', 'copyright'),\n",
       " ('any', 'project', 'gutenberg™'),\n",
       " ('donations', 'to', 'the'),\n",
       " ('literary', 'archive', 'foundation,'),\n",
       " ('of', 'replacement', 'or'),\n",
       " ('you', 'received', 'the'),\n",
       " ('received', 'the', 'work'),\n",
       " ('of', 'the', 'lunar'),\n",
       " ('he', 'was', 'the'),\n",
       " ('was', 'the', 'king'),\n",
       " ('hermitage', 'of', 'the'),\n",
       " ('chief', 'of', 'the'),\n",
       " ('was', 'the', 'daughter'),\n",
       " ('sight', 'of', 'the'),\n",
       " ('which', 'the', 'king'),\n",
       " ('companion', 'of', 'the'),\n",
       " ('the', 'king,', 'however,'),\n",
       " ('to', 'the', 'hermitage'),\n",
       " ('as', 'he', 'was'),\n",
       " ('messenger', 'from', 'the'),\n",
       " ('at', 'the', 'city'),\n",
       " ('is', 'at', 'first'),\n",
       " ('he', 'is', 'now'),\n",
       " ('is', 'now', 'at'),\n",
       " ('for', 'the', 'king'),\n",
       " ('and', 'offers', 'to'),\n",
       " ('to', 'see', 'if'),\n",
       " ('obstacle', 'to', 'their'),\n",
       " ('to', 'satisfy', 'the'),\n",
       " ('the', 'king', 'with'),\n",
       " ('king', 'with', 'an'),\n",
       " ('an', 'account', 'of'),\n",
       " ('the', 'consent', 'of'),\n",
       " ('to', 'wait', 'till'),\n",
       " ('on', 'the', 'condition'),\n",
       " ('of', 'the', 'marriage'),\n",
       " ('her', 'to', 'his'),\n",
       " ('when', 'sakuntala', 'is'),\n",
       " ('he', 'is', 'not'),\n",
       " ('pronounces', 'a', 'curse'),\n",
       " ('the', 'sage,', 'who'),\n",
       " ('at', 'the', 'sight'),\n",
       " ('in', 'the', 'seventh'),\n",
       " ('is', 'sent', 'by'),\n",
       " ('sent', 'by', 'her'),\n",
       " ('in', 'the', 'company'),\n",
       " ('the', 'company', 'of'),\n",
       " ('and', 'his', 'two'),\n",
       " ('which', 'she', 'has'),\n",
       " ('she', 'has', 'been'),\n",
       " ('thus', 'expresses', 'his'),\n",
       " ('he', 'calls', 'upon'),\n",
       " ('calls', 'upon', 'the'),\n",
       " ('to', 'give', 'her'),\n",
       " ('and', 'her', 'companions'),\n",
       " ('cause', 'of', 'their'),\n",
       " ('by', 'her', 'husband,'),\n",
       " ('it', 'is', 'lost,'),\n",
       " ('gives', 'vent', 'to'),\n",
       " ('the', 'king', 'would'),\n",
       " ('senseless', 'on', 'the'),\n",
       " ('comes', 'forward', 'and'),\n",
       " ('and', 'goes', 'to'),\n",
       " ('while', 'the', 'king'),\n",
       " ('charged', 'with', 'the'),\n",
       " ('before', 'the', 'king'),\n",
       " ('to', 'be', 'set'),\n",
       " ('recollection', 'of', 'his'),\n",
       " ('of', 'his', 'former'),\n",
       " ('of', 'sakuntala,', 'the'),\n",
       " ('while', 'he', 'is'),\n",
       " ('of', 'the', 'race'),\n",
       " ('he', 'is', 'in'),\n",
       " ('is', 'in', 'a'),\n",
       " ('with', 'whom', 'he'),\n",
       " ('whom', 'he', 'is'),\n",
       " ('the', 'hero', 'and'),\n",
       " ('nymphs', 'of', 'heaven,'),\n",
       " ('his', 'chariot,', 'and'),\n",
       " ('hastens', 'to', 'the'),\n",
       " ('falls', 'in', 'love'),\n",
       " ('love', 'with', 'her'),\n",
       " ('the', 'lovers', 'are'),\n",
       " ('this', 'is', 'no'),\n",
       " ('who', 'is', 'his'),\n",
       " ('his', 'love', 'for'),\n",
       " ('and', 'carries', 'the'),\n",
       " ('the', 'meantime,', 'the'),\n",
       " ('meantime,', 'the', 'king'),\n",
       " ('king', 'is', 'in'),\n",
       " ('with', 'her', 'friend'),\n",
       " ('through', 'the', 'air'),\n",
       " ('and', 'her', 'friend'),\n",
       " ('by', 'a', 'messenger'),\n",
       " ('and', 'the', 'king'),\n",
       " ('leaf', 'on', 'which'),\n",
       " ('but', 'it', 'is'),\n",
       " ('picked', 'up', 'by'),\n",
       " ('who', 'comes', 'to'),\n",
       " ('the', 'king', 'in'),\n",
       " ('goes', 'off', 'in'),\n",
       " ('she', 'had', 'to'),\n",
       " ('the', 'close', 'of'),\n",
       " ('close', 'of', 'the'),\n",
       " ('the', 'days', 'of'),\n",
       " ('me', 'in', 'the'),\n",
       " ('in', 'the', 'conflict'),\n",
       " ('garden', 'of', 'the'),\n",
       " ('the', 'palace', 'as'),\n",
       " ('palace', 'as', 'the'),\n",
       " ('a', 'messenger', 'arrives'),\n",
       " ('desires', 'to', 'see'),\n",
       " ('on', 'the', 'terrace'),\n",
       " ('terrace', 'of', 'the'),\n",
       " ('the', 'moon', 'is'),\n",
       " ('on', 'a', 'sudden,'),\n",
       " ('accompanied', 'by', 'his'),\n",
       " ('conscious', 'of', 'the'),\n",
       " ('she', 'takes', 'the'),\n",
       " ('takes', 'leave', 'and'),\n",
       " ('king', 'enters', 'a'),\n",
       " ('have', 'seen', 'his'),\n",
       " ('that', 'they', 'have'),\n",
       " ('during', 'the', 'night,'),\n",
       " ('he', 'thinks', 'he'),\n",
       " ('at', 'last', 'the'),\n",
       " ('which', 'is', 'now'),\n",
       " ('restored', 'to', 'her'),\n",
       " ('is', 'placed', 'on'),\n",
       " ('and', 'return', 'to'),\n",
       " ('by', 'which', 'the'),\n",
       " ('a', 'female', 'ascetic'),\n",
       " ('a', 'boy', 'with'),\n",
       " ('up', 'in', 'the'),\n",
       " ('in', 'the', 'forest'),\n",
       " ('back', 'to', 'his'),\n",
       " ('the', 'birth', 'of'),\n",
       " ('birth', 'of', 'the'),\n",
       " ('now', 'that', 'you'),\n",
       " ('to', 'announce', 'that'),\n",
       " ('that', 'the', 'god'),\n",
       " ('the', 'nymph', 'is'),\n",
       " ('is', 'inaugurated', 'as'),\n",
       " ('homage', 'to', 'the'),\n",
       " ('in', 'favour', 'of'),\n",
       " ('the', 'founder', 'of'),\n",
       " ('founder', 'of', 'the'),\n",
       " ('of', 'the', 'maurya'),\n",
       " ('belongs', 'to', 'the'),\n",
       " ('as', 'a', 'present'),\n",
       " ('the', 'queen', 'by'),\n",
       " ('her', 'out', 'of'),\n",
       " ('out', 'of', 'the'),\n",
       " ('the', 'picture', 'of'),\n",
       " ('queen', 'for', 'her'),\n",
       " ('whom', 'he', 'has'),\n",
       " ('the', 'latter,', 'and'),\n",
       " ('friend', 'of', 'agnimitra,'),\n",
       " ('the', 'raja', 'of'),\n",
       " ('his', 'vidushaka', 'or'),\n",
       " ('a', 'quarrel', 'between'),\n",
       " ('quarrel', 'between', 'the'),\n",
       " ('is', 'induced', 'to'),\n",
       " ('queen,', 'who', 'has'),\n",
       " ('the', 'hour', 'of'),\n",
       " ('on', 'which', 'the'),\n",
       " ('and', 'the', 'queen,'),\n",
       " ('to', 'put', 'forth'),\n",
       " ('the', 'favourite', 'of'),\n",
       " ('try', 'the', 'effect'),\n",
       " ('and', 'accompanied', 'by'),\n",
       " ('has', 'been', 'watching'),\n",
       " ('now', 'informs', 'the'),\n",
       " ('the', 'king', 'that'),\n",
       " ('malavika', 'has', 'been'),\n",
       " ('pays', 'a', 'visit'),\n",
       " ('is', 'engaged', 'in'),\n",
       " ('and', 'the', 'parivrajaka,'),\n",
       " ('the', 'teeth', 'of'),\n",
       " ('the', 'parivrajaka,', 'with'),\n",
       " ('to', 'be', 'in'),\n",
       " ('the', 'vidushaka', 'is'),\n",
       " ('is', 'in', 'great'),\n",
       " ('one', 'to', 'be'),\n",
       " ('the', 'queen', 'has'),\n",
       " ('sends', 'to', 'the'),\n",
       " ('returned', 'to', 'the'),\n",
       " ('queen,', 'and', 'the'),\n",
       " ('down', 'on', 'the'),\n",
       " ('again', 'disturbed', 'by'),\n",
       " ('is', 'disturbed', 'by'),\n",
       " ('and', 'the', 'raja'),\n",
       " ('hearing', 'that', 'the'),\n",
       " ('the', 'friend', 'of'),\n",
       " ('to', 'effect', 'his'),\n",
       " ('and', 'the', 'young'),\n",
       " ('herself', 'in', 'the'),\n",
       " ('on', 'their', 'way'),\n",
       " ('their', 'way', 'to'),\n",
       " ('the', 'vindhya', 'mountains,'),\n",
       " ('attacked', 'by', 'the'),\n",
       " ('and', 'decorated', 'with'),\n",
       " ('to', 'be', 'an'),\n",
       " ('a', 'letter', 'arrives'),\n",
       " ('letter', 'arrives', 'from'),\n",
       " ('should', 'have', 'a'),\n",
       " ('attended', 'only', 'by'),\n",
       " ('and', 'after', 'a'),\n",
       " ('the', 'news', 'of'),\n",
       " ('life', 'of', 'the'),\n",
       " ('dasaratha,', 'the', 'king'),\n",
       " ('janaka,', 'the', 'king'),\n",
       " ('king', 'of', 'mithila,'),\n",
       " ('enters', 'the', 'hermitage'),\n",
       " ('of', 'the', 'kausikI'),\n",
       " ('the', 'two', 'youths'),\n",
       " ('and', 'lakshmana,', 'and'),\n",
       " ('king', 'of', 'lanka,'),\n",
       " ('to', 'demand', 'sita'),\n",
       " ('and', 'mother', 'of'),\n",
       " ('is', 'killed', 'and'),\n",
       " ('killed', 'and', 'the'),\n",
       " ('to', 'the', 'minister'),\n",
       " ('the', 'minister', 'of'),\n",
       " ('the', 'minister', 'and'),\n",
       " ('some', 'of', 'his'),\n",
       " ('takes', 'advantage', 'of'),\n",
       " ('when', 'he', 'hears'),\n",
       " ('he', 'hears', 'of'),\n",
       " ('the', 'insulter', 'of'),\n",
       " ('to', 'his', 'own'),\n",
       " ('throughout', 'the', 'world'),\n",
       " ('the', 'world', 'the'),\n",
       " ('and', 'the', 'son'),\n",
       " ('the', 'fires', 'of'),\n",
       " ('the', 'two', 'ramas'),\n",
       " ('by', 'the', 'arrival'),\n",
       " ('and', 'then', 'to'),\n",
       " ('with', 'his', 'father'),\n",
       " ('his', 'maternal', 'uncle'),\n",
       " ('are', 'full', 'of'),\n",
       " ('on', 'which', 'his'),\n",
       " ('rama', 'with', 'his'),\n",
       " ('set', 'off', 'to'),\n",
       " ('and', 'sampati,', 'the'),\n",
       " ('perches', 'on', 'the'),\n",
       " ('in', 'pursuit', 'of'),\n",
       " ('pursuit', 'of', 'the'),\n",
       " ('a', 'holy', 'seer'),\n",
       " ('he', 'goes', 'off'),\n",
       " ('goes', 'off', 'to'),\n",
       " ('and', 'returns', 'with'),\n",
       " ('learning', 'from', 'the'),\n",
       " ('away', 'the', 'skeleton'),\n",
       " ('the', 'skeleton', 'of'),\n",
       " ('and', 'all', 'the'),\n",
       " ('and', 'his', 'own'),\n",
       " ('to', 'him,', 'to'),\n",
       " ('the', 'town', 'of'),\n",
       " ('by', 'means', 'of'),\n",
       " ('tells', 'him', 'of'),\n",
       " ('him', 'of', 'the'),\n",
       " ('ravana', 'to', 'restore'),\n",
       " ('at', 'the', 'feet'),\n",
       " ('ravana', 'goes', 'forth'),\n",
       " ('forth', 'to', 'the'),\n",
       " ('then', 'come', 'to'),\n",
       " ('all', 'the', 'chiefs'),\n",
       " ('are', 'reduced', 'to'),\n",
       " ('and', 'then', 'goes'),\n",
       " ('or', 'the', 'sun'),\n",
       " ('is', 'now', 'crowned'),\n",
       " ('then', 'enters', 'the'),\n",
       " ('which', 'was', 'once'),\n",
       " ('was', 'once', 'wrested'),\n",
       " ('of', 'the', 'party'),\n",
       " ('the', 'malaya', 'mountain,'),\n",
       " ('malaya', 'mountain,', 'the'),\n",
       " ('a', 'life', 'of'),\n",
       " ('with', 'his', 'wife'),\n",
       " ('the', 'love', 'of'),\n",
       " ('of', 'a', 'few'),\n",
       " ('then', 'the', 'sage'),\n",
       " ('to', 'leave', 'the'),\n",
       " ('in', 'his', \"people's\"),\n",
       " ('am', 'ready', 'to'),\n",
       " ('accordance', 'with', 'this'),\n",
       " ('rama', 'and', 'sita'),\n",
       " ('parts', 'of', 'the'),\n",
       " ('asks', 'her', 'to'),\n",
       " ('of', 'her', 'husband'),\n",
       " ('the', 'inner', 'apartments,'),\n",
       " ('about', 'her', 'and'),\n",
       " ('go', 'out', 'to'),\n",
       " ('then', 'enters', 'and'),\n",
       " ('to', 'mount', 'the'),\n",
       " ('which', 'is', 'to'),\n",
       " ('valmiki,', 'the', 'author'),\n",
       " ('of', 'the', 'ramayana,'),\n",
       " ('the', 'boys', 'have'),\n",
       " ('is', 'the', 'cause'),\n",
       " ('informs', 'him', 'that'),\n",
       " ('but', 'death', 'by'),\n",
       " ('appears', 'as', 'a'),\n",
       " ('through', 'the', 'favour'),\n",
       " ('but', 'at', 'last'),\n",
       " ('on', 'the', 'advice'),\n",
       " ('the', 'advice', 'of'),\n",
       " ('advice', 'of', 'his'),\n",
       " ('rama', 'and', 'other'),\n",
       " ('honour', 'of', 'the'),\n",
       " ('the', 'features', 'of'),\n",
       " ('general', 'of', 'the'),\n",
       " ('surprised', 'at', 'the'),\n",
       " ('and', 'fight', 'with'),\n",
       " ('in', 'which', 'he'),\n",
       " ('an', 'end', 'to'),\n",
       " ('respects', 'to', 'the'),\n",
       " ('the', 'hero', 'of'),\n",
       " ('he', 'disregarded', 'the'),\n",
       " ('to', 'which', 'she'),\n",
       " ('is', 'restored', 'to'),\n",
       " ('the', 'people', 'of'),\n",
       " ('and', 'in', 'my'),\n",
       " ('minister', 'to', 'the'),\n",
       " ('he', 'had', 'a'),\n",
       " ('now', 'arrived', 'at'),\n",
       " ('the', 'confluence', 'of'),\n",
       " ('confluence', 'of', 'the'),\n",
       " ('there', 'lived', 'in'),\n",
       " ('lived', 'in', 'padmavati,'),\n",
       " ('the', 'king', 'by'),\n",
       " ('that', 'they', 'should'),\n",
       " ('the', 'two', 'friends'),\n",
       " ('the', 'young', 'people'),\n",
       " ('the', 'promise', 'of'),\n",
       " ('and', 'with', 'the'),\n",
       " ('with', 'the', 'aid'),\n",
       " ('the', 'pupil', 'says,'),\n",
       " ('pupil', 'says,', '\"I'),\n",
       " ('of', 'the', 'minister'),\n",
       " ('says,', '\"I', 'have'),\n",
       " ('that', 'malatI', 'has'),\n",
       " ('a', 'picture', 'of'),\n",
       " ('with', 'the', 'object'),\n",
       " ('to', 'join', 'the'),\n",
       " ('with', 'a', 'view'),\n",
       " ('thanks', 'for', 'your'),\n",
       " ('of', 'the', 'dreadful'),\n",
       " ('observes,', '\"I', 'have'),\n",
       " ('to', 'the', 'temple'),\n",
       " ('does', 'not', 'melt'),\n",
       " ('it', 'from', 'my'),\n",
       " ('the', 'lunar', 'rays'),\n",
       " ('requested', 'by', 'lavangika,'),\n",
       " ('do', 'not', 'allow'),\n",
       " ('whom', 'they', 'have'),\n",
       " ('the', 'likeness', 'of'),\n",
       " ('no', 'one', 'can'),\n",
       " ('the', 'full', 'moon'),\n",
       " ('dispose', 'of', 'your'),\n",
       " ('your', 'own', 'daughter'),\n",
       " ('can', 'dispose', 'of'),\n",
       " ('the', 'intelligence', 'reaches'),\n",
       " ('intelligence', 'reaches', 'the'),\n",
       " ('a', 'son', 'has'),\n",
       " ('has', 'been', 'sent'),\n",
       " ('been', 'sent', 'to'),\n",
       " ('choice', 'of', 'a'),\n",
       " ('the', 'contrivance', 'of'),\n",
       " ('interview', 'between', 'the'),\n",
       " ('place', 'in', 'the'),\n",
       " ('in', 'the', 'public'),\n",
       " ('is', 'to', 'be'),\n",
       " ('has', 'escaped', 'from'),\n",
       " ('happens', 'to', 'be'),\n",
       " ('both', 'rush', 'to'),\n",
       " ('the', 'two', 'couples'),\n",
       " ('two', 'couples', 'are'),\n",
       " ('of', 'malatI', 'with'),\n",
       " ('with', 'grief', 'and'),\n",
       " ('going', 'to', 'the'),\n",
       " ('to', 'the', 'temple,'),\n",
       " ('the', 'attention', 'of'),\n",
       " ('voices', 'are', 'heard'),\n",
       " ('priestess', 'kamandaki,', 'who'),\n",
       " ('the', 'union', 'of'),\n",
       " ('puts', 'on', 'the'),\n",
       " ('in', 'the', 'inner'),\n",
       " ('the', 'place', 'where'),\n",
       " ('are', 'allowed', 'to'),\n",
       " ('in', 'revenge', 'for'),\n",
       " ('death', 'of', 'her'),\n",
       " ('they', 'are', 'happily'),\n",
       " ('who', 'has', 'acquired'),\n",
       " ('mahanataka,', 'or', 'the'),\n",
       " ('or', 'the', 'great'),\n",
       " ('for', 'the', 'purpose'),\n",
       " ('to', 'try', 'his'),\n",
       " ('the', 'kshatriya', 'hero'),\n",
       " ('is', 'married', 'to'),\n",
       " ('is', 'the', 'result'),\n",
       " ('result', 'of', 'the'),\n",
       " ('father', 'of', 'the'),\n",
       " ('of', 'the', 'monkeys,'),\n",
       " ('the', 'monkeys,', 'and'),\n",
       " ('vibhishana,', 'the', 'brother'),\n",
       " ('a', 'bridge', 'is'),\n",
       " ('bridge', 'is', 'built'),\n",
       " ('is', 'built', 'over'),\n",
       " ('built', 'over', 'the'),\n",
       " ('angada,', 'the', 'son'),\n",
       " ('son', 'of', 'bali,'),\n",
       " ('to', 'persuade', 'ravana'),\n",
       " ('persuade', 'ravana', 'to'),\n",
       " ('and', 'for', 'my'),\n",
       " ('the', 'charms', 'of'),\n",
       " ('advance', 'of', 'the'),\n",
       " ('moral', 'and', 'political'),\n",
       " ('is', 'not', 'to'),\n",
       " ('to', 'assume', 'the'),\n",
       " ('by', 'a', 'voice'),\n",
       " ('from', 'heaven,', 'which'),\n",
       " ('that', 'she', 'will'),\n",
       " ('dead', 'body', 'of'),\n",
       " ('body', 'of', 'her'),\n",
       " ('to', 'lanka,', 'and'),\n",
       " ('comes', 'forth', 'to'),\n",
       " ('is', 'killed', 'by'),\n",
       " ('son', 'of', 'ravana,'),\n",
       " ('him', 'by', 'brahma,'),\n",
       " ('on', 'the', 'ground,'),\n",
       " ('to', 'obtain', 'a'),\n",
       " ('hands', 'of', 'his'),\n",
       " ('in', 'his', 'sleep,'),\n",
       " ('his', 'sleep,', 'and'),\n",
       " ('the', 'true', 'spirit'),\n",
       " ('true', 'spirit', 'of'),\n",
       " ('the', 'progress', 'of'),\n",
       " ('and', 'death', 'is'),\n",
       " ('a', 'matter', 'of'),\n",
       " ('passing', 'through', 'the'),\n",
       " ('it', 'is', 'now'),\n",
       " ('on', 'the', 'throne'),\n",
       " ('after', 'some', 'time,'),\n",
       " ('he', 'orders', 'the'),\n",
       " ('aid', 'of', 'his'),\n",
       " ('one', 'of', 'his'),\n",
       " ('him', 'in', 'his'),\n",
       " ('for', 'his', 'master,'),\n",
       " ('at', 'the', 'same'),\n",
       " ('is', 'highly', 'indignant'),\n",
       " ('to', 'his', \"master's\"),\n",
       " ('then', 'appears', 'and'),\n",
       " ('destruction', 'of', 'the'),\n",
       " ('calls', 'for', 'his'),\n",
       " ('presents', 'him', 'with'),\n",
       " ('go', 'forth', 'to'),\n",
       " ('in', 'the', 'end,'),\n",
       " ('a', 'letter', 'from'),\n",
       " ('disguised', 'as', 'a'),\n",
       " ('to', 'meet', 'the'),\n",
       " ('to', 'his', 'aid,'),\n",
       " ('his', 'aid,', 'and'),\n",
       " ('of', 'ravana,', 'and'),\n",
       " ('his', 'wife', 'and'),\n",
       " ('on', 'the', 'way'),\n",
       " ('arrival', 'of', 'the'),\n",
       " ('the', 'braid', 'of'),\n",
       " ('wife', 'of', 'the'),\n",
       " ('dragged', 'by', 'the'),\n",
       " ('into', 'the', 'public'),\n",
       " ('the', 'kaurava', 'princes,'),\n",
       " ('to', 'prepare', 'for'),\n",
       " ('he', 'is', 'about'),\n",
       " ('disposed', 'to', 'be'),\n",
       " ('and', 'flowers', 'to'),\n",
       " ('flowers', 'to', 'the'),\n",
       " ('to', 'avert', 'the'),\n",
       " ('disturbed', 'by', 'a'),\n",
       " ('himself', 'in', 'the'),\n",
       " ('excited', 'by', 'the'),\n",
       " ('of', 'his', 'wife,'),\n",
       " ('of', 'the', 'court'),\n",
       " ('up', 'to', 'the'),\n",
       " ('is', 'seized', 'by'),\n",
       " ('recommends', 'him', 'to'),\n",
       " ('command', 'of', 'the'),\n",
       " ('mind', 'of', 'the'),\n",
       " ('himself', 'on', 'the'),\n",
       " ('has', 'at', 'last'),\n",
       " ('and', 'is', 'about'),\n",
       " ('about', 'to', 'sacrifice'),\n",
       " ('they', 'go', 'off'),\n",
       " ('is', 'prevented', 'by'),\n",
       " ('prevented', 'by', 'the'),\n",
       " ('endeavour', 'to', 'prevail'),\n",
       " ('entrance', 'of', 'the'),\n",
       " ('to', 'go', 'and'),\n",
       " ('slain', 'by', 'the'),\n",
       " ('satisfied', 'with', 'the'),\n",
       " ('to', 'prevail', 'on'),\n",
       " ('by', 'the', 'hair,'),\n",
       " ('or', 'the', 'offended'),\n",
       " ('a', 'vision', 'of'),\n",
       " ('king', 'enters', 'the'),\n",
       " ('of', 'his', 'palace'),\n",
       " ('eyes', 'are', 'red'),\n",
       " ('the', 'king,', 'on'),\n",
       " ('my', 'dear', 'queen!'),\n",
       " ('queen!', 'do', 'not'),\n",
       " ('do', 'not', 'be'),\n",
       " ('you', 'have', 'no'),\n",
       " ('and', 'asks', 'her'),\n",
       " ('arts', 'of', 'creation,'),\n",
       " ('of', 'creation,', 'preservation'),\n",
       " ('creation,', 'preservation', 'and'),\n",
       " ('appears', 'before', 'the'),\n",
       " ('him,', 'but', 'in'),\n",
       " ('king', 'addresses', 'his'),\n",
       " ('thus,', '\"it', 'is'),\n",
       " ('is', 'the', 'duty'),\n",
       " ('the', 'duty', 'of'),\n",
       " ('the', 'forest', 'of'),\n",
       " ('forest', 'of', 'meditation'),\n",
       " ('of', 'meditation', 'and'),\n",
       " ('the', 'king', 'thinks,'),\n",
       " ('world', 'is', 'the'),\n",
       " ('save', 'us!', 'save'),\n",
       " ('king', 'is', 'at'),\n",
       " ('is', 'at', 'a'),\n",
       " ('are', 'the', 'three'),\n",
       " ('I', 'will', 'burn'),\n",
       " ('will', 'burn', 'you'),\n",
       " ('a', 'plantain', 'tree'),\n",
       " ('will', 'not', 'be'),\n",
       " ('\"my', 'lord', 'kausika!'),\n",
       " ('for', 'the', 'sake'),\n",
       " ('the', 'sake', 'of'),\n",
       " ('this,', 'the', 'sage'),\n",
       " ('the', 'sage', 'becomes'),\n",
       " ('becomes', 'still', 'more'),\n",
       " ('king', 'replies,', '\"o'),\n",
       " ('replies,', '\"o', 'god!'),\n",
       " ('I', 'am', 'prepared'),\n",
       " ('am', 'prepared', 'to'),\n",
       " ('prepared', 'to', 'give'),\n",
       " ('world', 'with', 'all'),\n",
       " ('with', 'all', 'its'),\n",
       " ('I', 'accept', 'your'),\n",
       " ('you', 'have', 'made'),\n",
       " ('have', 'made', 'a'),\n",
       " ('of', 'one', 'thousand'),\n",
       " ('as', 'you', 'have'),\n",
       " ('collect', 'the', 'money'),\n",
       " ('him', 'that', 'it'),\n",
       " ('it', 'is', 'said'),\n",
       " ('the', 'queen', 'saibya'),\n",
       " ('of', 'the', 'last'),\n",
       " ('the', 'sage', 'whirls'),\n",
       " ('sage', 'whirls', 'his'),\n",
       " ('whirls', 'his', 'eyes'),\n",
       " ('his', 'eyes', 'and'),\n",
       " ('where', 'is', 'my'),\n",
       " ('at', 'this', 'the'),\n",
       " ('after', 'many', 'entreaties,'),\n",
       " ('many', 'entreaties,', 'the'),\n",
       " ('no', 'one', 'responds'),\n",
       " ('one', 'responds', 'to'),\n",
       " ('responds', 'to', 'his'),\n",
       " ('this', 'time,', 'a'),\n",
       " ('a', 'female', 'slave'),\n",
       " ('female', 'slave', 'is'),\n",
       " ('slave', 'is', 'for'),\n",
       " ('is', 'for', 'sale'),\n",
       " ('consents', 'to', 'the'),\n",
       " ('in', 'a', 'dream'),\n",
       " ('that', 'such', 'a'),\n",
       " ('can', 'no', 'longer'),\n",
       " ('by', 'the', 'sale'),\n",
       " ('the', 'sale', 'of'),\n",
       " ('sale', 'of', 'my'),\n",
       " ('him', 'in', 'the'),\n",
       " ('the', 'vision', 'of'),\n",
       " ('on', 'the', 'ground'),\n",
       " ('virtue', 'assumes', 'the'),\n",
       " ('of', 'a', 'chandal'),\n",
       " ('a', 'chandal', 'and'),\n",
       " ('I', 'shall', 'be'),\n",
       " ('himself', 'to', 'the'),\n",
       " ...]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams_to_learn = ngrams_up_to_20[1].copy()\n",
    "[sent[0] for sent in my_filter(trigrams_to_learn.most_common())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f4d1b782-c3d7-4269-a22d-b4bbd62615fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_example.extend([[word_to_index[w] for w in sent[0][:-1]] for sent in my_filter(trigrams_to_learn.most_common())\n",
    "               if all([w in word_to_index for w in sent[0]])])\n",
    "y_train_example.extend([[word_to_index[w] for w in sent[0][1:]] for sent in my_filter(trigrams_to_learn.most_common())\n",
    "               if all([w in word_to_index for w in sent[0]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "60e11538-0f61-4717-ad74-d99dcfa41681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4434, 4434)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_example), len(y_train_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "08a7d6c2-c737-4679-8341-a2d04c48268f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[10], [10], [1152], [90], [63], [28], [921], [5], [271], [1480]],\n",
       " [[1151], [75], [4], [7], [143], [34], [0], [2055], [174], [6]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_example[1575:1585], y_train_example[1575:1585]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b8354093-de99-450b-af66-102e970d15b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_to_learn = ngrams_up_to_20[0]\n",
    "X_train_2 = [[word_to_index[sent[0][0]]] for sent in my_filter(bigrams_to_learn.most_common())\n",
    "                  if sent[0][0] in word_to_index and sent[0][1] in word_to_index]\n",
    "y_train_2 = [[word_to_index[sent[0][1]]] for sent in my_filter(bigrams_to_learn.most_common())\n",
    "                  if sent[0][0] in word_to_index and sent[0][1] in word_to_index]\n",
    "X_train_2, y_train_2 = fisher_yates(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6ae134e8-9b68-4e6d-8944-90ffca863041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3127, 3127)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_2), len(y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "706f8ebd-77ec-45c9-8c46-9147a8cc7b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[38], [42], [0], [2173], [91], [8], [2973], [7], [144], [2192]],\n",
       " [[0], [25], [719], [6], [89], [10], [62], [205], [40], [6]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2[0:10], y_train_2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "06108e64-050f-4515-9b64-511ec2f5ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.extend(X_train_2)\n",
    "y_train.extend(y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4e46f105-f2c5-428c-ad18-30b65610b611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4349, 4349)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ec8e7007-2094-41b1-961a-6ac4c9889c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([990], [0]),\n",
       " ([391], [12]),\n",
       " ([8], [161]),\n",
       " ([7], [1074]),\n",
       " ([2], [75]),\n",
       " ([1617], [6]),\n",
       " ([263], [5]),\n",
       " ([6], [17]),\n",
       " ([2], [0]),\n",
       " ([7], [183])]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list(zip(X_train, y_train)), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8bd1304b-095c-4c48-8d73-b12e870d0ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('the', 'king', 'of'), 37),\n",
       " (('*', '*', '*'), 33),\n",
       " (('the', 'king', 'and'), 20),\n",
       " (('project', 'gutenberg™', 'electronic'), 18),\n",
       " (('the', 'son', 'of'), 17),\n",
       " (('the', 'project', 'gutenberg'), 17),\n",
       " (('the', 'king', 'is'), 15),\n",
       " (('the', 'daughter', 'of'), 14),\n",
       " (('the', 'project', 'gutenberg™'), 13),\n",
       " (('project', 'gutenberg', 'literary'), 13)]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_to_learn = ngrams_up_to_20[1]\n",
    "ngrams_to_learn.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1776a631-e250-4471-bb73-84f9dcdf9100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'king', 'of'),\n",
       " ('*', '*', '*'),\n",
       " ('the', 'king', 'and'),\n",
       " ('project', 'gutenberg™', 'electronic'),\n",
       " ('the', 'son', 'of'),\n",
       " ('the', 'project', 'gutenberg'),\n",
       " ('the', 'king', 'is'),\n",
       " ('the', 'daughter', 'of'),\n",
       " ('the', 'project', 'gutenberg™'),\n",
       " ('project', 'gutenberg', 'literary')]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sent[0] for sent in my_filter(ngrams_to_learn.most_common(10))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "263cc313-952c-4fe8-bae2-fb7e35325a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[334, 4], [0, 141], [6, 689], [46, 1256], [8, 488]],\n",
       " [[4, 0], [141, 4], [689, 0], [1256, 25], [488, 9]],\n",
       " 1307,\n",
       " 1307)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2 = [[word_to_index[w] for w in sent[0][:-1]] for sent in my_filter(ngrams_to_learn.most_common())\n",
    "               if all([w in word_to_index for w in sent[0]])]\n",
    "y_train_2 = [[word_to_index[w] for w in sent[0][1:]] for sent in my_filter(ngrams_to_learn.most_common())\n",
    "               if all([w in word_to_index for w in sent[0]])]\n",
    "X_train_2, y_train_2 = fisher_yates(X_train_2, y_train_2)\n",
    "X_train_2[0:5], y_train_2[0:5], len(X_train_2), len(y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a92730d4-eede-4e16-932d-beba8937318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_filter(ngrams):\n",
    "    return filter(remove_periods, ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3bbefcff-8e91-4438-b9fa-13556691574c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1115, 13], [0, 1361], [12, 75], [16, 24], [5, 40]],\n",
       " [[13, 11], [1361, 4], [75, 25], [24, 45], [40, 0]],\n",
       " 2000,\n",
       " 2000)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2 = [[word_to_index[w] for w in sent[0][:-1]] for sent in my_filter(ngrams_to_learn.most_common())\n",
    "               if all([w in word_to_index for w in sent[0]])]\n",
    "y_train_2 = [[word_to_index[w] for w in sent[0][1:]] for sent in my_filter(ngrams_to_learn.most_common())\n",
    "               if all([w in word_to_index for w in sent[0]])]\n",
    "X_train_2 = X_train_2[:2000]\n",
    "y_train_2 = y_train_2[:2000]\n",
    "X_train_2, y_train_2 = fisher_yates(X_train_2, y_train_2)\n",
    "X_train_2[0:5], y_train_2[0:5], len(X_train_2), len(y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "66742c41-efb4-4f47-a735-4946923ac43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 15], [48, 48], [0, 15], [55, 72], [0, 60]] [[15, 4], [48, 48], [15, 5], [72, 158], [60, 4]] 20933 20933\n"
     ]
    }
   ],
   "source": [
    "ngrams_to_learn = ngrams_up_to_20[1]\n",
    "X_train_2 = [[word_to_index[w] for w in sent[0][:-1]] for sent in my_filter(ngrams_to_learn.most_common())\n",
    "               if all([w in word_to_index for w in sent[0]])]\n",
    "y_train_2 = [[word_to_index[w] for w in sent[0][1:]] for sent in my_filter(ngrams_to_learn.most_common())\n",
    "               if all([w in word_to_index for w in sent[0]])]\n",
    "print(X_train_2[0:5], y_train_2[0:5], len(X_train_2), len(y_train_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2a9b20b8-849b-46d1-983a-4620ea0e2dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['SENTENCE_END']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "8df8eced-1a4a-436d-b353-ab8914936fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_eos(trigram):\n",
    "    if trigram[1] == word_to_index['SENTENCE_END']:\n",
    "          return True  \n",
    "    return False\n",
    "\n",
    "trigrams_eos = list(filter(check_eos, y_train_2))\n",
    "len(trigrams_eos), trigrams_eos[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8baac84d-cdce-455a-a8ad-5fc29df3939a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 18/18 [00:01<00:00, 17.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(1, len(ngrams_up_to_20))):\n",
    "    ngrams_to_learn = ngrams_up_to_20[i]\n",
    "    X_train_2 = [[word_to_index[w] for w in sent[0][:-1]] for sent in my_filter(ngrams_to_learn.most_common())\n",
    "                   if all([w in word_to_index for w in sent[0]])]\n",
    "    y_train_2 = [[word_to_index[w] for w in sent[0][1:]] for sent in my_filter(ngrams_to_learn.most_common())\n",
    "                   if all([w in word_to_index for w in sent[0]])]\n",
    "    X_train_2 = X_train_2[:2000]\n",
    "    y_train_2 = y_train_2[:2000]\n",
    "    X_train_2, y_train_2 = fisher_yates(X_train_2, y_train_2)\n",
    "    X_train.extend(X_train_2)\n",
    "    y_train.extend(y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "93168f42-e368-4baf-a397-46126fdab673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34425, 34425)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a8add015-ede7-4230-a0d5-542f9c480208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([1971, 9, 0, 158, 74, 8, 448, 5, 1705, 6, 24, 702, 3034, 319], [9, 0, 158, 74, 8, 448, 5, 1705, 6, 24, 702, 3034, 319, 4]), ([3967, 6, 0, 532, 4, 1002, 5, 91, 99, 0, 124, 4], [6, 0, 532, 4, 1002, 5, 91, 99, 0, 124, 4, 85]), ([1360, 16, 24, 67, 77, 12, 130, 55, 72, 158, 113, 264, 149, 1953, 12, 0, 199], [16, 24, 67, 77, 12, 130, 55, 72, 158, 113, 264, 149, 1953, 12, 0, 199, 162]), ([0], [743]), ([54, 14, 876, 877, 1111, 4], [14, 876, 877, 1111, 4, 0]), ([1891], [4]), ([8, 1302, 146], [1302, 146, 0]), ([5, 592, 2520, 6, 2521, 47, 1184, 2522, 13], [592, 2520, 6, 2521, 47, 1184, 2522, 13, 1763]), ([7, 2040, 4, 908, 99, 52], [2040, 4, 908, 99, 52, 30]), ([12, 55, 72, 158, 113, 64, 24, 663, 0, 162, 4, 26, 243, 5], [55, 72, 158, 113, 64, 24, 663, 0, 162, 4, 26, 243, 5, 333])]\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(list(zip(X_train, y_train)), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b6e2d57e-a84a-47d8-b3f6-af06e11a1381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2145"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "878363ce-60c3-461f-940c-6e330d3ff404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SENTENCE_START',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'names',\n",
       " 'of',\n",
       " 'vishnu',\n",
       " 'is',\n",
       " 'purushottama',\n",
       " 'SENTENCE_END']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "42a0b33c-65b9-4aec-b015-5c83758fe840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 56, 4, 0, 2062, 4, 747, 8, 2063, 3]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[word_to_index[w] for w in sent] for sent in tokenized_sentences if all([w in word_to_index for w in sent])][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8c95e2f2-c60f-4467-b0e9-cd2ffeca83bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full_sentences = [[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences\n",
    "                         if all([w in word_to_index for w in sent])]\n",
    "y_train_full_sentences = [[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences\n",
    "                         if all([w in word_to_index for w in sent])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ee518273-8290-43cd-a2a3-631cd56e9900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 132, 17, 0, 715, 442], [2, 9, 1105, 319, 1, 90, 40, 7, 557, 15, 4, 0, 1106, 1107, 13, 320, 3056], [2, 14, 40, 0, 15, 4, 716], [2, 14, 211, 91, 79, 3057, 5, 9, 0, 1108, 4, 7, 874, 92, 387, 0, 224, 4, 0, 86, 1109, 1, 0, 244, 4, 0, 875, 1, 110, 78, 3058, 321, 34, 29, 6, 717, 0, 874], [2, 0, 15, 613, 3059, 5, 40, 1110, 558, 54, 14, 876, 877, 1111, 4, 0, 875, 3060, 0, 388, 3061]] [[132, 17, 0, 715, 442, 3], [9, 1105, 319, 1, 90, 40, 7, 557, 15, 4, 0, 1106, 1107, 13, 320, 3056, 3], [14, 40, 0, 15, 4, 716, 3], [14, 211, 91, 79, 3057, 5, 9, 0, 1108, 4, 7, 874, 92, 387, 0, 224, 4, 0, 86, 1109, 1, 0, 244, 4, 0, 875, 1, 110, 78, 3058, 321, 34, 29, 6, 717, 0, 874, 3], [0, 15, 613, 3059, 5, 40, 1110, 558, 54, 14, 876, 877, 1111, 4, 0, 875, 3060, 0, 388, 3061, 3]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_full_sentences[0:5], y_train_full_sentences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "14844205-761c-4e87-ac6b-16e5258f90bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['SENTENCE_START', '1e', 'SENTENCE_END'], ['SENTENCE_START', 'vibhishana', 'is', 'now', 'crowned', 'king', 'of', 'lanka', 'SENTENCE_END'], ['everything', 'about', 'the', 'matrimonial', 'promise', 'SENTENCE_END'], ['exclaims', ',', '``', 'where', 'is', 'she', 'SENTENCE_END'], ['alone', 'with', 'his', 'jester', 'SENTENCE_END'], ['SENTENCE_START', 'the', 'minister', 'is', 'glad', 'that', 'his', 'aims', 'are', 'fulfilled', 'SENTENCE_END'], ['who', ',', 'in', 'consideration', 'of', 'ganadasa', \"'s\", 'being', 'patronised', 'by', 'the', 'queen', ',', 'refers', 'the', 'dispute', 'to', 'her', 'SENTENCE_END'], ['the', 'dust', 'from', 'under', 'them', 'upon', 'his', 'head', 'SENTENCE_END'], ['the', 'performance', 'alone', ',', 'summons', 'the', 'queen', 'who', 'arrives', 'soon', 'SENTENCE_END'], ['yet', 'such', 'is', 'their', 'native', 'tenderness', 'that', 'they', 'can', 'not', 'assume', 'a', 'harsh', 'expression', 'SENTENCE_END']]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "last_n_words = []\n",
    "for i in range(3, 20):\n",
    "    tokenized_sentences_400 = random.sample(list(tokenized_sentences), 400)\n",
    "    for s in tokenized_sentences_400:\n",
    "        last_n_words.append(s[::-1][:i][::-1])\n",
    "\n",
    "print(random.sample(last_n_words, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "01a59244-3db4-44fe-a628-5b4cdfc166ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6800"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(last_n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a7da0b3a-8609-46a0-86a7-bc3272c8cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_eos = [[word_to_index[w] for w in sent[:-1]] for sent in last_n_words\n",
    "                         if all([w in word_to_index for w in sent])]\n",
    "y_train_eos = [[word_to_index[w] for w in sent[1:]] for sent in last_n_words\n",
    "                         if all([w in word_to_index for w in sent])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "44eeaa1b-67bd-4cd6-9d7c-0825b61bef24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6800, 6800)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_eos), len(y_train_eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7df79743-14b5-4be0-b9ff-38781b3ebb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.extend(X_train_eos)\n",
    "y_train.extend(y_train_eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f2caab67-ed04-4ecf-b876-39fa28447991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41225, 41225)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "4bf816ad-abec-42c9-b355-142a52f097a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/X_train_Mahabharat.pkl', 'wb') as file:\n",
    "    pickle.dump(X_train, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e284828f-1857-47c7-b518-9a55105cb9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/y_train_Mahabharat.pkl', 'wb') as file:\n",
    "    pickle.dump(y_train, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ef7b8628-b5f3-4bc9-83cb-fdc26e389e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tokenized_sentences_Mahabharat.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenized_sentences, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "67593f95-b233-45e6-a3df-98f3f09bb565",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/word_to_index_Mahabharat.pkl', 'wb') as file:\n",
    "    pickle.dump(word_to_index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "86caf2f1-34d3-4a1b-942a-92a11fd8cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/index_to_word_Mahabharat.pkl', 'wb') as file:\n",
    "    pickle.dump(index_to_word, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "56b9b8a0-a465-43c9-beee-154da19e896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = np.asarray(X_train,dtype=object)\n",
    "y_train2 = np.asarray(y_train,dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "52090548-bc7f-478c-9a10-32516de4847c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41225,), (41225,))"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape, y_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "48f714e6-6f37-4090-ba6a-a974a914c63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([558, 4, 0], [4, 0, 503]), ([0, 2171, 4, 0, 253, 45, 638, 2172, 28, 0, 2125, 6, 39, 0, 769, 370], [2171, 4, 0, 253, 45, 638, 2172, 28, 0, 2125, 6, 39, 0, 769, 370, 73]), ([682, 4], [4, 0]), ([2, 5147, 100, 1, 0, 36, 274, 1, 33, 80, 5148], [5147, 100, 1, 0, 36, 274, 1, 33, 80, 5148, 3]), ([0, 764], [764, 3]), ([1653, 25, 0, 2324, 4, 10, 392, 5, 163, 808, 6, 327, 0, 3721, 392], [25, 0, 2324, 4, 10, 392, 5, 163, 808, 6, 327, 0, 3721, 392, 5]), ([9, 47, 677], [47, 677, 3]), ([8, 645, 22, 0, 565, 13, 202, 41, 14, 573, 6], [645, 22, 0, 565, 13, 202, 41, 14, 573, 6, 7]), ([54, 0, 4009, 240, 1295, 9, 4010, 0], [0, 4009, 240, 1295, 9, 4010, 0, 2201]), ([580, 202, 8, 2738, 13, 0, 1858, 4, 385, 50, 255, 4, 4938, 6, 0, 186, 4939, 12, 101], [202, 8, 2738, 13, 0, 1858, 4, 385, 50, 255, 4, 4938, 6, 0, 186, 4939, 12, 101, 14])]\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(list(zip(X_train2, y_train2)), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5585bfd6-eb4c-4cee-babc-645896aee0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 100)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocabulary_size, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "a33dd262-5bea-41ab-a204-e41dd3fac9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#glove_dir = 'data/glove'\n",
    "glove_dir = \"data\"\n",
    "\n",
    "embeddings_index = {} #initialize dictionary\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding='utf8')\n",
    "try:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "except:\n",
    "    print(line)\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d7cb0209-01e5-4042-b9ac-cbc8f4eec539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "08902cb4-b17a-4090-a361-a8039bcb7b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
    "for word, i in vocab:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < vocabulary_size:\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "2fb4bb08-6b8f-4472-8c81-9cf4280e6b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 100)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c5ca7f1e-73fd-4442-8117-2fe7604145ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('people', 21)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "d54a807d-aa96-4ce3-a6f2-8e086e276291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.29607001, -0.069697  , -0.18896   ,  1.02550006, -0.43494999,\n",
       "        0.39840001,  0.42721   , -0.2018    ,  0.0083542 , -0.12468   ,\n",
       "        0.031648  ,  0.11965   , -0.75585997,  0.058595  , -0.76255   ,\n",
       "       -0.2359    ,  0.49783999, -0.26975   ,  0.06629   , -0.21338999,\n",
       "       -0.96616   , -0.25887999, -0.44992   ,  0.050812  , -0.051264  ,\n",
       "       -0.31377   , -0.11808   ,  0.56151998,  0.36386999,  0.013472  ,\n",
       "        0.18610001,  0.47850001, -0.28551999,  0.55418998, -0.33179   ,\n",
       "        0.22527   ,  0.29462999, -0.45513001,  0.019279  ,  0.45493999,\n",
       "        0.062119  ,  0.50862998, -0.39831001,  0.24673   ,  0.29069   ,\n",
       "       -0.11369   , -0.39943999,  0.34242001, -0.33991   ,  0.27941   ,\n",
       "       -0.020151  ,  0.83740002,  0.3152    , -0.20747   , -0.34476   ,\n",
       "        0.57929999,  0.14407   ,  0.24065   , -0.59442002, -0.14353999,\n",
       "       -0.36322999, -0.08545   ,  0.32905   , -0.029023  , -0.14643   ,\n",
       "        0.32894999,  0.050045  , -0.46294999,  0.32347   ,  0.26108   ,\n",
       "        0.51784003, -0.33182001,  0.26205   ,  0.57756001, -0.34968999,\n",
       "        0.15379   , -0.27551001,  0.70459998,  0.13801999,  0.0044302 ,\n",
       "        0.12709001,  0.13613001, -0.31384   , -0.14734   ,  0.71490997,\n",
       "        0.17443   , -0.55672002, -0.33789   ,  0.0051317 , -0.05963   ,\n",
       "        0.45631999, -0.067641  , -0.078542  , -0.21822   ,  0.47678   ,\n",
       "       -0.21588001,  0.1127    , -0.44117999, -0.6286    ,  0.41538   ])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5656fa22-d887-4c7b-a61e-c2486cf448ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def find_closest_embeddings(embedding):\n",
    "    return sorted(embeddings_index.keys(), key=lambda word: spatial.distance.euclidean(embeddings_index[word], embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "dcd72748-bdb8-47e6-aaa7-f92c587d7ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prince', 'queen', 'monarch', 'brother', 'uncle']"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest_embeddings(embeddings_index[\"king\"])[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "6211f2f2-f930-48a0-b476-1ede824dfc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flashlight', 'twig', 'clipboard', 'shove', 'hand', 'fingers', 'clutching', 'clutched', 'tossing', 'stroking']\n"
     ]
    }
   ],
   "source": [
    "print(find_closest_embeddings(\n",
    "    embeddings_index[\"twig\"] - embeddings_index[\"branch\"] + embeddings_index[\"hand\"]\n",
    ")[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "76409abe-332f-4a53-86c8-a57d30b3fbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 100)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size, embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb0a128-c39b-469c-b80d-ee12a4a487de",
   "metadata": {},
   "source": [
    "### 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "ad5e279a-5fa3-4caf-9db5-a2fe76cb9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "class KerasRNN:\n",
    "    def __init__(self, word_dim, hidden_dim=100, glove_embedding_matrix=None):\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        # Embedding layer initialized with GloVe embeddings\n",
    "        if glove_embedding_matrix is not None:\n",
    "            self.model.add(Embedding(input_dim=word_dim, output_dim=glove_embedding_matrix.shape[1],\n",
    "                                     weights=[glove_embedding_matrix], trainable=False))\n",
    "        \n",
    "        # Adding LSTM layers\n",
    "        self.model.add(LSTM(hidden_dim, return_sequences=True))\n",
    "        self.model.add(LSTM(hidden_dim))\n",
    "        \n",
    "        # Output layer\n",
    "        self.model.add(Dense(word_dim, activation='softmax'))\n",
    "        \n",
    "        # Compile the model\n",
    "        self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "00243cf2-fc56-41c6-9fbe-c3e0b93ae56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 10000)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Layer\n",
    "\n",
    "class CustomRNN(Model):\n",
    "    def __init__(self, vocabulary_size, hidden_dim=100, embedding_matrix=None):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_layer = Embedding(input_dim=vocabulary_size, output_dim=embedding_matrix.shape[1],\n",
    "                                         weights=[embedding_matrix], trainable=False)\n",
    "        self.lstm_layer = LSTM(hidden_dim, return_sequences=True, return_state=True)\n",
    "        self.dense_layer = Dense(vocabulary_size, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Get embeddings\n",
    "        x = self.embedding_layer(inputs)\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm_outputs, _, _ = self.lstm_layer(x)\n",
    "\n",
    "        # Apply dense layer to each time step\n",
    "        output = self.dense_layer(lstm_outputs)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Example of how to use this model\n",
    "vocabulary_size = 10000  # just an example size\n",
    "embedding_dim = 100      # dimension of your GloVe embeddings\n",
    "embedding_matrix = tf.random.normal([vocabulary_size, embedding_dim])  # Mocked embedding matrix\n",
    "\n",
    "# Instantiate and use the RNN\n",
    "model = CustomRNN(vocabulary_size, embedding_matrix=embedding_matrix)\n",
    "\n",
    "# Prepare some mock input data: Batch of sequences\n",
    "input_data = tf.random.uniform((32, 10), dtype=tf.int32, maxval=vocabulary_size)  # batch_size = 32, sequence_length = 10\n",
    "\n",
    "# Forward propagation\n",
    "outputs = model(input_data)\n",
    "print(outputs.shape)  # Should print (32, 10, vocabulary_size) indicating batch_size, sequence_length, vocabulary_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "429fe6c3-0677-40e3-88dc-c04ad3cf0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "        # Perform forward propagation\n",
    "        o, s = self.forward_propagation(x)\n",
    "        # Return the index of the highest score from the last timestep output\n",
    "        return tf.argmax(o[-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "3a5df0cb-6367-4349-946e-d5f5fec4dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # Return the index of the highest score from each output across the sequence\n",
    "    return tf.argmax(o, axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "e06e1105-cd73-455f-aed6-05e1ef4ca7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "print (\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in X_train2[1000]]), X_train2[1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "2e1ad359-9538-49af-bdac-06f35502b36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "the king enters the forest of meditation and is\n",
      "[0, 15, 165, 0, 182, 4, 2585, 5, 8]\n"
     ]
    }
   ],
   "source": [
    "print (\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in X_train2[20000]]), X_train2[20000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "7910e3aa-9c26-4733-8c7d-dc3b9093ca6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, [21, 25, 180, 1154])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size, X_train2[10000]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "bc3787d1-ece7-4402-ba21-0477ca157584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378ms/step - accuracy: 0.0000e+00 - loss: 8.0066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3a1b2dc30>"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class CustomRNN(tf.keras.Model):\n",
    "    def __init__(self, vocabulary_size, embedding_dim=100, rnn_units=100):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocabulary_size, output_dim=embedding_dim)\n",
    "        self.rnn = tf.keras.layers.SimpleRNN(rnn_units, return_sequences=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocabulary_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.rnn(x)\n",
    "        return self.dense(x)  # logits returned here\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Use sparse categorical crossentropy which does softmax internally and calculates loss\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    return scce(y_true, y_pred)\n",
    "\n",
    "# Example usage:\n",
    "model = CustomRNN(vocabulary_size=3000)\n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
    "\n",
    "# Example training data\n",
    "# Create dummy data\n",
    "import numpy as np\n",
    "np.random.seed(17)\n",
    "X_train = np.random.randint(0, 3000, (32, 10))  # 32 sequences, each of length 10\n",
    "y_train = np.random.randint(0, 3000, (32, 10))  # Corresponding targets\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = tf.convert_to_tensor(X_train, dtype=tf.int32)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=1, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "cab5ef61-7510-4dd8-87bf-b266eb38833c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 8.006368\n",
      "Actual loss: 9.899039\n"
     ]
    }
   ],
   "source": [
    "# Ensure model is compiled with a loss function\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Evaluate the model on a subset of the data\n",
    "loss, accuracy = model.evaluate(X_train[:1000], y_train[:1000], verbose=0)\n",
    "print(\"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print(\"Actual loss: %f\" % loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "7a47f48e-521e-4d04-b218-fe9b94ccbb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 428ms/step - accuracy: 0.0000e+00 - loss: 16.5894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3a4decdf0>"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomRNN(tf.keras.Model):\n",
    "    def __init__(self, vocabulary_size, embedding_dim=100, rnn_units=100):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocabulary_size, output_dim=embedding_dim)\n",
    "        self.rnn = tf.keras.layers.SimpleRNN(rnn_units, return_sequences=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocabulary_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.rnn(x)\n",
    "        return self.dense(x)\n",
    "\n",
    "# Initialize and compile the model\n",
    "vocabulary_size = 3000\n",
    "model = CustomRNN(vocabulary_size)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Example training with minimal data\n",
    "x = np.random.randint(0, vocabulary_size, (1, 10))  # Single training example\n",
    "y = np.random.randint(0, vocabulary_size, (1, 10))  # Corresponding labels\n",
    "\n",
    "# Train the model\n",
    "model.fit(x, y, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "a7a9ab84-5ce9-4745-82b2-6d5e4129b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNN(tf.keras.Model):\n",
    "    def __init__(self, vocabulary_size, embedding_dim=10, rnn_units=10, bptt_truncate=1000):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, trainable=True)\n",
    "        self.rnn = tf.keras.layers.SimpleRNN(rnn_units, return_sequences=True, trainable=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocabulary_size, trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.rnn(x)\n",
    "        return self.dense(x)\n",
    "\n",
    "    def compute_loss(self, x, y):\n",
    "        logits = self(x, training=True)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y, logits, from_logits=True)\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "        # Convert x and y to tensors\n",
    "        x = tf.convert_to_tensor(x, dtype=tf.int32)\n",
    "        y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.compute_loss(x, y)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        for var, grad in zip(self.trainable_variables, gradients):\n",
    "            print(f\"Performing gradient check for parameter {var.name} with size {var.shape}\")\n",
    "            \n",
    "            it = np.nditer(var.numpy(), flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "                \n",
    "                original_value = var.numpy()[ix]\n",
    "                \n",
    "                # Increase by h\n",
    "                var.assign_add(tf.reshape(tf.constant(h, dtype=tf.float32), [1] * len(var.shape)))\n",
    "                loss_plus_h = self.compute_loss(x, y)\n",
    "                \n",
    "                # Decrease by h\n",
    "                var.assign(original_value)  # Reset to original\n",
    "                var.assign_sub(tf.reshape(tf.constant(h, dtype=tf.float32), [1] * len(var.shape)))\n",
    "                loss_minus_h = self.compute_loss(x, y)\n",
    "                \n",
    "                # Reset to original\n",
    "                var.assign(original_value)\n",
    "                \n",
    "                # Calculate numerical gradient\n",
    "                estimated_gradient = (loss_plus_h - loss_minus_h) / (2 * h)\n",
    "                backprop_gradient = grad.numpy()[ix]\n",
    "                \n",
    "                relative_error = np.abs(backprop_gradient - estimated_gradient) / (np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "                \n",
    "                if relative_error > error_threshold:\n",
    "                    print(f\"Gradient Check ERROR: parameter={var.name} index={ix}\")\n",
    "                    print(f\"+h Loss: {loss_plus_h.numpy()}\")\n",
    "                    print(f\"-h Loss: {loss_minus_h.numpy()}\")\n",
    "                    print(f\"Estimated_gradient: {estimated_gradient}\")\n",
    "                    print(f\"Backpropagation gradient: {backprop_gradient}\")\n",
    "                    print(f\"Relative Error: {relative_error}\")\n",
    "                    return False\n",
    "                \n",
    "                it.iternext()\n",
    "            \n",
    "            print(f\"Gradient check for parameter {var.name} passed.\")\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "bbd7c632-2b27-40ce-ac83-96f881fdc1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "\n",
    "    for epoch in range(nepoch):\n",
    "        print(f\"Epoch {epoch+1}/{nepoch}\")\n",
    "\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            preds = model(X_train, training=False)\n",
    "            loss = loss_fn(y_train, preds)\n",
    "            losses.append((num_examples_seen, loss.numpy()))\n",
    "            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(f\"{current_time}: Loss after num_examples_seen={num_examples_seen} epoch={epoch}: {loss.numpy()}\")\n",
    "\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if len(losses) > 1 and losses[-1][1] > losses[-2][1]:\n",
    "                learning_rate *= 0.5\n",
    "                optimizer.learning_rate = learning_rate\n",
    "                print(f\"Setting learning rate to {learning_rate}\")\n",
    "\n",
    "        # Training loop\n",
    "        for i in range(len(y_train)):\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(X_train[i:i+1], training=True)  # Slice i:i+1 to keep batch dimension\n",
    "                loss = loss_fn(y_train[i:i+1], predictions)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "2dac0e64-cd9b-4718-b561-60a57f7a595d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "fed7ab19-4742-47d7-a159-5269cdce0d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6446"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "909a574e-ecb4-4760-9081-20a67cd42009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, word_to_index, index_to_word, sentence_start_token, sentence_end_token, unknown_token, senten_max_length):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    \n",
    "    # Prepare the model for predicting next words\n",
    "    while (new_sentence[-1] != word_to_index[sentence_end_token]) and len(new_sentence) < senten_max_length:\n",
    "        # Prepare input for prediction\n",
    "        current_input = tf.expand_dims(new_sentence, axis=0)  # Expand dims for batch size\n",
    "        \n",
    "        # Predict logits for next word\n",
    "        logits = model(current_input)\n",
    "        next_word_probs = tf.nn.softmax(logits[:, -1, :])  # Get softmax for the last time step\n",
    "\n",
    "        # Sample the next word using logits\n",
    "        sampled_word = tf.random.categorical(next_word_probs, num_samples=1)\n",
    "        \n",
    "        # Ensure not to choose unknown token\n",
    "        while sampled_word.numpy()[0][0] == word_to_index[unknown_token]:\n",
    "            sampled_word = tf.random.categorical(next_word_probs, num_samples=1)\n",
    "        \n",
    "        # Append the sampled word to the sentence\n",
    "        new_sentence.append(sampled_word.numpy()[0][0])\n",
    "        \n",
    "        if len(new_sentence) >= senten_max_length or new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "            break\n",
    "\n",
    "    # Convert indices to words\n",
    "    sentence_str = [index_to_word[idx] for idx in new_sentence if idx in index_to_word]\n",
    "    \n",
    "    return ' '.join(sentence_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eade7e8-676a-4113-8187-c6441dd3ae71",
   "metadata": {},
   "source": [
    "### 5. Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "28a5f1a9-92d2-4cb1-9e7d-fcc74a79ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, vocabulary_size, embedding_dim=100, rnn_units=100):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocabulary_size, output_dim=embedding_dim)\n",
    "        self.rnn = tf.keras.layers.SimpleRNN(rnn_units, return_sequences=True)  # Only returning the sequence output\n",
    "        self.dense = tf.keras.layers.Dense(vocabulary_size, activation='softmax')  # Using softmax for categorical output\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.rnn(x)  # No need to unpack, only one output since return_state=False\n",
    "        return self.dense(x)\n",
    "\n",
    "# Parameters\n",
    "vocabulary_size = 300000  # Adjust according to your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "6a424dc6-8ab2-413c-bb03-2c2a9fde3dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(vocabulary_size)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "4f5a71f5-5629-4126-87c6-ac158e541184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data for demonstration (ensure you have real data in practice)\n",
    "import numpy as np\n",
    "np.random.seed(17)\n",
    "X_train2 = np.random.randint(0, vocabulary_size, (100, 10))  # 100 sequences, each of length 10\n",
    "y_train2 = np.random.randint(0, vocabulary_size, (100, 10))  # Corresponding labels for each sequence element\n",
    "\n",
    "# Convert to tensors if not already\n",
    "X_train2 = tf.convert_to_tensor(X_train2, dtype=tf.int32)\n",
    "y_train2 = tf.convert_to_tensor(y_train2, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "a9a6b7f1-180f-48bd-ad59-03feed7cd130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 542ms/step - accuracy: 0.0000e+00 - loss: 12.6115\n",
      "Epoch 2/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 507ms/step - accuracy: 0.8242 - loss: 12.5934\n",
      "Epoch 3/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 521ms/step - accuracy: 0.9921 - loss: 12.5728\n",
      "Epoch 4/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 512ms/step - accuracy: 0.9994 - loss: 12.5435\n",
      "Epoch 5/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 531ms/step - accuracy: 0.9968 - loss: 12.4962\n",
      "Epoch 6/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 519ms/step - accuracy: 0.7896 - loss: 12.4075\n",
      "Epoch 7/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 513ms/step - accuracy: 0.4653 - loss: 12.2383\n",
      "Epoch 8/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 514ms/step - accuracy: 0.3083 - loss: 11.9549\n",
      "Epoch 9/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 519ms/step - accuracy: 0.1991 - loss: 11.5560\n",
      "Epoch 10/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 577ms/step - accuracy: 0.1407 - loss: 11.0635\n",
      "Epoch 11/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 526ms/step - accuracy: 0.0927 - loss: 10.5083\n",
      "Epoch 12/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 529ms/step - accuracy: 0.0623 - loss: 9.9207\n",
      "Epoch 13/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 521ms/step - accuracy: 0.0185 - loss: 9.3358\n",
      "Epoch 14/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 516ms/step - accuracy: 0.0022 - loss: 8.7965\n",
      "Epoch 15/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 508ms/step - accuracy: 4.0000e-04 - loss: 8.3409\n",
      "Epoch 16/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 502ms/step - accuracy: 4.0000e-04 - loss: 7.9891\n",
      "Epoch 17/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 506ms/step - accuracy: 4.0000e-04 - loss: 7.7397\n",
      "Epoch 18/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 512ms/step - accuracy: 4.0000e-04 - loss: 7.5751\n",
      "Epoch 19/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 536ms/step - accuracy: 4.0000e-04 - loss: 7.4703\n",
      "Epoch 20/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 522ms/step - accuracy: 4.0000e-04 - loss: 7.4027\n",
      "Epoch 21/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 506ms/step - accuracy: 4.0000e-04 - loss: 7.3574\n",
      "Epoch 22/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 516ms/step - accuracy: 4.0000e-04 - loss: 7.3257\n",
      "Epoch 23/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 500ms/step - accuracy: 4.0000e-04 - loss: 7.3026\n",
      "Epoch 24/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 505ms/step - accuracy: 4.0000e-04 - loss: 7.2846\n",
      "Epoch 25/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 531ms/step - accuracy: 8.0000e-04 - loss: 7.2687\n",
      "Epoch 26/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 575ms/step - accuracy: 8.0000e-04 - loss: 7.2531\n",
      "Epoch 27/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 510ms/step - accuracy: 4.0000e-04 - loss: 7.2366\n",
      "Epoch 28/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 505ms/step - accuracy: 8.0000e-04 - loss: 7.2187\n",
      "Epoch 29/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 504ms/step - accuracy: 8.0000e-04 - loss: 7.1990\n",
      "Epoch 30/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 504ms/step - accuracy: 8.0000e-04 - loss: 7.1775\n",
      "Epoch 31/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 503ms/step - accuracy: 8.0000e-04 - loss: 7.1547\n",
      "Epoch 32/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 509ms/step - accuracy: 0.0012 - loss: 7.1313    \n",
      "Epoch 33/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 519ms/step - accuracy: 0.0012 - loss: 7.1079    \n",
      "Epoch 34/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 517ms/step - accuracy: 8.0000e-04 - loss: 7.0856\n",
      "Epoch 35/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 522ms/step - accuracy: 0.0016 - loss: 7.0647  \n",
      "Epoch 36/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 514ms/step - accuracy: 0.0016 - loss: 7.0455  \n",
      "Epoch 37/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 500ms/step - accuracy: 0.0016 - loss: 7.0280  \n",
      "Epoch 38/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 503ms/step - accuracy: 0.0016 - loss: 7.0120  \n",
      "Epoch 39/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 513ms/step - accuracy: 0.0016 - loss: 6.9974  \n",
      "Epoch 40/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 513ms/step - accuracy: 0.0012 - loss: 6.9839    \n",
      "Epoch 41/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 518ms/step - accuracy: 0.0012 - loss: 6.9714    \n",
      "Epoch 42/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 517ms/step - accuracy: 0.0016 - loss: 6.9596  \n",
      "Epoch 43/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 513ms/step - accuracy: 0.0012 - loss: 6.9486    \n",
      "Epoch 44/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 510ms/step - accuracy: 0.0012 - loss: 6.9381    \n",
      "Epoch 45/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 507ms/step - accuracy: 0.0012 - loss: 6.9280    \n",
      "Epoch 46/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 508ms/step - accuracy: 0.0012 - loss: 6.9184    \n",
      "Epoch 47/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 519ms/step - accuracy: 0.0016 - loss: 6.9091  \n",
      "Epoch 48/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 517ms/step - accuracy: 0.0016 - loss: 6.9000  \n",
      "Epoch 49/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 511ms/step - accuracy: 0.0012 - loss: 6.8912    \n",
      "Epoch 50/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 512ms/step - accuracy: 0.0012 - loss: 6.8825    \n",
      "Epoch 51/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 504ms/step - accuracy: 0.0012 - loss: 6.8741    \n",
      "Epoch 52/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 510ms/step - accuracy: 0.0012 - loss: 6.8657    \n",
      "Epoch 53/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 507ms/step - accuracy: 0.0012 - loss: 6.8576    \n",
      "Epoch 54/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 504ms/step - accuracy: 0.0012 - loss: 6.8495    \n",
      "Epoch 55/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 522ms/step - accuracy: 0.0016 - loss: 6.8416  \n",
      "Epoch 56/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 518ms/step - accuracy: 0.0016 - loss: 6.8339  \n",
      "Epoch 57/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 505ms/step - accuracy: 0.0016 - loss: 6.8263  \n",
      "Epoch 58/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 511ms/step - accuracy: 0.0016 - loss: 6.8188  \n",
      "Epoch 59/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 594ms/step - accuracy: 0.0016 - loss: 6.8115  \n",
      "Epoch 60/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 516ms/step - accuracy: 0.0016 - loss: 6.8044  \n",
      "Epoch 61/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 518ms/step - accuracy: 0.0016 - loss: 6.7974  \n",
      "Epoch 62/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 525ms/step - accuracy: 0.0016 - loss: 6.7906  \n",
      "Epoch 63/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 513ms/step - accuracy: 0.0020 - loss: 6.7840  \n",
      "Epoch 64/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 502ms/step - accuracy: 0.0024 - loss: 6.7775  \n",
      "Epoch 65/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 503ms/step - accuracy: 0.0024 - loss: 6.7712  \n",
      "Epoch 66/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 500ms/step - accuracy: 0.0024 - loss: 6.7650  \n",
      "Epoch 67/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 506ms/step - accuracy: 0.0024 - loss: 6.7590  \n",
      "Epoch 68/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 506ms/step - accuracy: 0.0024 - loss: 6.7531  \n",
      "Epoch 69/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 516ms/step - accuracy: 0.0024 - loss: 6.7474  \n",
      "Epoch 70/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 504ms/step - accuracy: 0.0024 - loss: 6.7418  \n",
      "Epoch 71/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 513ms/step - accuracy: 0.0024 - loss: 6.7363  \n",
      "Epoch 72/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 520ms/step - accuracy: 0.0028 - loss: 6.7309  \n",
      "Epoch 73/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 518ms/step - accuracy: 0.0028 - loss: 6.7256  \n",
      "Epoch 74/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 520ms/step - accuracy: 0.0028 - loss: 6.7203  \n",
      "Epoch 75/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 509ms/step - accuracy: 0.0028 - loss: 6.7152  \n",
      "Epoch 76/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 532ms/step - accuracy: 0.0028 - loss: 6.7102  \n",
      "Epoch 77/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 533ms/step - accuracy: 0.0028 - loss: 6.7052  \n",
      "Epoch 78/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 511ms/step - accuracy: 0.0028 - loss: 6.7004  \n",
      "Epoch 79/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 513ms/step - accuracy: 0.0028 - loss: 6.6955  \n",
      "Epoch 80/80\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 508ms/step - accuracy: 0.0028 - loss: 6.6908  \n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train2, y_train2, batch_size=32, epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "6688279a-719e-4e29-96c5-a4c238a0ac90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 67ms/step - accuracy: 0.0758 - loss: 7.5922\n",
      "Epoch 2/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 70ms/step - accuracy: 0.0800 - loss: 6.4926\n",
      "Epoch 3/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 72ms/step - accuracy: 0.0877 - loss: 6.3333\n",
      "Epoch 4/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 70ms/step - accuracy: 0.1011 - loss: 6.2176\n",
      "Epoch 5/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 72ms/step - accuracy: 0.1126 - loss: 6.0869\n",
      "Epoch 6/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 72ms/step - accuracy: 0.1212 - loss: 5.9782\n",
      "Epoch 7/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 73ms/step - accuracy: 0.1273 - loss: 5.8680\n",
      "Epoch 8/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 72ms/step - accuracy: 0.1302 - loss: 5.7793\n",
      "Epoch 9/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 73ms/step - accuracy: 0.1327 - loss: 5.7255\n",
      "Epoch 10/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 71ms/step - accuracy: 0.1355 - loss: 5.6289\n",
      "Epoch 11/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - accuracy: 0.1386 - loss: 5.5362\n",
      "Epoch 12/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - accuracy: 0.1423 - loss: 5.4262\n",
      "Epoch 13/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - accuracy: 0.1442 - loss: 5.3286\n",
      "Epoch 14/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 76ms/step - accuracy: 0.1478 - loss: 5.2172\n",
      "Epoch 15/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 74ms/step - accuracy: 0.1511 - loss: 5.1252\n",
      "Epoch 16/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - accuracy: 0.1557 - loss: 5.0346\n",
      "Epoch 17/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.1581 - loss: 4.9458\n",
      "Epoch 18/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - accuracy: 0.1601 - loss: 4.8616\n",
      "Epoch 19/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 80ms/step - accuracy: 0.1662 - loss: 4.7697\n",
      "Epoch 20/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.1713 - loss: 4.6660\n",
      "Epoch 21/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - accuracy: 0.1792 - loss: 4.5593\n",
      "Epoch 22/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.1829 - loss: 4.4653\n",
      "Epoch 23/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - accuracy: 0.1919 - loss: 4.3558\n",
      "Epoch 24/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.1967 - loss: 4.2510\n",
      "Epoch 25/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.2050 - loss: 4.1472\n",
      "Epoch 26/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - accuracy: 0.2119 - loss: 4.0491\n",
      "Epoch 27/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 81ms/step - accuracy: 0.2193 - loss: 3.9568\n",
      "Epoch 28/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 81ms/step - accuracy: 0.2280 - loss: 3.8747\n",
      "Epoch 29/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 80ms/step - accuracy: 0.2380 - loss: 3.7795\n",
      "Epoch 30/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 80ms/step - accuracy: 0.2461 - loss: 3.6946\n",
      "Epoch 31/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.2601 - loss: 3.6051\n",
      "Epoch 32/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 80ms/step - accuracy: 0.2728 - loss: 3.5156\n",
      "Epoch 33/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.2854 - loss: 3.4220\n",
      "Epoch 34/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - accuracy: 0.3017 - loss: 3.3307\n",
      "Epoch 35/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - accuracy: 0.3193 - loss: 3.2426\n",
      "Epoch 36/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 80ms/step - accuracy: 0.3347 - loss: 3.1464\n",
      "Epoch 37/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - accuracy: 0.3555 - loss: 3.0556\n",
      "Epoch 38/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.3738 - loss: 2.9631\n",
      "Epoch 39/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - accuracy: 0.3957 - loss: 2.8728\n",
      "Epoch 40/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 76ms/step - accuracy: 0.4103 - loss: 2.7899\n",
      "Epoch 41/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 80ms/step - accuracy: 0.4288 - loss: 2.7052\n",
      "Epoch 42/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - accuracy: 0.4384 - loss: 2.6320\n",
      "Epoch 43/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - accuracy: 0.4606 - loss: 2.5589\n",
      "Epoch 44/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 80ms/step - accuracy: 0.4705 - loss: 2.4892\n",
      "Epoch 45/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.4875 - loss: 2.4183\n",
      "Epoch 46/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - accuracy: 0.4960 - loss: 2.3681\n",
      "Epoch 47/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 82ms/step - accuracy: 0.3948 - loss: 3.3401\n",
      "Epoch 48/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.4393 - loss: 2.7523\n",
      "Epoch 49/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 82ms/step - accuracy: 0.4637 - loss: 2.5785\n",
      "Epoch 50/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 80ms/step - accuracy: 0.4594 - loss: 2.5994\n",
      "Epoch 51/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 82ms/step - accuracy: 0.4085 - loss: 2.8598\n",
      "Epoch 52/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 82ms/step - accuracy: 0.4407 - loss: 2.6380\n",
      "Epoch 53/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 82ms/step - accuracy: 0.4442 - loss: 2.6525\n",
      "Epoch 54/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 86ms/step - accuracy: 0.4842 - loss: 2.4288\n",
      "Epoch 55/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 76ms/step - accuracy: 0.4752 - loss: 2.4579\n",
      "Epoch 56/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 83ms/step - accuracy: 0.5209 - loss: 2.2351\n",
      "Epoch 57/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 81ms/step - accuracy: 0.4661 - loss: 2.5100\n",
      "Epoch 58/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 81ms/step - accuracy: 0.5150 - loss: 2.2620\n",
      "Epoch 59/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 82ms/step - accuracy: 0.5440 - loss: 2.1473\n",
      "Epoch 60/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 80ms/step - accuracy: 0.5839 - loss: 1.9720\n",
      "Epoch 61/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - accuracy: 0.6201 - loss: 1.8124\n",
      "Epoch 62/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.6558 - loss: 1.6714\n",
      "Epoch 63/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.6783 - loss: 1.5597\n",
      "Epoch 64/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - accuracy: 0.6971 - loss: 1.4694\n",
      "Epoch 65/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 81ms/step - accuracy: 0.7156 - loss: 1.3863\n",
      "Epoch 66/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - accuracy: 0.7361 - loss: 1.2986\n",
      "Epoch 67/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - accuracy: 0.7550 - loss: 1.2051\n",
      "Epoch 68/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - accuracy: 0.7827 - loss: 1.1177\n",
      "Epoch 69/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 80ms/step - accuracy: 0.7989 - loss: 1.0397\n",
      "Epoch 70/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.8172 - loss: 0.9607\n",
      "Epoch 71/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 80ms/step - accuracy: 0.8340 - loss: 0.8926\n",
      "Epoch 72/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - accuracy: 0.8492 - loss: 0.8375\n",
      "Epoch 73/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - accuracy: 0.8635 - loss: 0.7799\n",
      "Epoch 74/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - accuracy: 0.8752 - loss: 0.7335\n",
      "Epoch 75/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - accuracy: 0.8845 - loss: 0.6902\n",
      "Epoch 76/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 80ms/step - accuracy: 0.8884 - loss: 0.6532\n",
      "Epoch 77/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.8920 - loss: 0.6294\n",
      "Epoch 78/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 80ms/step - accuracy: 0.9009 - loss: 0.5897\n",
      "Epoch 79/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - accuracy: 0.9137 - loss: 0.5456\n",
      "Epoch 80/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 81ms/step - accuracy: 0.9218 - loss: 0.5107\n",
      "Epoch 81/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - accuracy: 0.9302 - loss: 0.4653\n",
      "Epoch 82/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.9408 - loss: 0.4255\n",
      "Epoch 83/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - accuracy: 0.9468 - loss: 0.3943\n",
      "Epoch 84/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - accuracy: 0.9570 - loss: 0.3720\n",
      "Epoch 85/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.9572 - loss: 0.3553\n",
      "Epoch 86/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.9619 - loss: 0.3291\n",
      "Epoch 87/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 81ms/step - accuracy: 0.9662 - loss: 0.3090\n",
      "Epoch 88/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.9634 - loss: 0.2998\n",
      "Epoch 89/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - accuracy: 0.9687 - loss: 0.2886\n",
      "Epoch 90/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.9723 - loss: 0.2683\n",
      "Epoch 91/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - accuracy: 0.9746 - loss: 0.2561\n",
      "Epoch 92/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 81ms/step - accuracy: 0.9810 - loss: 0.2353\n",
      "Epoch 93/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 85ms/step - accuracy: 0.9825 - loss: 0.2237\n",
      "Epoch 94/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 85ms/step - accuracy: 0.9789 - loss: 0.2281\n",
      "Epoch 95/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 81ms/step - accuracy: 0.9854 - loss: 0.1882\n",
      "Epoch 96/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - accuracy: 0.9871 - loss: 0.1801\n",
      "Epoch 97/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 74ms/step - accuracy: 0.9847 - loss: 0.1875\n",
      "Epoch 98/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 73ms/step - accuracy: 0.9844 - loss: 0.1786\n",
      "Epoch 99/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 80ms/step - accuracy: 0.9823 - loss: 0.1819\n",
      "Epoch 100/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - accuracy: 0.9897 - loss: 0.1528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3a2d938b0>"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=train_length),\n",
    "    LSTM(150, return_sequences=True),\n",
    "    LSTM(150),\n",
    "    Dense(len(tokenizer.word_index) + 1, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=100, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e39f1-5e9e-441d-9563-cbb0c94044bd",
   "metadata": {},
   "source": [
    "### 6. Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "7a21ede2-9bfd-444b-b649-9c903f9e3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, num_generate=100):\n",
    "    for _ in range(num_generate):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=train_length-1, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)\n",
    "        predicted_index = np.argmax(predicted_probs, axis=-1)\n",
    "        # Ensure that predicted_index is an integer, not an array\n",
    "        if isinstance(predicted_index, np.ndarray):\n",
    "            predicted_index = predicted_index[0]  # Take the first element if it's an array\n",
    "        output_word = tokenizer.index_word.get(predicted_index, '')  # Safely get the word\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "ff53b2b2-663a-4bb3-a48f-8ff2b986baf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this world we rarely behold such characters as theirs. Their lofty rank is the abode of wisdom and of piety, of valour and of virtue. Their fame spreads white and spotless through the universe. A son has sprung from Devarata whose opening virtues early give occasion of rejoicing to the world. Now, in his bloom, this youth has been sent to our city to collect ripe stores of knowledge. His name is Madhava. if feels her maidens a majesty as my lord i am pledge and one charita or his hundred to well slain her purpose i resemble not be made it thinks and unprotected for this rai of matrimonial world his friend gotama is discovered the king of his worshippers of the\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the text generation function\n",
    "seed_text = \"In this world we rarely behold such characters as theirs. Their lofty rank is the abode of wisdom and of piety, of valour and of virtue. Their fame spreads white and spotless through the universe. A son has sprung from Devarata whose opening virtues early give occasion of rejoicing to the world. Now, in his bloom, this youth has been sent to our city to collect ripe stores of knowledge. His name is Madhava.\"\n",
    "generated_text = generate_text(seed_text, num_generate=50)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6a848-ab16-40f1-a320-1dea8f4eb17b",
   "metadata": {},
   "source": [
    "### Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5fca53-1bf0-4521-a1e0-43c42330ac1d",
   "metadata": {},
   "source": [
    "We have successfully developed and trained a model using TensorFlow with the Keras API, incorporating GloVe embeddings to enhance its output capabilities. By integrating GloVe embeddings, our model can leverage pre-trained word vectors, which improves its understanding of context and semantics in text generation. Utilizing TensorFlow and the Keras Sequential API has allowed us to streamline the model architecture, making it efficient and scalable for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b660018-1276-4e41-a470-21acedf4f46b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
